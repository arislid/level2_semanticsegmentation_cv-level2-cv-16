[01/01 10:31:36] detectron2 INFO: Rank of current process: 0. World size: 1
[01/01 10:31:37] detectron2 INFO: Environment info:
----------------------  -----------------------------------------------------------------------
sys.platform            linux
Python                  3.8.15 (default, Nov 24 2022, 15:19:38) [GCC 11.2.0]
numpy                   1.23.4
detectron2              0.5 @/opt/conda/envs/maskdino/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.0
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.7.0 @/opt/conda/envs/maskdino/lib/python3.8/site-packages/torch
PyTorch debug build     True
GPU available           Yes
GPU 0                   Tesla V100-PCIE-32GB (arch=7.0)
CUDA_HOME               /usr/local/cuda
Pillow                  9.3.0
torchvision             0.8.0 @/opt/conda/envs/maskdino/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0
fvcore                  0.1.5.post20221221
iopath                  0.1.8
cv2                     4.6.0
----------------------  -----------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.3
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/01 10:31:37] detectron2 INFO: Command line arguments: Namespace(config_file='configs/ade20k/semantic-segmentation/semask_swin/custom_trash_semantic_segmentation.yaml', dist_url='tcp://127.0.0.1:50162', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)
[01/01 10:31:37] detectron2 INFO: Contents of args.config_file=configs/ade20k/semantic-segmentation/semask_swin/custom_trash_semantic_segmentation.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msemask_maskformer2_R50_bs16_160k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mD2SeMaskSwinTransformer[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m18[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m6[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m12[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m24[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m48[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;197mSEM_WINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;197mNUM_SEM_BLOCKS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./pretrained/semask_large_mask2former_msfapn_ade20k.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m123.675[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m116.280[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m103.530[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m58.395[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m57.120[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m57.375[39m[38;5;15m][39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m("trash_recycle_sem_seg_train_0",)[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m("trash_recycle_sem_seg_val_0",)[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;81m!!python/object/apply:eval[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186m[int(x[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m0.1[39m[38;5;15m [39m[38;5;186m*[39m[38;5;15m [39m[38;5;186m640)[39m[38;5;15m [39m[38;5;186mfor[39m[38;5;15m [39m[38;5;186mx[39m[38;5;15m [39m[38;5;186min[39m[38;5;15m [39m[38;5;186mrange(5,[39m[38;5;15m [39m[38;5;186m21)][39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mchoice[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mabsolute[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m(512,[39m[38;5;141m [39m[38;5;141m512)[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m640[39m[38;5;15m  [39m[38;5;242m# used in dataset mapper[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mRGB[39m[38;5;186m"[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1300[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m320[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m480[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m640[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m800[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m960[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1120[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2240[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBEST_CHECKPOINTER[39m[38;5;15m:[39m
[38;5;15m        [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m        [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186msem_seg[39m[38;5;186m"[39m
[38;5;15m        [39m[38;5;197mMETRIC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mmIoU[39m[38;5;186m"[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./trash_dataV1_check_best_mIoU[39m[38;5;186m"[39m

[01/01 10:31:37] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mtrash_recycle_sem_seg_val_0[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mtrash_recycle_sem_seg_train_0[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_semantic[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m320[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m448[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m576[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m704[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m832[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m896[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m960[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1088[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1152[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1216[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mD2SeMaskSwinTransformer[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCATE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSeMaskMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mBranchMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m11[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mPixelFANDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mNUM_SEM_BLOCKS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mSEM_WINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./pretrained/semask_large_mask2former_msfapn_ade20k.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./trash_dataV1_WarmupPolyLR_1e-5[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m42[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBEST_CHECKPOINTER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMETRIC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmIoU[39m
[38;5;15m    [39m[38;5;197mMODE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmax[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msem_seg[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m39000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2600[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2240[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m320[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m480[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m960[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1120[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1300[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/01 10:31:37] detectron2 INFO: Full config saved to ./trash_dataV1_WarmupPolyLR_1e-5/config.yaml
[01/01 10:31:42] d2.engine.defaults INFO: Model:
SeMaskMaskFormer(
  (backbone): D2SeMaskSwinTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.013)
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (semantic_layer): SWSeMaskBlock(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (class_injection): ModuleList(
            (0): SemanticAttention(
              (softmax): Softmax(dim=-1)
              (mlp_cls_q): Linear(in_features=192, out_features=11, bias=True)
              (mlp_cls_k): Linear(in_features=192, out_features=11, bias=True)
              (mlp_v): Linear(in_features=192, out_features=192, bias=True)
              (mlp_res): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BasicLayer(
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.026)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.039)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (semantic_layer): SWSeMaskBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (class_injection): ModuleList(
            (0): SemanticAttention(
              (softmax): Softmax(dim=-1)
              (mlp_cls_q): Linear(in_features=384, out_features=11, bias=True)
              (mlp_cls_k): Linear(in_features=384, out_features=11, bias=True)
              (mlp_v): Linear(in_features=384, out_features=384, bias=True)
              (mlp_res): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.052)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.065)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.078)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.091)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.104)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.117)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.130)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.143)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.157)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.170)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.183)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.196)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.209)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.222)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.235)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.248)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.261)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.274)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (semantic_layer): SeMaskBlock(
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (class_injection): ModuleList(
            (0): SemanticAttention(
              (softmax): Softmax(dim=-1)
              (mlp_cls_q): Linear(in_features=768, out_features=11, bias=True)
              (mlp_cls_k): Linear(in_features=768, out_features=11, bias=True)
              (mlp_v): Linear(in_features=768, out_features=768, bias=True)
              (mlp_res): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          (reduction): Linear(in_features=3072, out_features=1536, bias=False)
          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=1536, out_features=4608, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1536, out_features=1536, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.287)
            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1536, out_features=6144, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=6144, out_features=1536, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=1536, out_features=4608, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1536, out_features=1536, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.300)
            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1536, out_features=6144, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=6144, out_features=1536, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (semantic_layer): SeMaskBlock(
          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          (class_injection): ModuleList(
            (0): SemanticAttention(
              (softmax): Softmax(dim=-1)
              (mlp_cls_q): Linear(in_features=1536, out_features=11, bias=True)
              (mlp_cls_k): Linear(in_features=1536, out_features=11, bias=True)
              (mlp_v): Linear(in_features=1536, out_features=1536, bias=True)
              (mlp_res): Linear(in_features=1536, out_features=1536, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (sem_seg_head): BranchMaskFormerHead(
    (sem_head): SemanticHead(
      (scale_heads): ModuleList(
        (0): Sequential()
        (1): Sequential(
          (0): Upsample(scale_factor=2.0, mode=bilinear)
        )
        (2): Sequential(
          (0): Upsample(scale_factor=2.0, mode=bilinear)
          (1): Upsample(scale_factor=2.0, mode=bilinear)
        )
        (3): Sequential(
          (0): Upsample(scale_factor=2.0, mode=bilinear)
          (1): Upsample(scale_factor=2.0, mode=bilinear)
          (2): Upsample(scale_factor=2.0, mode=bilinear)
        )
      )
    )
    (pixel_decoder): PixelFANDecoder(
      (adapter_1): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (align_1): FeatureAlign(
        (offset): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (dcpack_L2): DCN(
          (conv_offset_mask): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (relu): ReLU(inplace=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_2): Conv2d(
        384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (align_2): FeatureAlign(
        (offset): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (dcpack_L2): DCN(
          (conv_offset_mask): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (relu): ReLU(inplace=True)
      )
      (layer_2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (adapter_3): Conv2d(
        768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (align_3): FeatureAlign(
        (offset): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (dcpack_L2): DCN(
          (conv_offset_mask): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (relu): ReLU(inplace=True)
      )
      (layer_3): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_4): Conv2d(
        1536, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (mask_features): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=12, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): SeMaskCriterion SeMaskSetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'labels_cate', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_cate': 0.4, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_cate_0': 0.4, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_cate_1': 0.4, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_cate_2': 0.4, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_cate_3': 0.4, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_cate_4': 0.4, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_cate_5': 0.4, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_cate_6': 0.4, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_cate_7': 0.4, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_cate_8': 0.4}
      num_classes: 11
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/01 10:31:43] d2.data.datasets.coco INFO: Loaded 2618 images with semantic segmentation from /opt/ml/input/data/images/train_0
[01/01 10:31:43] d2.data.build INFO: Using training sampler TrainingSampler
[01/01 10:31:43] d2.data.common INFO: Serializing 2618 elements to byte tensors and concatenating them all ...
[01/01 10:31:43] d2.data.common INFO: Serialized dataset takes 0.36 MiB
[01/01 10:31:43] fvcore.common.checkpoint INFO: [Checkpointer] Loading from ./pretrained/semask_large_mask2former_msfapn_ade20k.pkl ...
[01/01 10:31:44] fvcore.common.checkpoint INFO: Reading a file from 'third_party'
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_k.bias in checkpoint is torch.Size([150]), while shape of backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_k.bias in model is torch.Size([11]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_k.bias will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_k.weight in checkpoint is torch.Size([150, 192]), while shape of backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_k.weight in model is torch.Size([11, 192]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_k.weight will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_q.bias in checkpoint is torch.Size([150]), while shape of backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_q.bias in model is torch.Size([11]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_q.bias will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_q.weight in checkpoint is torch.Size([150, 192]), while shape of backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_q.weight in model is torch.Size([11, 192]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_q.weight will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_k.bias in checkpoint is torch.Size([150]), while shape of backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_k.bias in model is torch.Size([11]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_k.bias will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_k.weight in checkpoint is torch.Size([150, 384]), while shape of backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_k.weight in model is torch.Size([11, 384]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_k.weight will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_q.bias in checkpoint is torch.Size([150]), while shape of backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_q.bias in model is torch.Size([11]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_q.bias will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_q.weight in checkpoint is torch.Size([150, 384]), while shape of backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_q.weight in model is torch.Size([11, 384]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_q.weight will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_k.bias in checkpoint is torch.Size([150]), while shape of backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_k.bias in model is torch.Size([11]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_k.bias will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_k.weight in checkpoint is torch.Size([150, 768]), while shape of backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_k.weight in model is torch.Size([11, 768]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_k.weight will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_q.bias in checkpoint is torch.Size([150]), while shape of backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_q.bias in model is torch.Size([11]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_q.bias will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_q.weight in checkpoint is torch.Size([150, 768]), while shape of backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_q.weight in model is torch.Size([11, 768]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_q.weight will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_k.bias in checkpoint is torch.Size([150]), while shape of backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_k.bias in model is torch.Size([11]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_k.bias will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_k.weight in checkpoint is torch.Size([150, 1536]), while shape of backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_k.weight in model is torch.Size([11, 1536]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_k.weight will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_q.bias in checkpoint is torch.Size([150]), while shape of backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_q.bias in model is torch.Size([11]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_q.bias will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_q.weight in checkpoint is torch.Size([150, 1536]), while shape of backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_q.weight in model is torch.Size([11, 1536]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_q.weight will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of criterion.empty_weight in checkpoint is torch.Size([151]), while shape of criterion.empty_weight in model is torch.Size([12]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: criterion.empty_weight will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of sem_seg_head.pixel_decoder.mask_features.weight in checkpoint is torch.Size([256, 256, 1, 1]), while shape of sem_seg_head.pixel_decoder.mask_features.weight in model is torch.Size([256, 256, 3, 3]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: sem_seg_head.pixel_decoder.mask_features.weight will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of sem_seg_head.predictor.class_embed.bias in checkpoint is torch.Size([151]), while shape of sem_seg_head.predictor.class_embed.bias in model is torch.Size([12]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: sem_seg_head.predictor.class_embed.bias will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: Shape of sem_seg_head.predictor.class_embed.weight in checkpoint is torch.Size([151, 256]), while shape of sem_seg_head.predictor.class_embed.weight in model is torch.Size([12, 256]).
[01/01 10:31:44] d2.checkpoint.c2_model_loading WARNING: sem_seg_head.predictor.class_embed.weight will not be loaded. Please double check and see if this is desired.
[01/01 10:31:44] d2.checkpoint.c2_model_loading INFO: Following weights matched with model:
| Names in Model                                                               | Names in Checkpoint                                                                                                                    | Shapes                                                     |
|:-----------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------|
| backbone.layers.0.blocks.0.attn.*                                            | backbone.layers.0.blocks.0.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (192,) (192,192) (576,) (576,192) (529,6) (144,144)        |
| backbone.layers.0.blocks.0.mlp.fc1.*                                         | backbone.layers.0.blocks.0.mlp.fc1.{bias,weight}                                                                                       | (768,) (768,192)                                           |
| backbone.layers.0.blocks.0.mlp.fc2.*                                         | backbone.layers.0.blocks.0.mlp.fc2.{bias,weight}                                                                                       | (192,) (192,768)                                           |
| backbone.layers.0.blocks.0.norm1.*                                           | backbone.layers.0.blocks.0.norm1.{bias,weight}                                                                                         | (192,) (192,)                                              |
| backbone.layers.0.blocks.0.norm2.*                                           | backbone.layers.0.blocks.0.norm2.{bias,weight}                                                                                         | (192,) (192,)                                              |
| backbone.layers.0.blocks.1.attn.*                                            | backbone.layers.0.blocks.1.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (192,) (192,192) (576,) (576,192) (529,6) (144,144)        |
| backbone.layers.0.blocks.1.mlp.fc1.*                                         | backbone.layers.0.blocks.1.mlp.fc1.{bias,weight}                                                                                       | (768,) (768,192)                                           |
| backbone.layers.0.blocks.1.mlp.fc2.*                                         | backbone.layers.0.blocks.1.mlp.fc2.{bias,weight}                                                                                       | (192,) (192,768)                                           |
| backbone.layers.0.blocks.1.norm1.*                                           | backbone.layers.0.blocks.1.norm1.{bias,weight}                                                                                         | (192,) (192,)                                              |
| backbone.layers.0.blocks.1.norm2.*                                           | backbone.layers.0.blocks.1.norm2.{bias,weight}                                                                                         | (192,) (192,)                                              |
| backbone.layers.0.downsample.norm.*                                          | backbone.layers.0.downsample.norm.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.0.downsample.reduction.weight                                | backbone.layers.0.downsample.reduction.weight                                                                                          | (384, 768)                                                 |
| backbone.layers.0.semantic_layer.class_injection.0.*                         | backbone.layers.0.semantic_layer.class_injection.0.{gamma,mlp_res.bias,mlp_res.weight,mlp_v.bias,mlp_v.weight}                         | (1,) (192,) (192,192) (192,) (192,192)                     |
| backbone.layers.0.semantic_layer.norm1.*                                     | backbone.layers.0.semantic_layer.norm1.{bias,weight}                                                                                   | (192,) (192,)                                              |
| backbone.layers.1.blocks.0.attn.*                                            | backbone.layers.1.blocks.0.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (384,) (384,384) (1152,) (1152,384) (529,12) (144,144)     |
| backbone.layers.1.blocks.0.mlp.fc1.*                                         | backbone.layers.1.blocks.0.mlp.fc1.{bias,weight}                                                                                       | (1536,) (1536,384)                                         |
| backbone.layers.1.blocks.0.mlp.fc2.*                                         | backbone.layers.1.blocks.0.mlp.fc2.{bias,weight}                                                                                       | (384,) (384,1536)                                          |
| backbone.layers.1.blocks.0.norm1.*                                           | backbone.layers.1.blocks.0.norm1.{bias,weight}                                                                                         | (384,) (384,)                                              |
| backbone.layers.1.blocks.0.norm2.*                                           | backbone.layers.1.blocks.0.norm2.{bias,weight}                                                                                         | (384,) (384,)                                              |
| backbone.layers.1.blocks.1.attn.*                                            | backbone.layers.1.blocks.1.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (384,) (384,384) (1152,) (1152,384) (529,12) (144,144)     |
| backbone.layers.1.blocks.1.mlp.fc1.*                                         | backbone.layers.1.blocks.1.mlp.fc1.{bias,weight}                                                                                       | (1536,) (1536,384)                                         |
| backbone.layers.1.blocks.1.mlp.fc2.*                                         | backbone.layers.1.blocks.1.mlp.fc2.{bias,weight}                                                                                       | (384,) (384,1536)                                          |
| backbone.layers.1.blocks.1.norm1.*                                           | backbone.layers.1.blocks.1.norm1.{bias,weight}                                                                                         | (384,) (384,)                                              |
| backbone.layers.1.blocks.1.norm2.*                                           | backbone.layers.1.blocks.1.norm2.{bias,weight}                                                                                         | (384,) (384,)                                              |
| backbone.layers.1.downsample.norm.*                                          | backbone.layers.1.downsample.norm.{bias,weight}                                                                                        | (1536,) (1536,)                                            |
| backbone.layers.1.downsample.reduction.weight                                | backbone.layers.1.downsample.reduction.weight                                                                                          | (768, 1536)                                                |
| backbone.layers.1.semantic_layer.class_injection.0.*                         | backbone.layers.1.semantic_layer.class_injection.0.{gamma,mlp_res.bias,mlp_res.weight,mlp_v.bias,mlp_v.weight}                         | (1,) (384,) (384,384) (384,) (384,384)                     |
| backbone.layers.1.semantic_layer.norm1.*                                     | backbone.layers.1.semantic_layer.norm1.{bias,weight}                                                                                   | (384,) (384,)                                              |
| backbone.layers.2.blocks.0.attn.*                                            | backbone.layers.2.blocks.0.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.0.mlp.fc1.*                                         | backbone.layers.2.blocks.0.mlp.fc1.{bias,weight}                                                                                       | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.0.mlp.fc2.*                                         | backbone.layers.2.blocks.0.mlp.fc2.{bias,weight}                                                                                       | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.0.norm1.*                                           | backbone.layers.2.blocks.0.norm1.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.0.norm2.*                                           | backbone.layers.2.blocks.0.norm2.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.1.attn.*                                            | backbone.layers.2.blocks.1.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.1.mlp.fc1.*                                         | backbone.layers.2.blocks.1.mlp.fc1.{bias,weight}                                                                                       | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.1.mlp.fc2.*                                         | backbone.layers.2.blocks.1.mlp.fc2.{bias,weight}                                                                                       | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.1.norm1.*                                           | backbone.layers.2.blocks.1.norm1.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.1.norm2.*                                           | backbone.layers.2.blocks.1.norm2.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.10.attn.*                                           | backbone.layers.2.blocks.10.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}      | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.10.mlp.fc1.*                                        | backbone.layers.2.blocks.10.mlp.fc1.{bias,weight}                                                                                      | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.10.mlp.fc2.*                                        | backbone.layers.2.blocks.10.mlp.fc2.{bias,weight}                                                                                      | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.10.norm1.*                                          | backbone.layers.2.blocks.10.norm1.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.10.norm2.*                                          | backbone.layers.2.blocks.10.norm2.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.11.attn.*                                           | backbone.layers.2.blocks.11.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}      | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.11.mlp.fc1.*                                        | backbone.layers.2.blocks.11.mlp.fc1.{bias,weight}                                                                                      | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.11.mlp.fc2.*                                        | backbone.layers.2.blocks.11.mlp.fc2.{bias,weight}                                                                                      | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.11.norm1.*                                          | backbone.layers.2.blocks.11.norm1.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.11.norm2.*                                          | backbone.layers.2.blocks.11.norm2.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.12.attn.*                                           | backbone.layers.2.blocks.12.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}      | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.12.mlp.fc1.*                                        | backbone.layers.2.blocks.12.mlp.fc1.{bias,weight}                                                                                      | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.12.mlp.fc2.*                                        | backbone.layers.2.blocks.12.mlp.fc2.{bias,weight}                                                                                      | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.12.norm1.*                                          | backbone.layers.2.blocks.12.norm1.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.12.norm2.*                                          | backbone.layers.2.blocks.12.norm2.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.13.attn.*                                           | backbone.layers.2.blocks.13.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}      | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.13.mlp.fc1.*                                        | backbone.layers.2.blocks.13.mlp.fc1.{bias,weight}                                                                                      | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.13.mlp.fc2.*                                        | backbone.layers.2.blocks.13.mlp.fc2.{bias,weight}                                                                                      | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.13.norm1.*                                          | backbone.layers.2.blocks.13.norm1.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.13.norm2.*                                          | backbone.layers.2.blocks.13.norm2.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.14.attn.*                                           | backbone.layers.2.blocks.14.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}      | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.14.mlp.fc1.*                                        | backbone.layers.2.blocks.14.mlp.fc1.{bias,weight}                                                                                      | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.14.mlp.fc2.*                                        | backbone.layers.2.blocks.14.mlp.fc2.{bias,weight}                                                                                      | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.14.norm1.*                                          | backbone.layers.2.blocks.14.norm1.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.14.norm2.*                                          | backbone.layers.2.blocks.14.norm2.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.15.attn.*                                           | backbone.layers.2.blocks.15.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}      | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.15.mlp.fc1.*                                        | backbone.layers.2.blocks.15.mlp.fc1.{bias,weight}                                                                                      | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.15.mlp.fc2.*                                        | backbone.layers.2.blocks.15.mlp.fc2.{bias,weight}                                                                                      | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.15.norm1.*                                          | backbone.layers.2.blocks.15.norm1.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.15.norm2.*                                          | backbone.layers.2.blocks.15.norm2.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.16.attn.*                                           | backbone.layers.2.blocks.16.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}      | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.16.mlp.fc1.*                                        | backbone.layers.2.blocks.16.mlp.fc1.{bias,weight}                                                                                      | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.16.mlp.fc2.*                                        | backbone.layers.2.blocks.16.mlp.fc2.{bias,weight}                                                                                      | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.16.norm1.*                                          | backbone.layers.2.blocks.16.norm1.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.16.norm2.*                                          | backbone.layers.2.blocks.16.norm2.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.17.attn.*                                           | backbone.layers.2.blocks.17.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}      | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.17.mlp.fc1.*                                        | backbone.layers.2.blocks.17.mlp.fc1.{bias,weight}                                                                                      | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.17.mlp.fc2.*                                        | backbone.layers.2.blocks.17.mlp.fc2.{bias,weight}                                                                                      | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.17.norm1.*                                          | backbone.layers.2.blocks.17.norm1.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.17.norm2.*                                          | backbone.layers.2.blocks.17.norm2.{bias,weight}                                                                                        | (768,) (768,)                                              |
| backbone.layers.2.blocks.2.attn.*                                            | backbone.layers.2.blocks.2.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.2.mlp.fc1.*                                         | backbone.layers.2.blocks.2.mlp.fc1.{bias,weight}                                                                                       | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.2.mlp.fc2.*                                         | backbone.layers.2.blocks.2.mlp.fc2.{bias,weight}                                                                                       | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.2.norm1.*                                           | backbone.layers.2.blocks.2.norm1.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.2.norm2.*                                           | backbone.layers.2.blocks.2.norm2.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.3.attn.*                                            | backbone.layers.2.blocks.3.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.3.mlp.fc1.*                                         | backbone.layers.2.blocks.3.mlp.fc1.{bias,weight}                                                                                       | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.3.mlp.fc2.*                                         | backbone.layers.2.blocks.3.mlp.fc2.{bias,weight}                                                                                       | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.3.norm1.*                                           | backbone.layers.2.blocks.3.norm1.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.3.norm2.*                                           | backbone.layers.2.blocks.3.norm2.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.4.attn.*                                            | backbone.layers.2.blocks.4.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.4.mlp.fc1.*                                         | backbone.layers.2.blocks.4.mlp.fc1.{bias,weight}                                                                                       | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.4.mlp.fc2.*                                         | backbone.layers.2.blocks.4.mlp.fc2.{bias,weight}                                                                                       | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.4.norm1.*                                           | backbone.layers.2.blocks.4.norm1.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.4.norm2.*                                           | backbone.layers.2.blocks.4.norm2.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.5.attn.*                                            | backbone.layers.2.blocks.5.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.5.mlp.fc1.*                                         | backbone.layers.2.blocks.5.mlp.fc1.{bias,weight}                                                                                       | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.5.mlp.fc2.*                                         | backbone.layers.2.blocks.5.mlp.fc2.{bias,weight}                                                                                       | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.5.norm1.*                                           | backbone.layers.2.blocks.5.norm1.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.5.norm2.*                                           | backbone.layers.2.blocks.5.norm2.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.6.attn.*                                            | backbone.layers.2.blocks.6.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.6.mlp.fc1.*                                         | backbone.layers.2.blocks.6.mlp.fc1.{bias,weight}                                                                                       | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.6.mlp.fc2.*                                         | backbone.layers.2.blocks.6.mlp.fc2.{bias,weight}                                                                                       | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.6.norm1.*                                           | backbone.layers.2.blocks.6.norm1.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.6.norm2.*                                           | backbone.layers.2.blocks.6.norm2.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.7.attn.*                                            | backbone.layers.2.blocks.7.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.7.mlp.fc1.*                                         | backbone.layers.2.blocks.7.mlp.fc1.{bias,weight}                                                                                       | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.7.mlp.fc2.*                                         | backbone.layers.2.blocks.7.mlp.fc2.{bias,weight}                                                                                       | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.7.norm1.*                                           | backbone.layers.2.blocks.7.norm1.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.7.norm2.*                                           | backbone.layers.2.blocks.7.norm2.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.8.attn.*                                            | backbone.layers.2.blocks.8.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.8.mlp.fc1.*                                         | backbone.layers.2.blocks.8.mlp.fc1.{bias,weight}                                                                                       | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.8.mlp.fc2.*                                         | backbone.layers.2.blocks.8.mlp.fc2.{bias,weight}                                                                                       | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.8.norm1.*                                           | backbone.layers.2.blocks.8.norm1.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.8.norm2.*                                           | backbone.layers.2.blocks.8.norm2.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.9.attn.*                                            | backbone.layers.2.blocks.9.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| backbone.layers.2.blocks.9.mlp.fc1.*                                         | backbone.layers.2.blocks.9.mlp.fc1.{bias,weight}                                                                                       | (3072,) (3072,768)                                         |
| backbone.layers.2.blocks.9.mlp.fc2.*                                         | backbone.layers.2.blocks.9.mlp.fc2.{bias,weight}                                                                                       | (768,) (768,3072)                                          |
| backbone.layers.2.blocks.9.norm1.*                                           | backbone.layers.2.blocks.9.norm1.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.blocks.9.norm2.*                                           | backbone.layers.2.blocks.9.norm2.{bias,weight}                                                                                         | (768,) (768,)                                              |
| backbone.layers.2.downsample.norm.*                                          | backbone.layers.2.downsample.norm.{bias,weight}                                                                                        | (3072,) (3072,)                                            |
| backbone.layers.2.downsample.reduction.weight                                | backbone.layers.2.downsample.reduction.weight                                                                                          | (1536, 3072)                                               |
| backbone.layers.2.semantic_layer.class_injection.0.*                         | backbone.layers.2.semantic_layer.class_injection.0.{gamma,mlp_res.bias,mlp_res.weight,mlp_v.bias,mlp_v.weight}                         | (1,) (768,) (768,768) (768,) (768,768)                     |
| backbone.layers.2.semantic_layer.norm1.*                                     | backbone.layers.2.semantic_layer.norm1.{bias,weight}                                                                                   | (768,) (768,)                                              |
| backbone.layers.3.blocks.0.attn.*                                            | backbone.layers.3.blocks.0.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (1536,) (1536,1536) (4608,) (4608,1536) (529,48) (144,144) |
| backbone.layers.3.blocks.0.mlp.fc1.*                                         | backbone.layers.3.blocks.0.mlp.fc1.{bias,weight}                                                                                       | (6144,) (6144,1536)                                        |
| backbone.layers.3.blocks.0.mlp.fc2.*                                         | backbone.layers.3.blocks.0.mlp.fc2.{bias,weight}                                                                                       | (1536,) (1536,6144)                                        |
| backbone.layers.3.blocks.0.norm1.*                                           | backbone.layers.3.blocks.0.norm1.{bias,weight}                                                                                         | (1536,) (1536,)                                            |
| backbone.layers.3.blocks.0.norm2.*                                           | backbone.layers.3.blocks.0.norm2.{bias,weight}                                                                                         | (1536,) (1536,)                                            |
| backbone.layers.3.blocks.1.attn.*                                            | backbone.layers.3.blocks.1.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}       | (1536,) (1536,1536) (4608,) (4608,1536) (529,48) (144,144) |
| backbone.layers.3.blocks.1.mlp.fc1.*                                         | backbone.layers.3.blocks.1.mlp.fc1.{bias,weight}                                                                                       | (6144,) (6144,1536)                                        |
| backbone.layers.3.blocks.1.mlp.fc2.*                                         | backbone.layers.3.blocks.1.mlp.fc2.{bias,weight}                                                                                       | (1536,) (1536,6144)                                        |
| backbone.layers.3.blocks.1.norm1.*                                           | backbone.layers.3.blocks.1.norm1.{bias,weight}                                                                                         | (1536,) (1536,)                                            |
| backbone.layers.3.blocks.1.norm2.*                                           | backbone.layers.3.blocks.1.norm2.{bias,weight}                                                                                         | (1536,) (1536,)                                            |
| backbone.layers.3.semantic_layer.class_injection.0.*                         | backbone.layers.3.semantic_layer.class_injection.0.{gamma,mlp_res.bias,mlp_res.weight,mlp_v.bias,mlp_v.weight}                         | (1,) (1536,) (1536,1536) (1536,) (1536,1536)               |
| backbone.layers.3.semantic_layer.norm1.*                                     | backbone.layers.3.semantic_layer.norm1.{bias,weight}                                                                                   | (1536,) (1536,)                                            |
| backbone.norm0.*                                                             | backbone.norm0.{bias,weight}                                                                                                           | (192,) (192,)                                              |
| backbone.norm1.*                                                             | backbone.norm1.{bias,weight}                                                                                                           | (384,) (384,)                                              |
| backbone.norm2.*                                                             | backbone.norm2.{bias,weight}                                                                                                           | (768,) (768,)                                              |
| backbone.norm3.*                                                             | backbone.norm3.{bias,weight}                                                                                                           | (1536,) (1536,)                                            |
| backbone.patch_embed.norm.*                                                  | backbone.patch_embed.norm.{bias,weight}                                                                                                | (192,) (192,)                                              |
| backbone.patch_embed.proj.*                                                  | backbone.patch_embed.proj.{bias,weight}                                                                                                | (192,) (192,3,4,4)                                         |
| sem_seg_head.pixel_decoder.adapter_1.*                                       | sem_seg_head.pixel_decoder.adapter_1.{norm.bias,norm.weight,weight}                                                                    | (256,) (256,) (256,192,1,1)                                |
| sem_seg_head.pixel_decoder.align_1.dcpack_L2.*                               | sem_seg_head.pixel_decoder.align_1.dcpack_L2.{bias,conv_offset_mask.bias,conv_offset_mask.weight,weight}                               | (256,) (216,) (216,256,3,3) (256,256,3,3)                  |
| sem_seg_head.pixel_decoder.align_1.offset.*                                  | sem_seg_head.pixel_decoder.align_1.offset.{norm.bias,norm.weight,weight}                                                               | (256,) (256,) (256,512,1,1)                                |
| sem_seg_head.pixel_decoder.layer_1.*                                         | sem_seg_head.pixel_decoder.layer_1.{norm.bias,norm.weight,weight}                                                                      | (256,) (256,) (256,256,3,3)                                |
| sem_seg_head.pixel_decoder.mask_features.bias                                | sem_seg_head.pixel_decoder.mask_features.bias                                                                                          | (256,)                                                     |
| sem_seg_head.predictor.decoder_norm.*                                        | sem_seg_head.predictor.decoder_norm.{bias,weight}                                                                                      | (256,) (256,)                                              |
| sem_seg_head.predictor.level_embed.weight                                    | sem_seg_head.predictor.level_embed.weight                                                                                              | (3, 256)                                                   |
| sem_seg_head.predictor.mask_embed.layers.0.*                                 | sem_seg_head.predictor.mask_embed.layers.0.{bias,weight}                                                                               | (256,) (256,256)                                           |
| sem_seg_head.predictor.mask_embed.layers.1.*                                 | sem_seg_head.predictor.mask_embed.layers.1.{bias,weight}                                                                               | (256,) (256,256)                                           |
| sem_seg_head.predictor.mask_embed.layers.2.*                                 | sem_seg_head.predictor.mask_embed.layers.2.{bias,weight}                                                                               | (256,) (256,256)                                           |
| sem_seg_head.predictor.query_embed.weight                                    | sem_seg_head.predictor.query_embed.weight                                                                                              | (100, 256)                                                 |
| sem_seg_head.predictor.query_feat.weight                                     | sem_seg_head.predictor.query_feat.weight                                                                                               | (100, 256)                                                 |
| sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.* | sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.*           | sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias,weight}                                                         | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.* | sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.*           | sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias,weight}                                                         | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.* | sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.*           | sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias,weight}                                                         | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.* | sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.*           | sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias,weight}                                                         | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.* | sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.*           | sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias,weight}                                                         | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.* | sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.*           | sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias,weight}                                                         | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.* | sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.*           | sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias,weight}                                                         | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.* | sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.*           | sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias,weight}                                                         | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.* | sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight} | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.*           | sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias,weight}                                                         | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_ffn_layers.0.linear1.*                    | sem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias,weight}                                                                  | (2048,) (2048,256)                                         |
| sem_seg_head.predictor.transformer_ffn_layers.0.linear2.*                    | sem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias,weight}                                                                  | (256,) (256,2048)                                          |
| sem_seg_head.predictor.transformer_ffn_layers.0.norm.*                       | sem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias,weight}                                                                     | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_ffn_layers.1.linear1.*                    | sem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias,weight}                                                                  | (2048,) (2048,256)                                         |
| sem_seg_head.predictor.transformer_ffn_layers.1.linear2.*                    | sem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias,weight}                                                                  | (256,) (256,2048)                                          |
| sem_seg_head.predictor.transformer_ffn_layers.1.norm.*                       | sem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias,weight}                                                                     | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_ffn_layers.2.linear1.*                    | sem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias,weight}                                                                  | (2048,) (2048,256)                                         |
| sem_seg_head.predictor.transformer_ffn_layers.2.linear2.*                    | sem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias,weight}                                                                  | (256,) (256,2048)                                          |
| sem_seg_head.predictor.transformer_ffn_layers.2.norm.*                       | sem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias,weight}                                                                     | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_ffn_layers.3.linear1.*                    | sem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias,weight}                                                                  | (2048,) (2048,256)                                         |
| sem_seg_head.predictor.transformer_ffn_layers.3.linear2.*                    | sem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias,weight}                                                                  | (256,) (256,2048)                                          |
| sem_seg_head.predictor.transformer_ffn_layers.3.norm.*                       | sem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias,weight}                                                                     | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_ffn_layers.4.linear1.*                    | sem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias,weight}                                                                  | (2048,) (2048,256)                                         |
| sem_seg_head.predictor.transformer_ffn_layers.4.linear2.*                    | sem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias,weight}                                                                  | (256,) (256,2048)                                          |
| sem_seg_head.predictor.transformer_ffn_layers.4.norm.*                       | sem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias,weight}                                                                     | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_ffn_layers.5.linear1.*                    | sem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias,weight}                                                                  | (2048,) (2048,256)                                         |
| sem_seg_head.predictor.transformer_ffn_layers.5.linear2.*                    | sem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias,weight}                                                                  | (256,) (256,2048)                                          |
| sem_seg_head.predictor.transformer_ffn_layers.5.norm.*                       | sem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias,weight}                                                                     | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_ffn_layers.6.linear1.*                    | sem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias,weight}                                                                  | (2048,) (2048,256)                                         |
| sem_seg_head.predictor.transformer_ffn_layers.6.linear2.*                    | sem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias,weight}                                                                  | (256,) (256,2048)                                          |
| sem_seg_head.predictor.transformer_ffn_layers.6.norm.*                       | sem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias,weight}                                                                     | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_ffn_layers.7.linear1.*                    | sem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias,weight}                                                                  | (2048,) (2048,256)                                         |
| sem_seg_head.predictor.transformer_ffn_layers.7.linear2.*                    | sem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias,weight}                                                                  | (256,) (256,2048)                                          |
| sem_seg_head.predictor.transformer_ffn_layers.7.norm.*                       | sem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias,weight}                                                                     | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_ffn_layers.8.linear1.*                    | sem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias,weight}                                                                  | (2048,) (2048,256)                                         |
| sem_seg_head.predictor.transformer_ffn_layers.8.linear2.*                    | sem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias,weight}                                                                  | (256,) (256,2048)                                          |
| sem_seg_head.predictor.transformer_ffn_layers.8.norm.*                       | sem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias,weight}                                                                     | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_self_attention_layers.0.norm.*            | sem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias,weight}                                                          | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.*       | sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight}       | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_self_attention_layers.1.norm.*            | sem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias,weight}                                                          | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.*       | sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight}       | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_self_attention_layers.2.norm.*            | sem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias,weight}                                                          | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.*       | sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight}       | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_self_attention_layers.3.norm.*            | sem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias,weight}                                                          | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.*       | sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight}       | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_self_attention_layers.4.norm.*            | sem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias,weight}                                                          | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.*       | sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight}       | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_self_attention_layers.5.norm.*            | sem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias,weight}                                                          | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.*       | sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight}       | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_self_attention_layers.6.norm.*            | sem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias,weight}                                                          | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.*       | sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight}       | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_self_attention_layers.7.norm.*            | sem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias,weight}                                                          | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.*       | sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight}       | (768,) (768,256) (256,) (256,256)                          |
| sem_seg_head.predictor.transformer_self_attention_layers.8.norm.*            | sem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias,weight}                                                          | (256,) (256,)                                              |
| sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.*       | sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias,in_proj_weight,out_proj.bias,out_proj.weight}       | (768,) (768,256) (256,) (256,256)                          |
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_k.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_k.weight' to the model due to incompatible shapes: (150, 192) in the checkpoint but (11, 192) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_q.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_q.weight' to the model due to incompatible shapes: (150, 192) in the checkpoint but (11, 192) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_k.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_k.weight' to the model due to incompatible shapes: (150, 384) in the checkpoint but (11, 384) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_q.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_q.weight' to the model due to incompatible shapes: (150, 384) in the checkpoint but (11, 384) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_k.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_k.weight' to the model due to incompatible shapes: (150, 768) in the checkpoint but (11, 768) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_q.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_q.weight' to the model due to incompatible shapes: (150, 768) in the checkpoint but (11, 768) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_k.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_k.weight' to the model due to incompatible shapes: (150, 1536) in the checkpoint but (11, 1536) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_q.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_q.weight' to the model due to incompatible shapes: (150, 1536) in the checkpoint but (11, 1536) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (151,) in the checkpoint but (12,) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'sem_seg_head.pixel_decoder.mask_features.weight' to the model due to incompatible shapes: (256, 256, 1, 1) in the checkpoint but (256, 256, 3, 3) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'sem_seg_head.predictor.class_embed.bias' to the model due to incompatible shapes: (151,) in the checkpoint but (12,) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Skip loading parameter 'sem_seg_head.predictor.class_embed.weight' to the model due to incompatible shapes: (151, 256) in the checkpoint but (12, 256) in the model! You might want to double check if this is expected.
[01/01 10:31:44] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.layers.0.semantic_layer.class_injection.0.mlp_cls_k.{bias, weight}[0m
[34mbackbone.layers.0.semantic_layer.class_injection.0.mlp_cls_q.{bias, weight}[0m
[34mbackbone.layers.1.semantic_layer.class_injection.0.mlp_cls_k.{bias, weight}[0m
[34mbackbone.layers.1.semantic_layer.class_injection.0.mlp_cls_q.{bias, weight}[0m
[34mbackbone.layers.2.semantic_layer.class_injection.0.mlp_cls_k.{bias, weight}[0m
[34mbackbone.layers.2.semantic_layer.class_injection.0.mlp_cls_q.{bias, weight}[0m
[34mbackbone.layers.3.semantic_layer.class_injection.0.mlp_cls_k.{bias, weight}[0m
[34mbackbone.layers.3.semantic_layer.class_injection.0.mlp_cls_q.{bias, weight}[0m
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.adapter_2.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_2.weight[0m
[34msem_seg_head.pixel_decoder.adapter_3.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_3.weight[0m
[34msem_seg_head.pixel_decoder.align_1.dcpack_L2.conv_offset_mask.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.align_1.dcpack_L2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.align_1.offset.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.align_1.offset.weight[0m
[34msem_seg_head.pixel_decoder.align_2.dcpack_L2.conv_offset_mask.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.align_2.dcpack_L2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.align_2.offset.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.align_2.offset.weight[0m
[34msem_seg_head.pixel_decoder.align_3.dcpack_L2.conv_offset_mask.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.align_3.dcpack_L2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.align_3.offset.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.align_3.offset.weight[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.layer_2.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_2.weight[0m
[34msem_seg_head.pixel_decoder.layer_3.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_3.weight[0m
[34msem_seg_head.pixel_decoder.layer_4.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_4.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[01/01 10:31:44] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35msem_seg_head.pixel_decoder.pixel_decoder.adapter_1.norm.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.adapter_1.weight[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.align_1.dcpack_L2.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.align_1.dcpack_L2.conv_offset_mask.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.align_1.offset.norm.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.align_1.offset.weight[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.layer_1.norm.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.layer_1.weight[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.mask_features.bias[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.input_proj.0.0.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.input_proj.0.1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.input_proj.1.0.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.input_proj.1.1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.input_proj.2.0.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.input_proj.2.1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
  [35msem_seg_head.pixel_decoder.pixel_decoder.transformer.level_embed[0m
[01/01 10:31:44] d2.engine.train_loop INFO: Starting training from iteration 0
[01/01 10:32:13] d2.utils.events INFO:  eta: 14:50:59  iter: 19  total_loss: 82.98  loss_ce: 2.513  loss_cate: 1.463  loss_mask: 2.294  loss_dice: 3.118  loss_ce_0: 5.618  loss_cate_0: 0  loss_mask_0: 2.272  loss_dice_0: 3.058  loss_ce_1: 2.422  loss_cate_1: 0  loss_mask_1: 2.316  loss_dice_1: 3.049  loss_ce_2: 2.245  loss_cate_2: 0  loss_mask_2: 2.272  loss_dice_2: 3.072  loss_ce_3: 2.347  loss_cate_3: 0  loss_mask_3: 2.357  loss_dice_3: 3.17  loss_ce_4: 2.404  loss_cate_4: 0  loss_mask_4: 2.314  loss_dice_4: 3.156  loss_ce_5: 2.404  loss_cate_5: 0  loss_mask_5: 2.31  loss_dice_5: 3.163  loss_ce_6: 2.448  loss_cate_6: 0  loss_mask_6: 2.276  loss_dice_6: 3.165  loss_ce_7: 2.443  loss_cate_7: 0  loss_mask_7: 2.348  loss_dice_7: 3.108  loss_ce_8: 2.532  loss_cate_8: 0  loss_mask_8: 2.304  loss_dice_8: 3.098  time: 1.3488  data_time: 0.0332  lr: 9.9956e-06  max_mem: 23753M
[01/01 10:32:41] d2.utils.events INFO:  eta: 14:57:15  iter: 39  total_loss: 73.95  loss_ce: 2.063  loss_cate: 1.046  loss_mask: 1.943  loss_dice: 2.75  loss_ce_0: 5.504  loss_cate_0: 0  loss_mask_0: 2.034  loss_dice_0: 2.63  loss_ce_1: 2.118  loss_cate_1: 0  loss_mask_1: 2  loss_dice_1: 2.507  loss_ce_2: 2.082  loss_cate_2: 0  loss_mask_2: 1.976  loss_dice_2: 2.689  loss_ce_3: 1.924  loss_cate_3: 0  loss_mask_3: 2.069  loss_dice_3: 2.758  loss_ce_4: 1.846  loss_cate_4: 0  loss_mask_4: 1.958  loss_dice_4: 2.978  loss_ce_5: 1.922  loss_cate_5: 0  loss_mask_5: 1.86  loss_dice_5: 2.878  loss_ce_6: 1.88  loss_cate_6: 0  loss_mask_6: 2.006  loss_dice_6: 2.855  loss_ce_7: 1.928  loss_cate_7: 0  loss_mask_7: 1.919  loss_dice_7: 2.892  loss_ce_8: 1.938  loss_cate_8: 0  loss_mask_8: 1.889  loss_dice_8: 2.853  time: 1.3706  data_time: 0.0117  lr: 9.991e-06  max_mem: 23758M
[01/01 10:33:09] d2.utils.events INFO:  eta: 14:57:08  iter: 59  total_loss: 56.48  loss_ce: 1.59  loss_cate: 0.7331  loss_mask: 1.436  loss_dice: 2.068  loss_ce_0: 5.198  loss_cate_0: 0  loss_mask_0: 1.432  loss_dice_0: 1.844  loss_ce_1: 1.831  loss_cate_1: 0  loss_mask_1: 1.534  loss_dice_1: 1.945  loss_ce_2: 1.64  loss_cate_2: 0  loss_mask_2: 1.598  loss_dice_2: 2.013  loss_ce_3: 1.594  loss_cate_3: 0  loss_mask_3: 1.507  loss_dice_3: 1.942  loss_ce_4: 1.536  loss_cate_4: 0  loss_mask_4: 1.443  loss_dice_4: 1.98  loss_ce_5: 1.511  loss_cate_5: 0  loss_mask_5: 1.523  loss_dice_5: 1.992  loss_ce_6: 1.474  loss_cate_6: 0  loss_mask_6: 1.515  loss_dice_6: 2.21  loss_ce_7: 1.536  loss_cate_7: 0  loss_mask_7: 1.536  loss_dice_7: 2.025  loss_ce_8: 1.518  loss_cate_8: 0  loss_mask_8: 1.576  loss_dice_8: 2.139  time: 1.3730  data_time: 0.0113  lr: 9.9864e-06  max_mem: 23758M
[01/01 10:33:36] d2.utils.events INFO:  eta: 14:56:40  iter: 79  total_loss: 56.54  loss_ce: 1.51  loss_cate: 0.6353  loss_mask: 1.432  loss_dice: 2.432  loss_ce_0: 4.937  loss_cate_0: 0  loss_mask_0: 1.385  loss_dice_0: 2.199  loss_ce_1: 1.908  loss_cate_1: 0  loss_mask_1: 1.336  loss_dice_1: 2.173  loss_ce_2: 1.632  loss_cate_2: 0  loss_mask_2: 1.42  loss_dice_2: 2.259  loss_ce_3: 1.574  loss_cate_3: 0  loss_mask_3: 1.43  loss_dice_3: 2.328  loss_ce_4: 1.574  loss_cate_4: 0  loss_mask_4: 1.379  loss_dice_4: 2.266  loss_ce_5: 1.488  loss_cate_5: 0  loss_mask_5: 1.418  loss_dice_5: 2.303  loss_ce_6: 1.399  loss_cate_6: 0  loss_mask_6: 1.477  loss_dice_6: 2.371  loss_ce_7: 1.379  loss_cate_7: 0  loss_mask_7: 1.44  loss_dice_7: 2.379  loss_ce_8: 1.395  loss_cate_8: 0  loss_mask_8: 1.389  loss_dice_8: 2.343  time: 1.3754  data_time: 0.0101  lr: 9.9818e-06  max_mem: 23758M
[01/01 10:34:04] d2.utils.events INFO:  eta: 14:56:02  iter: 99  total_loss: 48.7  loss_ce: 1.409  loss_cate: 0.5949  loss_mask: 1.294  loss_dice: 1.766  loss_ce_0: 4.815  loss_cate_0: 0  loss_mask_0: 1.342  loss_dice_0: 1.698  loss_ce_1: 1.678  loss_cate_1: 0  loss_mask_1: 1.218  loss_dice_1: 1.761  loss_ce_2: 1.527  loss_cate_2: 0  loss_mask_2: 1.271  loss_dice_2: 1.856  loss_ce_3: 1.555  loss_cate_3: 0  loss_mask_3: 1.307  loss_dice_3: 1.798  loss_ce_4: 1.459  loss_cate_4: 0  loss_mask_4: 1.245  loss_dice_4: 1.761  loss_ce_5: 1.392  loss_cate_5: 0  loss_mask_5: 1.299  loss_dice_5: 1.859  loss_ce_6: 1.395  loss_cate_6: 0  loss_mask_6: 1.265  loss_dice_6: 1.782  loss_ce_7: 1.405  loss_cate_7: 0  loss_mask_7: 1.31  loss_dice_7: 1.748  loss_ce_8: 1.435  loss_cate_8: 0  loss_mask_8: 1.264  loss_dice_8: 1.777  time: 1.3755  data_time: 0.0101  lr: 9.9772e-06  max_mem: 23765M
[01/01 10:34:31] d2.utils.events INFO:  eta: 14:55:12  iter: 119  total_loss: 49.34  loss_ce: 1.208  loss_cate: 0.4377  loss_mask: 1.136  loss_dice: 1.958  loss_ce_0: 4.507  loss_cate_0: 0  loss_mask_0: 1.101  loss_dice_0: 1.809  loss_ce_1: 1.529  loss_cate_1: 0  loss_mask_1: 1.152  loss_dice_1: 1.87  loss_ce_2: 1.305  loss_cate_2: 0  loss_mask_2: 1.148  loss_dice_2: 1.87  loss_ce_3: 1.382  loss_cate_3: 0  loss_mask_3: 1.143  loss_dice_3: 1.752  loss_ce_4: 1.303  loss_cate_4: 0  loss_mask_4: 1.264  loss_dice_4: 1.822  loss_ce_5: 1.303  loss_cate_5: 0  loss_mask_5: 1.217  loss_dice_5: 1.858  loss_ce_6: 1.299  loss_cate_6: 0  loss_mask_6: 1.104  loss_dice_6: 1.955  loss_ce_7: 1.308  loss_cate_7: 0  loss_mask_7: 1.125  loss_dice_7: 1.921  loss_ce_8: 1.283  loss_cate_8: 0  loss_mask_8: 1.187  loss_dice_8: 1.878  time: 1.3762  data_time: 0.0100  lr: 9.9725e-06  max_mem: 23765M
[01/01 10:34:59] d2.utils.events INFO:  eta: 14:54:38  iter: 139  total_loss: 43.45  loss_ce: 1.011  loss_cate: 0.5215  loss_mask: 1.13  loss_dice: 1.767  loss_ce_0: 4.336  loss_cate_0: 0  loss_mask_0: 1.178  loss_dice_0: 1.483  loss_ce_1: 1.415  loss_cate_1: 0  loss_mask_1: 1.266  loss_dice_1: 1.658  loss_ce_2: 1.271  loss_cate_2: 0  loss_mask_2: 1.287  loss_dice_2: 1.768  loss_ce_3: 1.152  loss_cate_3: 0  loss_mask_3: 1.107  loss_dice_3: 1.815  loss_ce_4: 1.15  loss_cate_4: 0  loss_mask_4: 1.121  loss_dice_4: 1.742  loss_ce_5: 1.136  loss_cate_5: 0  loss_mask_5: 1.166  loss_dice_5: 1.708  loss_ce_6: 1.107  loss_cate_6: 0  loss_mask_6: 1.23  loss_dice_6: 1.694  loss_ce_7: 1.086  loss_cate_7: 0  loss_mask_7: 1.221  loss_dice_7: 1.695  loss_ce_8: 1.081  loss_cate_8: 0  loss_mask_8: 1.167  loss_dice_8: 1.688  time: 1.3780  data_time: 0.0110  lr: 9.9679e-06  max_mem: 23765M
[01/01 10:35:27] d2.utils.events INFO:  eta: 14:54:11  iter: 159  total_loss: 38.29  loss_ce: 0.9362  loss_cate: 0.4555  loss_mask: 1.032  loss_dice: 1.272  loss_ce_0: 4.146  loss_cate_0: 0  loss_mask_0: 0.9081  loss_dice_0: 1.279  loss_ce_1: 1.258  loss_cate_1: 0  loss_mask_1: 0.9161  loss_dice_1: 1.276  loss_ce_2: 1.062  loss_cate_2: 0  loss_mask_2: 1.039  loss_dice_2: 1.326  loss_ce_3: 1.169  loss_cate_3: 0  loss_mask_3: 0.8654  loss_dice_3: 1.329  loss_ce_4: 1.118  loss_cate_4: 0  loss_mask_4: 0.9477  loss_dice_4: 1.341  loss_ce_5: 1.051  loss_cate_5: 0  loss_mask_5: 1.061  loss_dice_5: 1.223  loss_ce_6: 0.9672  loss_cate_6: 0  loss_mask_6: 1.042  loss_dice_6: 1.273  loss_ce_7: 0.9814  loss_cate_7: 0  loss_mask_7: 1.001  loss_dice_7: 1.256  loss_ce_8: 0.9483  loss_cate_8: 0  loss_mask_8: 1.06  loss_dice_8: 1.279  time: 1.3779  data_time: 0.0130  lr: 9.9633e-06  max_mem: 23765M
[01/01 10:35:55] d2.utils.events INFO:  eta: 14:53:41  iter: 179  total_loss: 42.61  loss_ce: 1.176  loss_cate: 0.3815  loss_mask: 1.032  loss_dice: 1.637  loss_ce_0: 4.029  loss_cate_0: 0  loss_mask_0: 1.118  loss_dice_0: 1.504  loss_ce_1: 1.184  loss_cate_1: 0  loss_mask_1: 1.068  loss_dice_1: 1.511  loss_ce_2: 1.087  loss_cate_2: 0  loss_mask_2: 1.008  loss_dice_2: 1.761  loss_ce_3: 1.142  loss_cate_3: 0  loss_mask_3: 1.103  loss_dice_3: 1.609  loss_ce_4: 1.17  loss_cate_4: 0  loss_mask_4: 1.106  loss_dice_4: 1.63  loss_ce_5: 1.204  loss_cate_5: 0  loss_mask_5: 1.143  loss_dice_5: 1.615  loss_ce_6: 1.185  loss_cate_6: 0  loss_mask_6: 1.068  loss_dice_6: 1.637  loss_ce_7: 1.142  loss_cate_7: 0  loss_mask_7: 1.022  loss_dice_7: 1.62  loss_ce_8: 1.126  loss_cate_8: 0  loss_mask_8: 1.167  loss_dice_8: 1.665  time: 1.3785  data_time: 0.0130  lr: 9.9587e-06  max_mem: 23765M
[01/01 10:36:22] d2.utils.events INFO:  eta: 14:53:05  iter: 199  total_loss: 43.19  loss_ce: 1.122  loss_cate: 0.427  loss_mask: 1.276  loss_dice: 1.656  loss_ce_0: 3.853  loss_cate_0: 0  loss_mask_0: 1.095  loss_dice_0: 1.513  loss_ce_1: 1.26  loss_cate_1: 0  loss_mask_1: 1.146  loss_dice_1: 1.575  loss_ce_2: 1.179  loss_cate_2: 0  loss_mask_2: 1.214  loss_dice_2: 1.737  loss_ce_3: 1.232  loss_cate_3: 0  loss_mask_3: 1.168  loss_dice_3: 1.771  loss_ce_4: 1.271  loss_cate_4: 0  loss_mask_4: 1.258  loss_dice_4: 1.843  loss_ce_5: 1.138  loss_cate_5: 0  loss_mask_5: 1.21  loss_dice_5: 1.776  loss_ce_6: 1.29  loss_cate_6: 0  loss_mask_6: 1.206  loss_dice_6: 1.598  loss_ce_7: 1.171  loss_cate_7: 0  loss_mask_7: 1.223  loss_dice_7: 1.637  loss_ce_8: 1.238  loss_cate_8: 0  loss_mask_8: 1.237  loss_dice_8: 1.731  time: 1.3780  data_time: 0.0112  lr: 9.9541e-06  max_mem: 23773M
[01/01 10:36:50] d2.utils.events INFO:  eta: 14:52:31  iter: 219  total_loss: 43.23  loss_ce: 1.012  loss_cate: 0.3813  loss_mask: 1.081  loss_dice: 1.818  loss_ce_0: 3.715  loss_cate_0: 0  loss_mask_0: 1.028  loss_dice_0: 1.485  loss_ce_1: 1.244  loss_cate_1: 0  loss_mask_1: 1.009  loss_dice_1: 1.71  loss_ce_2: 1.223  loss_cate_2: 0  loss_mask_2: 1.126  loss_dice_2: 1.71  loss_ce_3: 1.227  loss_cate_3: 0  loss_mask_3: 1.118  loss_dice_3: 1.658  loss_ce_4: 1.144  loss_cate_4: 0  loss_mask_4: 1.059  loss_dice_4: 1.728  loss_ce_5: 1.092  loss_cate_5: 0  loss_mask_5: 1.087  loss_dice_5: 1.711  loss_ce_6: 1.084  loss_cate_6: 0  loss_mask_6: 1.058  loss_dice_6: 1.786  loss_ce_7: 1.074  loss_cate_7: 0  loss_mask_7: 1.087  loss_dice_7: 1.778  loss_ce_8: 1.164  loss_cate_8: 0  loss_mask_8: 1.051  loss_dice_8: 1.77  time: 1.3780  data_time: 0.0113  lr: 9.9494e-06  max_mem: 23773M
[01/01 10:37:17] d2.utils.events INFO:  eta: 14:51:28  iter: 239  total_loss: 36.86  loss_ce: 0.9259  loss_cate: 0.3627  loss_mask: 1.154  loss_dice: 1.484  loss_ce_0: 3.427  loss_cate_0: 0  loss_mask_0: 0.9592  loss_dice_0: 1.307  loss_ce_1: 1.092  loss_cate_1: 0  loss_mask_1: 1.04  loss_dice_1: 1.445  loss_ce_2: 0.9733  loss_cate_2: 0  loss_mask_2: 0.9878  loss_dice_2: 1.399  loss_ce_3: 1.047  loss_cate_3: 0  loss_mask_3: 0.9511  loss_dice_3: 1.383  loss_ce_4: 0.9138  loss_cate_4: 0  loss_mask_4: 1.081  loss_dice_4: 1.419  loss_ce_5: 0.9654  loss_cate_5: 0  loss_mask_5: 0.9701  loss_dice_5: 1.253  loss_ce_6: 0.8668  loss_cate_6: 0  loss_mask_6: 1.13  loss_dice_6: 1.369  loss_ce_7: 0.9114  loss_cate_7: 0  loss_mask_7: 1.151  loss_dice_7: 1.361  loss_ce_8: 0.9478  loss_cate_8: 0  loss_mask_8: 1.195  loss_dice_8: 1.432  time: 1.3776  data_time: 0.0103  lr: 9.9448e-06  max_mem: 23773M
[01/01 10:37:45] d2.utils.events INFO:  eta: 14:50:52  iter: 259  total_loss: 40.85  loss_ce: 1.112  loss_cate: 0.3411  loss_mask: 1.046  loss_dice: 1.683  loss_ce_0: 3.333  loss_cate_0: 0  loss_mask_0: 1.057  loss_dice_0: 1.365  loss_ce_1: 1.173  loss_cate_1: 0  loss_mask_1: 1.09  loss_dice_1: 1.536  loss_ce_2: 1.203  loss_cate_2: 0  loss_mask_2: 1.016  loss_dice_2: 1.51  loss_ce_3: 1.071  loss_cate_3: 0  loss_mask_3: 1.053  loss_dice_3: 1.601  loss_ce_4: 1.081  loss_cate_4: 0  loss_mask_4: 1.153  loss_dice_4: 1.593  loss_ce_5: 1.146  loss_cate_5: 0  loss_mask_5: 1.062  loss_dice_5: 1.59  loss_ce_6: 1.168  loss_cate_6: 0  loss_mask_6: 1.054  loss_dice_6: 1.664  loss_ce_7: 1.216  loss_cate_7: 0  loss_mask_7: 1.045  loss_dice_7: 1.651  loss_ce_8: 1.077  loss_cate_8: 0  loss_mask_8: 1.051  loss_dice_8: 1.627  time: 1.3775  data_time: 0.0105  lr: 9.9402e-06  max_mem: 23773M
[01/01 10:38:13] d2.utils.events INFO:  eta: 14:50:25  iter: 279  total_loss: 36.81  loss_ce: 0.8344  loss_cate: 0.3818  loss_mask: 0.8726  loss_dice: 1.495  loss_ce_0: 3.156  loss_cate_0: 0  loss_mask_0: 1.016  loss_dice_0: 1.291  loss_ce_1: 1.073  loss_cate_1: 0  loss_mask_1: 0.9838  loss_dice_1: 1.459  loss_ce_2: 0.9945  loss_cate_2: 0  loss_mask_2: 0.9496  loss_dice_2: 1.417  loss_ce_3: 0.905  loss_cate_3: 0  loss_mask_3: 0.9271  loss_dice_3: 1.386  loss_ce_4: 0.8966  loss_cate_4: 0  loss_mask_4: 0.9302  loss_dice_4: 1.433  loss_ce_5: 0.8637  loss_cate_5: 0  loss_mask_5: 0.9907  loss_dice_5: 1.393  loss_ce_6: 0.8583  loss_cate_6: 0  loss_mask_6: 0.9634  loss_dice_6: 1.428  loss_ce_7: 0.7736  loss_cate_7: 0  loss_mask_7: 0.9494  loss_dice_7: 1.443  loss_ce_8: 0.8899  loss_cate_8: 0  loss_mask_8: 0.9565  loss_dice_8: 1.381  time: 1.3789  data_time: 0.0126  lr: 9.9356e-06  max_mem: 23773M
[01/01 10:38:41] d2.utils.events INFO:  eta: 14:49:55  iter: 299  total_loss: 31.38  loss_ce: 0.8049  loss_cate: 0.3522  loss_mask: 0.8653  loss_dice: 1.333  loss_ce_0: 3.018  loss_cate_0: 0  loss_mask_0: 1.047  loss_dice_0: 1.117  loss_ce_1: 0.9732  loss_cate_1: 0  loss_mask_1: 0.8676  loss_dice_1: 1.322  loss_ce_2: 0.9126  loss_cate_2: 0  loss_mask_2: 0.865  loss_dice_2: 1.265  loss_ce_3: 0.8984  loss_cate_3: 0  loss_mask_3: 0.891  loss_dice_3: 1.341  loss_ce_4: 0.8265  loss_cate_4: 0  loss_mask_4: 0.8724  loss_dice_4: 1.364  loss_ce_5: 0.8372  loss_cate_5: 0  loss_mask_5: 0.8559  loss_dice_5: 1.361  loss_ce_6: 0.796  loss_cate_6: 0  loss_mask_6: 0.9038  loss_dice_6: 1.322  loss_ce_7: 0.7863  loss_cate_7: 0  loss_mask_7: 0.8763  loss_dice_7: 1.286  loss_ce_8: 0.7182  loss_cate_8: 0  loss_mask_8: 0.851  loss_dice_8: 1.339  time: 1.3791  data_time: 0.0154  lr: 9.931e-06  max_mem: 23777M
[01/01 10:39:08] d2.utils.events INFO:  eta: 14:49:25  iter: 319  total_loss: 39.83  loss_ce: 0.93  loss_cate: 0.4402  loss_mask: 1.049  loss_dice: 1.713  loss_ce_0: 3.004  loss_cate_0: 0  loss_mask_0: 1.043  loss_dice_0: 1.488  loss_ce_1: 1.184  loss_cate_1: 0  loss_mask_1: 0.9864  loss_dice_1: 1.616  loss_ce_2: 1.062  loss_cate_2: 0  loss_mask_2: 1.026  loss_dice_2: 1.676  loss_ce_3: 1.096  loss_cate_3: 0  loss_mask_3: 1.075  loss_dice_3: 1.606  loss_ce_4: 1.109  loss_cate_4: 0  loss_mask_4: 1.023  loss_dice_4: 1.646  loss_ce_5: 1.073  loss_cate_5: 0  loss_mask_5: 0.9427  loss_dice_5: 1.591  loss_ce_6: 0.932  loss_cate_6: 0  loss_mask_6: 0.9261  loss_dice_6: 1.673  loss_ce_7: 0.8709  loss_cate_7: 0  loss_mask_7: 0.9939  loss_dice_7: 1.733  loss_ce_8: 0.9185  loss_cate_8: 0  loss_mask_8: 1.025  loss_dice_8: 1.754  time: 1.3792  data_time: 0.0125  lr: 9.9264e-06  max_mem: 23777M
[01/01 10:39:36] d2.utils.events INFO:  eta: 14:48:50  iter: 339  total_loss: 35.93  loss_ce: 0.8199  loss_cate: 0.3596  loss_mask: 0.9552  loss_dice: 1.471  loss_ce_0: 2.811  loss_cate_0: 0  loss_mask_0: 0.9829  loss_dice_0: 1.275  loss_ce_1: 1.11  loss_cate_1: 0  loss_mask_1: 0.8944  loss_dice_1: 1.383  loss_ce_2: 0.9868  loss_cate_2: 0  loss_mask_2: 0.9706  loss_dice_2: 1.444  loss_ce_3: 0.9777  loss_cate_3: 0  loss_mask_3: 0.9415  loss_dice_3: 1.431  loss_ce_4: 1.025  loss_cate_4: 0  loss_mask_4: 0.9633  loss_dice_4: 1.431  loss_ce_5: 0.9358  loss_cate_5: 0  loss_mask_5: 0.9444  loss_dice_5: 1.472  loss_ce_6: 0.8696  loss_cate_6: 0  loss_mask_6: 0.937  loss_dice_6: 1.474  loss_ce_7: 0.8484  loss_cate_7: 0  loss_mask_7: 0.974  loss_dice_7: 1.484  loss_ce_8: 0.8148  loss_cate_8: 0  loss_mask_8: 0.9794  loss_dice_8: 1.425  time: 1.3791  data_time: 0.0116  lr: 9.9217e-06  max_mem: 23777M
[01/01 10:40:04] d2.utils.events INFO:  eta: 14:48:29  iter: 359  total_loss: 40.15  loss_ce: 1.002  loss_cate: 0.3798  loss_mask: 1.081  loss_dice: 1.59  loss_ce_0: 2.814  loss_cate_0: 0  loss_mask_0: 1.045  loss_dice_0: 1.503  loss_ce_1: 1.108  loss_cate_1: 0  loss_mask_1: 0.9486  loss_dice_1: 1.472  loss_ce_2: 1.002  loss_cate_2: 0  loss_mask_2: 1.024  loss_dice_2: 1.489  loss_ce_3: 1.025  loss_cate_3: 0  loss_mask_3: 1.027  loss_dice_3: 1.436  loss_ce_4: 1.02  loss_cate_4: 0  loss_mask_4: 1.045  loss_dice_4: 1.484  loss_ce_5: 1.011  loss_cate_5: 0  loss_mask_5: 1.029  loss_dice_5: 1.471  loss_ce_6: 1.065  loss_cate_6: 0  loss_mask_6: 0.976  loss_dice_6: 1.504  loss_ce_7: 1  loss_cate_7: 0  loss_mask_7: 1.041  loss_dice_7: 1.54  loss_ce_8: 1.118  loss_cate_8: 0  loss_mask_8: 1.072  loss_dice_8: 1.535  time: 1.3793  data_time: 0.0126  lr: 9.9171e-06  max_mem: 23777M
[01/01 10:40:31] d2.utils.events INFO:  eta: 14:48:00  iter: 379  total_loss: 32.18  loss_ce: 0.8314  loss_cate: 0.273  loss_mask: 1.007  loss_dice: 1.229  loss_ce_0: 2.518  loss_cate_0: 0  loss_mask_0: 0.9558  loss_dice_0: 1.095  loss_ce_1: 0.9519  loss_cate_1: 0  loss_mask_1: 0.9262  loss_dice_1: 1.202  loss_ce_2: 0.8423  loss_cate_2: 0  loss_mask_2: 0.9893  loss_dice_2: 1.158  loss_ce_3: 0.8722  loss_cate_3: 0  loss_mask_3: 0.9948  loss_dice_3: 1.165  loss_ce_4: 0.8335  loss_cate_4: 0  loss_mask_4: 0.9539  loss_dice_4: 1.209  loss_ce_5: 0.7984  loss_cate_5: 0  loss_mask_5: 1.033  loss_dice_5: 1.167  loss_ce_6: 0.7891  loss_cate_6: 0  loss_mask_6: 1.002  loss_dice_6: 1.238  loss_ce_7: 0.7574  loss_cate_7: 0  loss_mask_7: 1.034  loss_dice_7: 1.245  loss_ce_8: 0.7682  loss_cate_8: 0  loss_mask_8: 1.011  loss_dice_8: 1.245  time: 1.3793  data_time: 0.0128  lr: 9.9125e-06  max_mem: 23777M
[01/01 10:40:59] d2.utils.events INFO:  eta: 14:47:31  iter: 399  total_loss: 35.35  loss_ce: 0.8384  loss_cate: 0.2753  loss_mask: 0.9695  loss_dice: 1.575  loss_ce_0: 2.433  loss_cate_0: 0  loss_mask_0: 0.8337  loss_dice_0: 1.393  loss_ce_1: 1.067  loss_cate_1: 0  loss_mask_1: 0.9452  loss_dice_1: 1.478  loss_ce_2: 0.9511  loss_cate_2: 0  loss_mask_2: 1.009  loss_dice_2: 1.421  loss_ce_3: 0.9368  loss_cate_3: 0  loss_mask_3: 0.9286  loss_dice_3: 1.491  loss_ce_4: 0.8521  loss_cate_4: 0  loss_mask_4: 0.9329  loss_dice_4: 1.537  loss_ce_5: 0.8137  loss_cate_5: 0  loss_mask_5: 0.9337  loss_dice_5: 1.496  loss_ce_6: 0.8239  loss_cate_6: 0  loss_mask_6: 0.959  loss_dice_6: 1.524  loss_ce_7: 0.7971  loss_cate_7: 0  loss_mask_7: 0.9372  loss_dice_7: 1.604  loss_ce_8: 0.9128  loss_cate_8: 0  loss_mask_8: 1.029  loss_dice_8: 1.549  time: 1.3793  data_time: 0.0130  lr: 9.9079e-06  max_mem: 23777M
[01/01 10:41:26] d2.utils.events INFO:  eta: 14:47:05  iter: 419  total_loss: 30.38  loss_ce: 0.831  loss_cate: 0.2815  loss_mask: 0.7974  loss_dice: 1.245  loss_ce_0: 2.373  loss_cate_0: 0  loss_mask_0: 0.7555  loss_dice_0: 1.192  loss_ce_1: 0.8602  loss_cate_1: 0  loss_mask_1: 0.7914  loss_dice_1: 1.227  loss_ce_2: 0.7805  loss_cate_2: 0  loss_mask_2: 0.8595  loss_dice_2: 1.288  loss_ce_3: 0.8634  loss_cate_3: 0  loss_mask_3: 0.8354  loss_dice_3: 1.181  loss_ce_4: 0.8397  loss_cate_4: 0  loss_mask_4: 0.8057  loss_dice_4: 1.314  loss_ce_5: 0.8577  loss_cate_5: 0  loss_mask_5: 0.8005  loss_dice_5: 1.252  loss_ce_6: 0.7772  loss_cate_6: 0  loss_mask_6: 0.8533  loss_dice_6: 1.251  loss_ce_7: 0.7669  loss_cate_7: 0  loss_mask_7: 0.9156  loss_dice_7: 1.271  loss_ce_8: 0.7457  loss_cate_8: 0  loss_mask_8: 0.7999  loss_dice_8: 1.274  time: 1.3794  data_time: 0.0108  lr: 9.9033e-06  max_mem: 23777M
[01/01 10:41:54] d2.utils.events INFO:  eta: 14:46:35  iter: 439  total_loss: 34.59  loss_ce: 0.9308  loss_cate: 0.3292  loss_mask: 0.8729  loss_dice: 1.428  loss_ce_0: 2.251  loss_cate_0: 0  loss_mask_0: 0.8923  loss_dice_0: 1.347  loss_ce_1: 1.082  loss_cate_1: 0  loss_mask_1: 0.8866  loss_dice_1: 1.344  loss_ce_2: 0.9779  loss_cate_2: 0  loss_mask_2: 0.8869  loss_dice_2: 1.383  loss_ce_3: 0.9623  loss_cate_3: 0  loss_mask_3: 0.8792  loss_dice_3: 1.352  loss_ce_4: 0.9003  loss_cate_4: 0  loss_mask_4: 0.8755  loss_dice_4: 1.339  loss_ce_5: 0.8985  loss_cate_5: 0  loss_mask_5: 0.9212  loss_dice_5: 1.35  loss_ce_6: 0.9498  loss_cate_6: 0  loss_mask_6: 0.8715  loss_dice_6: 1.361  loss_ce_7: 0.9216  loss_cate_7: 0  loss_mask_7: 0.8643  loss_dice_7: 1.376  loss_ce_8: 0.9684  loss_cate_8: 0  loss_mask_8: 0.854  loss_dice_8: 1.435  time: 1.3795  data_time: 0.0110  lr: 9.8986e-06  max_mem: 23777M
[01/01 10:42:22] d2.utils.events INFO:  eta: 14:46:05  iter: 459  total_loss: 33.49  loss_ce: 0.7725  loss_cate: 0.2229  loss_mask: 1.018  loss_dice: 1.291  loss_ce_0: 2.213  loss_cate_0: 0  loss_mask_0: 1.044  loss_dice_0: 1.23  loss_ce_1: 0.9175  loss_cate_1: 0  loss_mask_1: 1.059  loss_dice_1: 1.214  loss_ce_2: 0.8451  loss_cate_2: 0  loss_mask_2: 1.052  loss_dice_2: 1.266  loss_ce_3: 0.7849  loss_cate_3: 0  loss_mask_3: 1.028  loss_dice_3: 1.263  loss_ce_4: 0.8454  loss_cate_4: 0  loss_mask_4: 1.069  loss_dice_4: 1.321  loss_ce_5: 0.7152  loss_cate_5: 0  loss_mask_5: 1.082  loss_dice_5: 1.32  loss_ce_6: 0.7588  loss_cate_6: 0  loss_mask_6: 1.069  loss_dice_6: 1.279  loss_ce_7: 0.7267  loss_cate_7: 0  loss_mask_7: 1.059  loss_dice_7: 1.261  loss_ce_8: 0.7973  loss_cate_8: 0  loss_mask_8: 1.042  loss_dice_8: 1.297  time: 1.3796  data_time: 0.0104  lr: 9.894e-06  max_mem: 23777M
[01/01 10:42:50] d2.utils.events INFO:  eta: 14:45:42  iter: 479  total_loss: 38.01  loss_ce: 0.8781  loss_cate: 0.2559  loss_mask: 0.9751  loss_dice: 1.712  loss_ce_0: 2.159  loss_cate_0: 0  loss_mask_0: 0.9189  loss_dice_0: 1.644  loss_ce_1: 0.9608  loss_cate_1: 0  loss_mask_1: 0.918  loss_dice_1: 1.742  loss_ce_2: 0.9547  loss_cate_2: 0  loss_mask_2: 0.8793  loss_dice_2: 1.696  loss_ce_3: 0.9433  loss_cate_3: 0  loss_mask_3: 0.9137  loss_dice_3: 1.736  loss_ce_4: 0.9267  loss_cate_4: 0  loss_mask_4: 0.87  loss_dice_4: 1.77  loss_ce_5: 0.9574  loss_cate_5: 0  loss_mask_5: 0.8784  loss_dice_5: 1.761  loss_ce_6: 0.9263  loss_cate_6: 0  loss_mask_6: 0.9442  loss_dice_6: 1.745  loss_ce_7: 0.9369  loss_cate_7: 0  loss_mask_7: 0.8861  loss_dice_7: 1.729  loss_ce_8: 0.9063  loss_cate_8: 0  loss_mask_8: 1.003  loss_dice_8: 1.75  time: 1.3797  data_time: 0.0103  lr: 9.8894e-06  max_mem: 23791M
[01/01 10:43:17] d2.utils.events INFO:  eta: 14:45:14  iter: 499  total_loss: 32.3  loss_ce: 0.936  loss_cate: 0.2268  loss_mask: 0.9327  loss_dice: 1.384  loss_ce_0: 2.087  loss_cate_0: 0  loss_mask_0: 0.8813  loss_dice_0: 1.177  loss_ce_1: 1.001  loss_cate_1: 0  loss_mask_1: 0.9631  loss_dice_1: 1.306  loss_ce_2: 0.8915  loss_cate_2: 0  loss_mask_2: 0.9791  loss_dice_2: 1.3  loss_ce_3: 0.9621  loss_cate_3: 0  loss_mask_3: 0.9447  loss_dice_3: 1.429  loss_ce_4: 0.9566  loss_cate_4: 0  loss_mask_4: 0.9645  loss_dice_4: 1.444  loss_ce_5: 0.9105  loss_cate_5: 0  loss_mask_5: 0.9973  loss_dice_5: 1.408  loss_ce_6: 0.8359  loss_cate_6: 0  loss_mask_6: 0.9708  loss_dice_6: 1.362  loss_ce_7: 0.9346  loss_cate_7: 0  loss_mask_7: 0.9575  loss_dice_7: 1.353  loss_ce_8: 0.9339  loss_cate_8: 0  loss_mask_8: 0.9098  loss_dice_8: 1.374  time: 1.3797  data_time: 0.0131  lr: 9.8848e-06  max_mem: 23791M
[01/01 10:43:45] d2.utils.events INFO:  eta: 14:44:49  iter: 519  total_loss: 33.55  loss_ce: 0.8632  loss_cate: 0.2719  loss_mask: 1.016  loss_dice: 1.319  loss_ce_0: 2.041  loss_cate_0: 0  loss_mask_0: 0.8385  loss_dice_0: 1.183  loss_ce_1: 0.9674  loss_cate_1: 0  loss_mask_1: 0.8781  loss_dice_1: 1.212  loss_ce_2: 0.8116  loss_cate_2: 0  loss_mask_2: 0.9002  loss_dice_2: 1.26  loss_ce_3: 0.9101  loss_cate_3: 0  loss_mask_3: 0.9896  loss_dice_3: 1.277  loss_ce_4: 0.9325  loss_cate_4: 0  loss_mask_4: 0.8981  loss_dice_4: 1.285  loss_ce_5: 0.8608  loss_cate_5: 0  loss_mask_5: 0.975  loss_dice_5: 1.38  loss_ce_6: 0.853  loss_cate_6: 0  loss_mask_6: 1.002  loss_dice_6: 1.298  loss_ce_7: 0.8005  loss_cate_7: 0  loss_mask_7: 1.047  loss_dice_7: 1.289  loss_ce_8: 0.8244  loss_cate_8: 0  loss_mask_8: 1.036  loss_dice_8: 1.32  time: 1.3807  data_time: 0.0154  lr: 9.8802e-06  max_mem: 23800M
[01/01 10:44:13] d2.utils.events INFO:  eta: 14:44:22  iter: 539  total_loss: 34.86  loss_ce: 0.9324  loss_cate: 0.2955  loss_mask: 0.9508  loss_dice: 1.261  loss_ce_0: 1.902  loss_cate_0: 0  loss_mask_0: 0.8016  loss_dice_0: 1.181  loss_ce_1: 1.073  loss_cate_1: 0  loss_mask_1: 0.859  loss_dice_1: 1.258  loss_ce_2: 0.9254  loss_cate_2: 0  loss_mask_2: 0.9681  loss_dice_2: 1.326  loss_ce_3: 0.9735  loss_cate_3: 0  loss_mask_3: 1.016  loss_dice_3: 1.229  loss_ce_4: 0.9275  loss_cate_4: 0  loss_mask_4: 0.9573  loss_dice_4: 1.28  loss_ce_5: 0.9764  loss_cate_5: 0  loss_mask_5: 0.9772  loss_dice_5: 1.252  loss_ce_6: 0.9088  loss_cate_6: 0  loss_mask_6: 0.9383  loss_dice_6: 1.302  loss_ce_7: 0.9773  loss_cate_7: 0  loss_mask_7: 0.9354  loss_dice_7: 1.273  loss_ce_8: 0.9209  loss_cate_8: 0  loss_mask_8: 0.9183  loss_dice_8: 1.265  time: 1.3809  data_time: 0.0115  lr: 9.8755e-06  max_mem: 23800M
[01/01 10:44:41] d2.utils.events INFO:  eta: 14:43:55  iter: 559  total_loss: 29.57  loss_ce: 0.7142  loss_cate: 0.2788  loss_mask: 0.9048  loss_dice: 1.223  loss_ce_0: 1.888  loss_cate_0: 0  loss_mask_0: 0.8905  loss_dice_0: 1.172  loss_ce_1: 0.8452  loss_cate_1: 0  loss_mask_1: 0.8952  loss_dice_1: 1.199  loss_ce_2: 0.7557  loss_cate_2: 0  loss_mask_2: 0.9449  loss_dice_2: 1.18  loss_ce_3: 0.6635  loss_cate_3: 0  loss_mask_3: 0.8323  loss_dice_3: 1.239  loss_ce_4: 0.6918  loss_cate_4: 0  loss_mask_4: 0.8969  loss_dice_4: 1.228  loss_ce_5: 0.7662  loss_cate_5: 0  loss_mask_5: 0.8664  loss_dice_5: 1.183  loss_ce_6: 0.758  loss_cate_6: 0  loss_mask_6: 0.8036  loss_dice_6: 1.21  loss_ce_7: 0.7335  loss_cate_7: 0  loss_mask_7: 0.8237  loss_dice_7: 1.198  loss_ce_8: 0.7208  loss_cate_8: 0  loss_mask_8: 0.7991  loss_dice_8: 1.175  time: 1.3810  data_time: 0.0118  lr: 9.8709e-06  max_mem: 23800M
[01/01 10:45:09] d2.utils.events INFO:  eta: 14:43:33  iter: 579  total_loss: 28.01  loss_ce: 0.6316  loss_cate: 0.2568  loss_mask: 0.7485  loss_dice: 1.161  loss_ce_0: 1.772  loss_cate_0: 0  loss_mask_0: 0.7951  loss_dice_0: 1.129  loss_ce_1: 0.8091  loss_cate_1: 0  loss_mask_1: 0.7507  loss_dice_1: 1.085  loss_ce_2: 0.7789  loss_cate_2: 0  loss_mask_2: 0.6925  loss_dice_2: 1.111  loss_ce_3: 0.7049  loss_cate_3: 0  loss_mask_3: 0.7556  loss_dice_3: 1.177  loss_ce_4: 0.691  loss_cate_4: 0  loss_mask_4: 0.7616  loss_dice_4: 1.178  loss_ce_5: 0.6628  loss_cate_5: 0  loss_mask_5: 0.7898  loss_dice_5: 1.229  loss_ce_6: 0.6102  loss_cate_6: 0  loss_mask_6: 0.8306  loss_dice_6: 1.25  loss_ce_7: 0.6211  loss_cate_7: 0  loss_mask_7: 0.8537  loss_dice_7: 1.262  loss_ce_8: 0.5427  loss_cate_8: 0  loss_mask_8: 0.8212  loss_dice_8: 1.224  time: 1.3816  data_time: 0.0193  lr: 9.8663e-06  max_mem: 23800M
[01/01 10:45:37] d2.utils.events INFO:  eta: 14:43:06  iter: 599  total_loss: 31.34  loss_ce: 0.6137  loss_cate: 0.2383  loss_mask: 0.7834  loss_dice: 1.495  loss_ce_0: 1.721  loss_cate_0: 0  loss_mask_0: 0.7535  loss_dice_0: 1.353  loss_ce_1: 0.8402  loss_cate_1: 0  loss_mask_1: 0.8207  loss_dice_1: 1.376  loss_ce_2: 0.726  loss_cate_2: 0  loss_mask_2: 0.8645  loss_dice_2: 1.416  loss_ce_3: 0.7374  loss_cate_3: 0  loss_mask_3: 0.8404  loss_dice_3: 1.419  loss_ce_4: 0.6559  loss_cate_4: 0  loss_mask_4: 0.8294  loss_dice_4: 1.438  loss_ce_5: 0.5676  loss_cate_5: 0  loss_mask_5: 0.8118  loss_dice_5: 1.485  loss_ce_6: 0.5978  loss_cate_6: 0  loss_mask_6: 0.7993  loss_dice_6: 1.496  loss_ce_7: 0.594  loss_cate_7: 0  loss_mask_7: 0.8244  loss_dice_7: 1.524  loss_ce_8: 0.602  loss_cate_8: 0  loss_mask_8: 0.8094  loss_dice_8: 1.5  time: 1.3817  data_time: 0.0130  lr: 9.8617e-06  max_mem: 23800M
[01/01 10:46:05] d2.utils.events INFO:  eta: 14:42:44  iter: 619  total_loss: 25.33  loss_ce: 0.6553  loss_cate: 0.2337  loss_mask: 0.8094  loss_dice: 0.9882  loss_ce_0: 1.633  loss_cate_0: 0  loss_mask_0: 0.7764  loss_dice_0: 1.021  loss_ce_1: 0.673  loss_cate_1: 0  loss_mask_1: 0.785  loss_dice_1: 0.9862  loss_ce_2: 0.5878  loss_cate_2: 0  loss_mask_2: 0.7958  loss_dice_2: 0.9793  loss_ce_3: 0.6099  loss_cate_3: 0  loss_mask_3: 0.7773  loss_dice_3: 0.9597  loss_ce_4: 0.6051  loss_cate_4: 0  loss_mask_4: 0.7807  loss_dice_4: 0.9567  loss_ce_5: 0.5812  loss_cate_5: 0  loss_mask_5: 0.7772  loss_dice_5: 0.9697  loss_ce_6: 0.5988  loss_cate_6: 0  loss_mask_6: 0.819  loss_dice_6: 0.9739  loss_ce_7: 0.6119  loss_cate_7: 0  loss_mask_7: 0.7598  loss_dice_7: 0.9917  loss_ce_8: 0.6813  loss_cate_8: 0  loss_mask_8: 0.816  loss_dice_8: 0.9896  time: 1.3835  data_time: 0.0129  lr: 9.857e-06  max_mem: 23800M
[01/01 10:46:33] d2.utils.events INFO:  eta: 14:42:16  iter: 639  total_loss: 25.6  loss_ce: 0.7028  loss_cate: 0.2196  loss_mask: 0.74  loss_dice: 1.195  loss_ce_0: 1.646  loss_cate_0: 0  loss_mask_0: 0.722  loss_dice_0: 1.129  loss_ce_1: 0.7683  loss_cate_1: 0  loss_mask_1: 0.7092  loss_dice_1: 1.116  loss_ce_2: 0.7235  loss_cate_2: 0  loss_mask_2: 0.7066  loss_dice_2: 1.188  loss_ce_3: 0.7535  loss_cate_3: 0  loss_mask_3: 0.7322  loss_dice_3: 1.274  loss_ce_4: 0.667  loss_cate_4: 0  loss_mask_4: 0.7153  loss_dice_4: 1.219  loss_ce_5: 0.6901  loss_cate_5: 0  loss_mask_5: 0.6954  loss_dice_5: 1.221  loss_ce_6: 0.6474  loss_cate_6: 0  loss_mask_6: 0.6799  loss_dice_6: 1.168  loss_ce_7: 0.7093  loss_cate_7: 0  loss_mask_7: 0.7759  loss_dice_7: 1.197  loss_ce_8: 0.6528  loss_cate_8: 0  loss_mask_8: 0.669  loss_dice_8: 1.172  time: 1.3836  data_time: 0.0107  lr: 9.8524e-06  max_mem: 23800M
[01/01 10:47:01] d2.utils.events INFO:  eta: 14:41:45  iter: 659  total_loss: 23.39  loss_ce: 0.5776  loss_cate: 0.239  loss_mask: 0.6441  loss_dice: 0.8665  loss_ce_0: 1.42  loss_cate_0: 0  loss_mask_0: 0.6851  loss_dice_0: 0.885  loss_ce_1: 0.6366  loss_cate_1: 0  loss_mask_1: 0.657  loss_dice_1: 0.9967  loss_ce_2: 0.6098  loss_cate_2: 0  loss_mask_2: 0.6845  loss_dice_2: 0.8548  loss_ce_3: 0.5899  loss_cate_3: 0  loss_mask_3: 0.6506  loss_dice_3: 0.9923  loss_ce_4: 0.5986  loss_cate_4: 0  loss_mask_4: 0.6767  loss_dice_4: 1.038  loss_ce_5: 0.5892  loss_cate_5: 0  loss_mask_5: 0.6428  loss_dice_5: 0.8737  loss_ce_6: 0.6076  loss_cate_6: 0  loss_mask_6: 0.6447  loss_dice_6: 0.8147  loss_ce_7: 0.5689  loss_cate_7: 0  loss_mask_7: 0.651  loss_dice_7: 0.8926  loss_ce_8: 0.5806  loss_cate_8: 0  loss_mask_8: 0.6414  loss_dice_8: 0.861  time: 1.3835  data_time: 0.0116  lr: 9.8478e-06  max_mem: 23800M
[01/01 10:47:29] d2.utils.events INFO:  eta: 14:41:20  iter: 679  total_loss: 22.96  loss_ce: 0.6297  loss_cate: 0.1913  loss_mask: 0.6902  loss_dice: 0.9457  loss_ce_0: 1.399  loss_cate_0: 0  loss_mask_0: 0.6498  loss_dice_0: 0.8817  loss_ce_1: 0.669  loss_cate_1: 0  loss_mask_1: 0.6483  loss_dice_1: 0.9119  loss_ce_2: 0.6489  loss_cate_2: 0  loss_mask_2: 0.6491  loss_dice_2: 0.9112  loss_ce_3: 0.6148  loss_cate_3: 0  loss_mask_3: 0.6892  loss_dice_3: 0.9708  loss_ce_4: 0.5602  loss_cate_4: 0  loss_mask_4: 0.6756  loss_dice_4: 0.97  loss_ce_5: 0.6742  loss_cate_5: 0  loss_mask_5: 0.6961  loss_dice_5: 0.9431  loss_ce_6: 0.6129  loss_cate_6: 0  loss_mask_6: 0.698  loss_dice_6: 0.9408  loss_ce_7: 0.612  loss_cate_7: 0  loss_mask_7: 0.697  loss_dice_7: 0.9391  loss_ce_8: 0.6515  loss_cate_8: 0  loss_mask_8: 0.7045  loss_dice_8: 0.8979  time: 1.3837  data_time: 0.0123  lr: 9.8432e-06  max_mem: 23800M
[01/01 10:47:57] d2.utils.events INFO:  eta: 14:40:59  iter: 699  total_loss: 28.79  loss_ce: 0.7227  loss_cate: 0.2182  loss_mask: 0.8556  loss_dice: 1.204  loss_ce_0: 1.46  loss_cate_0: 0  loss_mask_0: 0.7519  loss_dice_0: 1.134  loss_ce_1: 0.7861  loss_cate_1: 0  loss_mask_1: 0.8562  loss_dice_1: 1.239  loss_ce_2: 0.693  loss_cate_2: 0  loss_mask_2: 0.8471  loss_dice_2: 1.203  loss_ce_3: 0.7629  loss_cate_3: 0  loss_mask_3: 0.8147  loss_dice_3: 1.233  loss_ce_4: 0.7676  loss_cate_4: 0  loss_mask_4: 0.7713  loss_dice_4: 1.188  loss_ce_5: 0.7193  loss_cate_5: 0  loss_mask_5: 0.9096  loss_dice_5: 1.167  loss_ce_6: 0.7765  loss_cate_6: 0  loss_mask_6: 0.8681  loss_dice_6: 1.221  loss_ce_7: 0.6925  loss_cate_7: 0  loss_mask_7: 0.8358  loss_dice_7: 1.209  loss_ce_8: 0.7073  loss_cate_8: 0  loss_mask_8: 0.8296  loss_dice_8: 1.203  time: 1.3846  data_time: 0.0129  lr: 9.8385e-06  max_mem: 23800M
[01/01 10:48:25] d2.utils.events INFO:  eta: 14:40:36  iter: 719  total_loss: 25.39  loss_ce: 0.6156  loss_cate: 0.2054  loss_mask: 0.75  loss_dice: 0.9763  loss_ce_0: 1.323  loss_cate_0: 0  loss_mask_0: 0.7445  loss_dice_0: 1.009  loss_ce_1: 0.6679  loss_cate_1: 0  loss_mask_1: 0.7418  loss_dice_1: 1.008  loss_ce_2: 0.6766  loss_cate_2: 0  loss_mask_2: 0.675  loss_dice_2: 0.9523  loss_ce_3: 0.5536  loss_cate_3: 0  loss_mask_3: 0.7752  loss_dice_3: 1.009  loss_ce_4: 0.5835  loss_cate_4: 0  loss_mask_4: 0.796  loss_dice_4: 0.9555  loss_ce_5: 0.5551  loss_cate_5: 0  loss_mask_5: 0.7817  loss_dice_5: 0.9419  loss_ce_6: 0.5836  loss_cate_6: 0  loss_mask_6: 0.7521  loss_dice_6: 1.008  loss_ce_7: 0.6078  loss_cate_7: 0  loss_mask_7: 0.7549  loss_dice_7: 0.926  loss_ce_8: 0.5827  loss_cate_8: 0  loss_mask_8: 0.767  loss_dice_8: 0.9672  time: 1.3853  data_time: 0.0135  lr: 9.8339e-06  max_mem: 23800M
[01/01 10:48:53] d2.utils.events INFO:  eta: 14:40:06  iter: 739  total_loss: 27.21  loss_ce: 0.7285  loss_cate: 0.1701  loss_mask: 0.7048  loss_dice: 1.055  loss_ce_0: 1.44  loss_cate_0: 0  loss_mask_0: 0.6604  loss_dice_0: 1.061  loss_ce_1: 0.7834  loss_cate_1: 0  loss_mask_1: 0.6822  loss_dice_1: 1.023  loss_ce_2: 0.6875  loss_cate_2: 0  loss_mask_2: 0.6422  loss_dice_2: 1.07  loss_ce_3: 0.6991  loss_cate_3: 0  loss_mask_3: 0.7093  loss_dice_3: 1.08  loss_ce_4: 0.6556  loss_cate_4: 0  loss_mask_4: 0.7297  loss_dice_4: 1.112  loss_ce_5: 0.7236  loss_cate_5: 0  loss_mask_5: 0.7336  loss_dice_5: 1.135  loss_ce_6: 0.7274  loss_cate_6: 0  loss_mask_6: 0.6674  loss_dice_6: 1.07  loss_ce_7: 0.6762  loss_cate_7: 0  loss_mask_7: 0.7299  loss_dice_7: 1.041  loss_ce_8: 0.6949  loss_cate_8: 0  loss_mask_8: 0.6515  loss_dice_8: 1.034  time: 1.3852  data_time: 0.0122  lr: 9.8293e-06  max_mem: 23800M
[01/01 10:49:21] d2.utils.events INFO:  eta: 14:39:38  iter: 759  total_loss: 27.1  loss_ce: 0.678  loss_cate: 0.2538  loss_mask: 0.9543  loss_dice: 1.2  loss_ce_0: 1.386  loss_cate_0: 0  loss_mask_0: 0.8658  loss_dice_0: 1.116  loss_ce_1: 0.7456  loss_cate_1: 0  loss_mask_1: 0.8939  loss_dice_1: 1.17  loss_ce_2: 0.6969  loss_cate_2: 0  loss_mask_2: 0.8339  loss_dice_2: 1.175  loss_ce_3: 0.6725  loss_cate_3: 0  loss_mask_3: 0.8389  loss_dice_3: 1.281  loss_ce_4: 0.5862  loss_cate_4: 0  loss_mask_4: 0.9119  loss_dice_4: 1.192  loss_ce_5: 0.6578  loss_cate_5: 0  loss_mask_5: 0.8497  loss_dice_5: 1.222  loss_ce_6: 0.6752  loss_cate_6: 0  loss_mask_6: 0.8492  loss_dice_6: 1.183  loss_ce_7: 0.6564  loss_cate_7: 0  loss_mask_7: 0.9489  loss_dice_7: 1.204  loss_ce_8: 0.718  loss_cate_8: 0  loss_mask_8: 0.8934  loss_dice_8: 1.229  time: 1.3854  data_time: 0.0117  lr: 9.8247e-06  max_mem: 23800M
[01/01 10:49:49] d2.utils.events INFO:  eta: 14:39:26  iter: 779  total_loss: 28.91  loss_ce: 0.7384  loss_cate: 0.2653  loss_mask: 0.8738  loss_dice: 1.175  loss_ce_0: 1.525  loss_cate_0: 0  loss_mask_0: 0.7587  loss_dice_0: 1.103  loss_ce_1: 0.9178  loss_cate_1: 0  loss_mask_1: 0.7501  loss_dice_1: 1.177  loss_ce_2: 0.8168  loss_cate_2: 0  loss_mask_2: 0.7976  loss_dice_2: 1.105  loss_ce_3: 0.8386  loss_cate_3: 0  loss_mask_3: 0.7787  loss_dice_3: 1.117  loss_ce_4: 0.8142  loss_cate_4: 0  loss_mask_4: 0.7731  loss_dice_4: 1.109  loss_ce_5: 0.822  loss_cate_5: 0  loss_mask_5: 0.8533  loss_dice_5: 1.143  loss_ce_6: 0.8232  loss_cate_6: 0  loss_mask_6: 0.8448  loss_dice_6: 1.141  loss_ce_7: 0.7082  loss_cate_7: 0  loss_mask_7: 0.8693  loss_dice_7: 1.172  loss_ce_8: 0.7184  loss_cate_8: 0  loss_mask_8: 0.8672  loss_dice_8: 1.179  time: 1.3858  data_time: 0.0114  lr: 9.82e-06  max_mem: 23800M
[01/01 10:50:17] d2.utils.events INFO:  eta: 14:39:07  iter: 799  total_loss: 28.17  loss_ce: 0.7003  loss_cate: 0.2161  loss_mask: 0.7564  loss_dice: 1.145  loss_ce_0: 1.428  loss_cate_0: 0  loss_mask_0: 0.5847  loss_dice_0: 1.111  loss_ce_1: 0.7224  loss_cate_1: 0  loss_mask_1: 0.7969  loss_dice_1: 1.184  loss_ce_2: 0.7186  loss_cate_2: 0  loss_mask_2: 0.8014  loss_dice_2: 1.138  loss_ce_3: 0.7296  loss_cate_3: 0  loss_mask_3: 0.7601  loss_dice_3: 1.123  loss_ce_4: 0.6392  loss_cate_4: 0  loss_mask_4: 0.8015  loss_dice_4: 1.146  loss_ce_5: 0.6879  loss_cate_5: 0  loss_mask_5: 0.7703  loss_dice_5: 1.119  loss_ce_6: 0.7401  loss_cate_6: 0  loss_mask_6: 0.8027  loss_dice_6: 1.118  loss_ce_7: 0.7736  loss_cate_7: 0  loss_mask_7: 0.7864  loss_dice_7: 1.145  loss_ce_8: 0.7205  loss_cate_8: 0  loss_mask_8: 0.7067  loss_dice_8: 1.143  time: 1.3863  data_time: 0.0106  lr: 9.8154e-06  max_mem: 23800M
[01/01 10:50:45] d2.utils.events INFO:  eta: 14:38:45  iter: 819  total_loss: 29.64  loss_ce: 0.6766  loss_cate: 0.1836  loss_mask: 0.8007  loss_dice: 1.277  loss_ce_0: 1.287  loss_cate_0: 0  loss_mask_0: 0.7606  loss_dice_0: 1.256  loss_ce_1: 0.827  loss_cate_1: 0  loss_mask_1: 0.7082  loss_dice_1: 1.261  loss_ce_2: 0.7268  loss_cate_2: 0  loss_mask_2: 0.6371  loss_dice_2: 1.209  loss_ce_3: 0.6667  loss_cate_3: 0  loss_mask_3: 0.6748  loss_dice_3: 1.222  loss_ce_4: 0.7783  loss_cate_4: 0  loss_mask_4: 0.7353  loss_dice_4: 1.228  loss_ce_5: 0.759  loss_cate_5: 0  loss_mask_5: 0.7593  loss_dice_5: 1.263  loss_ce_6: 0.8116  loss_cate_6: 0  loss_mask_6: 0.7535  loss_dice_6: 1.215  loss_ce_7: 0.7565  loss_cate_7: 0  loss_mask_7: 0.8349  loss_dice_7: 1.277  loss_ce_8: 0.7335  loss_cate_8: 0  loss_mask_8: 0.7856  loss_dice_8: 1.278  time: 1.3868  data_time: 0.0119  lr: 9.8108e-06  max_mem: 23800M
[01/01 10:51:14] d2.utils.events INFO:  eta: 14:38:25  iter: 839  total_loss: 31.12  loss_ce: 0.7213  loss_cate: 0.2249  loss_mask: 0.8396  loss_dice: 1.359  loss_ce_0: 1.313  loss_cate_0: 0  loss_mask_0: 0.8408  loss_dice_0: 1.181  loss_ce_1: 0.7489  loss_cate_1: 0  loss_mask_1: 0.784  loss_dice_1: 1.175  loss_ce_2: 0.6984  loss_cate_2: 0  loss_mask_2: 0.7317  loss_dice_2: 1.261  loss_ce_3: 0.7121  loss_cate_3: 0  loss_mask_3: 0.7781  loss_dice_3: 1.304  loss_ce_4: 0.6977  loss_cate_4: 0  loss_mask_4: 0.7876  loss_dice_4: 1.251  loss_ce_5: 0.7386  loss_cate_5: 0  loss_mask_5: 0.7758  loss_dice_5: 1.295  loss_ce_6: 0.718  loss_cate_6: 0  loss_mask_6: 0.8179  loss_dice_6: 1.256  loss_ce_7: 0.7264  loss_cate_7: 0  loss_mask_7: 0.8194  loss_dice_7: 1.331  loss_ce_8: 0.7156  loss_cate_8: 0  loss_mask_8: 0.8167  loss_dice_8: 1.269  time: 1.3881  data_time: 0.0126  lr: 9.8062e-06  max_mem: 23800M
[01/01 10:51:43] d2.utils.events INFO:  eta: 14:38:02  iter: 859  total_loss: 32.27  loss_ce: 0.7805  loss_cate: 0.2127  loss_mask: 0.8115  loss_dice: 1.272  loss_ce_0: 1.37  loss_cate_0: 0  loss_mask_0: 0.797  loss_dice_0: 1.261  loss_ce_1: 0.7688  loss_cate_1: 0  loss_mask_1: 0.8205  loss_dice_1: 1.224  loss_ce_2: 0.8582  loss_cate_2: 0  loss_mask_2: 0.8437  loss_dice_2: 1.294  loss_ce_3: 0.853  loss_cate_3: 0  loss_mask_3: 0.8246  loss_dice_3: 1.268  loss_ce_4: 0.8178  loss_cate_4: 0  loss_mask_4: 0.7808  loss_dice_4: 1.274  loss_ce_5: 0.8555  loss_cate_5: 0  loss_mask_5: 0.7868  loss_dice_5: 1.292  loss_ce_6: 0.7961  loss_cate_6: 0  loss_mask_6: 0.7947  loss_dice_6: 1.244  loss_ce_7: 0.8294  loss_cate_7: 0  loss_mask_7: 0.7715  loss_dice_7: 1.283  loss_ce_8: 0.9283  loss_cate_8: 0  loss_mask_8: 0.8422  loss_dice_8: 1.256  time: 1.3887  data_time: 0.0128  lr: 9.8015e-06  max_mem: 23800M
[01/01 10:52:11] d2.utils.events INFO:  eta: 14:37:54  iter: 879  total_loss: 31.4  loss_ce: 0.875  loss_cate: 0.2071  loss_mask: 0.9558  loss_dice: 1.333  loss_ce_0: 1.281  loss_cate_0: 0  loss_mask_0: 0.9568  loss_dice_0: 1.386  loss_ce_1: 0.8907  loss_cate_1: 0  loss_mask_1: 0.9275  loss_dice_1: 1.279  loss_ce_2: 0.9294  loss_cate_2: 0  loss_mask_2: 0.9829  loss_dice_2: 1.283  loss_ce_3: 0.8176  loss_cate_3: 0  loss_mask_3: 0.8799  loss_dice_3: 1.233  loss_ce_4: 0.7761  loss_cate_4: 0  loss_mask_4: 0.9199  loss_dice_4: 1.244  loss_ce_5: 0.8281  loss_cate_5: 0  loss_mask_5: 0.8996  loss_dice_5: 1.336  loss_ce_6: 0.8943  loss_cate_6: 0  loss_mask_6: 0.8882  loss_dice_6: 1.285  loss_ce_7: 0.8382  loss_cate_7: 0  loss_mask_7: 0.927  loss_dice_7: 1.304  loss_ce_8: 0.8245  loss_cate_8: 0  loss_mask_8: 0.9211  loss_dice_8: 1.33  time: 1.3890  data_time: 0.0110  lr: 9.7969e-06  max_mem: 23800M
[01/01 10:52:39] d2.utils.events INFO:  eta: 14:37:35  iter: 899  total_loss: 28.78  loss_ce: 0.685  loss_cate: 0.2261  loss_mask: 0.9998  loss_dice: 1.227  loss_ce_0: 1.214  loss_cate_0: 0  loss_mask_0: 0.934  loss_dice_0: 1.201  loss_ce_1: 0.7504  loss_cate_1: 0  loss_mask_1: 0.9839  loss_dice_1: 1.22  loss_ce_2: 0.6611  loss_cate_2: 0  loss_mask_2: 0.9539  loss_dice_2: 1.091  loss_ce_3: 0.6968  loss_cate_3: 0  loss_mask_3: 0.9205  loss_dice_3: 1.137  loss_ce_4: 0.6842  loss_cate_4: 0  loss_mask_4: 0.8793  loss_dice_4: 1.219  loss_ce_5: 0.7325  loss_cate_5: 0  loss_mask_5: 0.925  loss_dice_5: 1.16  loss_ce_6: 0.7337  loss_cate_6: 0  loss_mask_6: 0.9852  loss_dice_6: 1.189  loss_ce_7: 0.6786  loss_cate_7: 0  loss_mask_7: 0.9836  loss_dice_7: 1.22  loss_ce_8: 0.6429  loss_cate_8: 0  loss_mask_8: 0.9611  loss_dice_8: 1.216  time: 1.3894  data_time: 0.0140  lr: 9.7923e-06  max_mem: 23800M
[01/01 10:53:08] d2.utils.events INFO:  eta: 14:37:18  iter: 919  total_loss: 29.09  loss_ce: 0.7957  loss_cate: 0.2197  loss_mask: 0.7449  loss_dice: 1.268  loss_ce_0: 1.327  loss_cate_0: 0  loss_mask_0: 0.732  loss_dice_0: 1.295  loss_ce_1: 0.8115  loss_cate_1: 0  loss_mask_1: 0.7365  loss_dice_1: 1.255  loss_ce_2: 0.8299  loss_cate_2: 0  loss_mask_2: 0.7689  loss_dice_2: 1.267  loss_ce_3: 0.7776  loss_cate_3: 0  loss_mask_3: 0.785  loss_dice_3: 1.277  loss_ce_4: 0.7491  loss_cate_4: 0  loss_mask_4: 0.7316  loss_dice_4: 1.245  loss_ce_5: 0.7311  loss_cate_5: 0  loss_mask_5: 0.7617  loss_dice_5: 1.252  loss_ce_6: 0.7526  loss_cate_6: 0  loss_mask_6: 0.7901  loss_dice_6: 1.224  loss_ce_7: 0.7989  loss_cate_7: 0  loss_mask_7: 0.7774  loss_dice_7: 1.213  loss_ce_8: 0.7174  loss_cate_8: 0  loss_mask_8: 0.7844  loss_dice_8: 1.24  time: 1.3903  data_time: 0.0142  lr: 9.7877e-06  max_mem: 23800M
[01/01 10:53:36] d2.utils.events INFO:  eta: 14:36:58  iter: 939  total_loss: 23.63  loss_ce: 0.6458  loss_cate: 0.1779  loss_mask: 0.7169  loss_dice: 1.021  loss_ce_0: 1.238  loss_cate_0: 0  loss_mask_0: 0.601  loss_dice_0: 1.108  loss_ce_1: 0.7575  loss_cate_1: 0  loss_mask_1: 0.6792  loss_dice_1: 1.018  loss_ce_2: 0.7463  loss_cate_2: 0  loss_mask_2: 0.6341  loss_dice_2: 0.9919  loss_ce_3: 0.7374  loss_cate_3: 0  loss_mask_3: 0.5843  loss_dice_3: 1.019  loss_ce_4: 0.6484  loss_cate_4: 0  loss_mask_4: 0.7025  loss_dice_4: 0.9886  loss_ce_5: 0.7014  loss_cate_5: 0  loss_mask_5: 0.6748  loss_dice_5: 1.047  loss_ce_6: 0.6319  loss_cate_6: 0  loss_mask_6: 0.673  loss_dice_6: 1.01  loss_ce_7: 0.6216  loss_cate_7: 0  loss_mask_7: 0.7007  loss_dice_7: 0.9781  loss_ce_8: 0.6548  loss_cate_8: 0  loss_mask_8: 0.7248  loss_dice_8: 1.033  time: 1.3910  data_time: 0.0138  lr: 9.783e-06  max_mem: 23800M
[01/01 10:54:04] d2.utils.events INFO:  eta: 14:36:44  iter: 959  total_loss: 30.34  loss_ce: 0.8153  loss_cate: 0.2343  loss_mask: 0.7573  loss_dice: 1.094  loss_ce_0: 1.31  loss_cate_0: 0  loss_mask_0: 0.7208  loss_dice_0: 1.145  loss_ce_1: 1.083  loss_cate_1: 0  loss_mask_1: 0.7103  loss_dice_1: 1.165  loss_ce_2: 0.8822  loss_cate_2: 0  loss_mask_2: 0.8117  loss_dice_2: 1.2  loss_ce_3: 0.9346  loss_cate_3: 0  loss_mask_3: 0.7237  loss_dice_3: 1.158  loss_ce_4: 0.9048  loss_cate_4: 0  loss_mask_4: 0.9185  loss_dice_4: 1.164  loss_ce_5: 0.8352  loss_cate_5: 0  loss_mask_5: 0.764  loss_dice_5: 1.041  loss_ce_6: 0.8557  loss_cate_6: 0  loss_mask_6: 0.7329  loss_dice_6: 1.138  loss_ce_7: 0.8411  loss_cate_7: 0  loss_mask_7: 0.7757  loss_dice_7: 1.163  loss_ce_8: 0.8022  loss_cate_8: 0  loss_mask_8: 0.7863  loss_dice_8: 1.137  time: 1.3913  data_time: 0.0117  lr: 9.7784e-06  max_mem: 23800M
[01/01 10:54:33] d2.utils.events INFO:  eta: 14:36:26  iter: 979  total_loss: 28.48  loss_ce: 0.7522  loss_cate: 0.2093  loss_mask: 0.7406  loss_dice: 1.166  loss_ce_0: 1.239  loss_cate_0: 0  loss_mask_0: 0.7426  loss_dice_0: 1.156  loss_ce_1: 0.7852  loss_cate_1: 0  loss_mask_1: 0.7909  loss_dice_1: 1.134  loss_ce_2: 0.6535  loss_cate_2: 0  loss_mask_2: 0.7792  loss_dice_2: 1.144  loss_ce_3: 0.716  loss_cate_3: 0  loss_mask_3: 0.7893  loss_dice_3: 1.202  loss_ce_4: 0.6574  loss_cate_4: 0  loss_mask_4: 0.7547  loss_dice_4: 1.177  loss_ce_5: 0.7771  loss_cate_5: 0  loss_mask_5: 0.7469  loss_dice_5: 1.099  loss_ce_6: 0.6942  loss_cate_6: 0  loss_mask_6: 0.7552  loss_dice_6: 1.119  loss_ce_7: 0.7194  loss_cate_7: 0  loss_mask_7: 0.7868  loss_dice_7: 1.184  loss_ce_8: 0.6877  loss_cate_8: 0  loss_mask_8: 0.7456  loss_dice_8: 1.187  time: 1.3917  data_time: 0.0124  lr: 9.7738e-06  max_mem: 23800M
[01/01 10:55:01] d2.utils.events INFO:  eta: 14:36:07  iter: 999  total_loss: 26.38  loss_ce: 0.6648  loss_cate: 0.1729  loss_mask: 0.7209  loss_dice: 1.074  loss_ce_0: 1.103  loss_cate_0: 0  loss_mask_0: 0.7635  loss_dice_0: 1.02  loss_ce_1: 0.7909  loss_cate_1: 0  loss_mask_1: 0.7945  loss_dice_1: 1.015  loss_ce_2: 0.773  loss_cate_2: 0  loss_mask_2: 0.7948  loss_dice_2: 1.01  loss_ce_3: 0.732  loss_cate_3: 0  loss_mask_3: 0.7846  loss_dice_3: 1.066  loss_ce_4: 0.6608  loss_cate_4: 0  loss_mask_4: 0.7886  loss_dice_4: 1.07  loss_ce_5: 0.7187  loss_cate_5: 0  loss_mask_5: 0.6951  loss_dice_5: 1.058  loss_ce_6: 0.7289  loss_cate_6: 0  loss_mask_6: 0.7055  loss_dice_6: 1.057  loss_ce_7: 0.6973  loss_cate_7: 0  loss_mask_7: 0.7349  loss_dice_7: 1.105  loss_ce_8: 0.6914  loss_cate_8: 0  loss_mask_8: 0.7356  loss_dice_8: 1.08  time: 1.3922  data_time: 0.0121  lr: 9.7692e-06  max_mem: 23800M
[01/01 10:55:30] d2.utils.events INFO:  eta: 14:35:56  iter: 1019  total_loss: 29.48  loss_ce: 0.6866  loss_cate: 0.2692  loss_mask: 0.8696  loss_dice: 1.24  loss_ce_0: 1.32  loss_cate_0: 0  loss_mask_0: 0.9068  loss_dice_0: 1.197  loss_ce_1: 0.8924  loss_cate_1: 0  loss_mask_1: 0.852  loss_dice_1: 1.219  loss_ce_2: 0.6908  loss_cate_2: 0  loss_mask_2: 0.8731  loss_dice_2: 1.174  loss_ce_3: 0.675  loss_cate_3: 0  loss_mask_3: 0.8169  loss_dice_3: 1.178  loss_ce_4: 0.6498  loss_cate_4: 0  loss_mask_4: 0.8762  loss_dice_4: 1.216  loss_ce_5: 0.7113  loss_cate_5: 0  loss_mask_5: 0.8525  loss_dice_5: 1.249  loss_ce_6: 0.7174  loss_cate_6: 0  loss_mask_6: 0.8201  loss_dice_6: 1.185  loss_ce_7: 0.7278  loss_cate_7: 0  loss_mask_7: 0.804  loss_dice_7: 1.243  loss_ce_8: 0.6936  loss_cate_8: 0  loss_mask_8: 0.7879  loss_dice_8: 1.233  time: 1.3929  data_time: 0.0126  lr: 9.7645e-06  max_mem: 23800M
[01/01 10:55:58] d2.utils.events INFO:  eta: 14:35:37  iter: 1039  total_loss: 27.63  loss_ce: 0.662  loss_cate: 0.1786  loss_mask: 0.7893  loss_dice: 1.12  loss_ce_0: 1.13  loss_cate_0: 0  loss_mask_0: 0.7901  loss_dice_0: 1.091  loss_ce_1: 0.7335  loss_cate_1: 0  loss_mask_1: 0.7501  loss_dice_1: 1.23  loss_ce_2: 0.6142  loss_cate_2: 0  loss_mask_2: 0.7702  loss_dice_2: 1.233  loss_ce_3: 0.6042  loss_cate_3: 0  loss_mask_3: 0.7668  loss_dice_3: 1.109  loss_ce_4: 0.6426  loss_cate_4: 0  loss_mask_4: 0.7834  loss_dice_4: 1.144  loss_ce_5: 0.6688  loss_cate_5: 0  loss_mask_5: 0.797  loss_dice_5: 1.213  loss_ce_6: 0.6615  loss_cate_6: 0  loss_mask_6: 0.7684  loss_dice_6: 1.144  loss_ce_7: 0.6583  loss_cate_7: 0  loss_mask_7: 0.7873  loss_dice_7: 1.121  loss_ce_8: 0.6156  loss_cate_8: 0  loss_mask_8: 0.7804  loss_dice_8: 1.104  time: 1.3933  data_time: 0.0130  lr: 9.7599e-06  max_mem: 23800M
[01/01 10:56:26] d2.utils.events INFO:  eta: 14:35:17  iter: 1059  total_loss: 29.15  loss_ce: 0.6932  loss_cate: 0.1633  loss_mask: 0.8909  loss_dice: 1.173  loss_ce_0: 1.225  loss_cate_0: 0  loss_mask_0: 0.9362  loss_dice_0: 1.138  loss_ce_1: 0.7031  loss_cate_1: 0  loss_mask_1: 0.8456  loss_dice_1: 1.179  loss_ce_2: 0.676  loss_cate_2: 0  loss_mask_2: 0.9198  loss_dice_2: 1.192  loss_ce_3: 0.7405  loss_cate_3: 0  loss_mask_3: 0.9058  loss_dice_3: 1.19  loss_ce_4: 0.6676  loss_cate_4: 0  loss_mask_4: 0.9589  loss_dice_4: 1.194  loss_ce_5: 0.7162  loss_cate_5: 0  loss_mask_5: 0.9026  loss_dice_5: 1.181  loss_ce_6: 0.6921  loss_cate_6: 0  loss_mask_6: 0.868  loss_dice_6: 1.146  loss_ce_7: 0.6761  loss_cate_7: 0  loss_mask_7: 0.907  loss_dice_7: 1.195  loss_ce_8: 0.7136  loss_cate_8: 0  loss_mask_8: 0.9705  loss_dice_8: 1.172  time: 1.3933  data_time: 0.0124  lr: 9.7553e-06  max_mem: 23800M
[01/01 10:56:54] d2.utils.events INFO:  eta: 14:34:55  iter: 1079  total_loss: 26.52  loss_ce: 0.7045  loss_cate: 0.2774  loss_mask: 0.8529  loss_dice: 1.015  loss_ce_0: 1.171  loss_cate_0: 0  loss_mask_0: 0.8756  loss_dice_0: 0.9777  loss_ce_1: 0.7144  loss_cate_1: 0  loss_mask_1: 1.003  loss_dice_1: 1.11  loss_ce_2: 0.659  loss_cate_2: 0  loss_mask_2: 0.9908  loss_dice_2: 1.062  loss_ce_3: 0.6704  loss_cate_3: 0  loss_mask_3: 0.864  loss_dice_3: 1.05  loss_ce_4: 0.6294  loss_cate_4: 0  loss_mask_4: 0.8464  loss_dice_4: 1.064  loss_ce_5: 0.6525  loss_cate_5: 0  loss_mask_5: 0.8518  loss_dice_5: 0.9921  loss_ce_6: 0.6253  loss_cate_6: 0  loss_mask_6: 0.9784  loss_dice_6: 1.052  loss_ce_7: 0.6459  loss_cate_7: 0  loss_mask_7: 0.9443  loss_dice_7: 1.012  loss_ce_8: 0.6257  loss_cate_8: 0  loss_mask_8: 0.9773  loss_dice_8: 1.024  time: 1.3935  data_time: 0.0136  lr: 9.7507e-06  max_mem: 23800M
[01/01 10:57:22] d2.utils.events INFO:  eta: 14:34:40  iter: 1099  total_loss: 30.73  loss_ce: 0.7399  loss_cate: 0.2533  loss_mask: 0.9515  loss_dice: 1.175  loss_ce_0: 1.258  loss_cate_0: 0  loss_mask_0: 0.8957  loss_dice_0: 1.046  loss_ce_1: 0.816  loss_cate_1: 0  loss_mask_1: 0.9313  loss_dice_1: 1.208  loss_ce_2: 0.7493  loss_cate_2: 0  loss_mask_2: 0.89  loss_dice_2: 1.137  loss_ce_3: 0.7939  loss_cate_3: 0  loss_mask_3: 0.9218  loss_dice_3: 1.144  loss_ce_4: 0.8031  loss_cate_4: 0  loss_mask_4: 0.8513  loss_dice_4: 1.097  loss_ce_5: 0.7992  loss_cate_5: 0  loss_mask_5: 0.8545  loss_dice_5: 1.11  loss_ce_6: 0.7944  loss_cate_6: 0  loss_mask_6: 0.9039  loss_dice_6: 1.136  loss_ce_7: 0.7485  loss_cate_7: 0  loss_mask_7: 0.9719  loss_dice_7: 1.128  loss_ce_8: 0.7574  loss_cate_8: 0  loss_mask_8: 0.956  loss_dice_8: 1.127  time: 1.3934  data_time: 0.0122  lr: 9.746e-06  max_mem: 23800M
[01/01 10:57:50] d2.utils.events INFO:  eta: 14:34:22  iter: 1119  total_loss: 22.88  loss_ce: 0.6  loss_cate: 0.1725  loss_mask: 0.7167  loss_dice: 1.172  loss_ce_0: 1.105  loss_cate_0: 0  loss_mask_0: 0.6874  loss_dice_0: 1.146  loss_ce_1: 0.686  loss_cate_1: 0  loss_mask_1: 0.6775  loss_dice_1: 1.18  loss_ce_2: 0.6402  loss_cate_2: 0  loss_mask_2: 0.6967  loss_dice_2: 1.147  loss_ce_3: 0.6196  loss_cate_3: 0  loss_mask_3: 0.7521  loss_dice_3: 1.17  loss_ce_4: 0.728  loss_cate_4: 0  loss_mask_4: 0.7494  loss_dice_4: 1.182  loss_ce_5: 0.6797  loss_cate_5: 0  loss_mask_5: 0.6817  loss_dice_5: 1.188  loss_ce_6: 0.6427  loss_cate_6: 0  loss_mask_6: 0.6351  loss_dice_6: 1.151  loss_ce_7: 0.5876  loss_cate_7: 0  loss_mask_7: 0.7047  loss_dice_7: 1.168  loss_ce_8: 0.6432  loss_cate_8: 0  loss_mask_8: 0.7072  loss_dice_8: 1.182  time: 1.3934  data_time: 0.0120  lr: 9.7414e-06  max_mem: 23800M
[01/01 10:58:18] d2.utils.events INFO:  eta: 14:33:57  iter: 1139  total_loss: 23.78  loss_ce: 0.6205  loss_cate: 0.1706  loss_mask: 0.5982  loss_dice: 0.9815  loss_ce_0: 1.032  loss_cate_0: 0  loss_mask_0: 0.5893  loss_dice_0: 1.055  loss_ce_1: 0.664  loss_cate_1: 0  loss_mask_1: 0.5936  loss_dice_1: 1.062  loss_ce_2: 0.6431  loss_cate_2: 0  loss_mask_2: 0.6172  loss_dice_2: 1.018  loss_ce_3: 0.6366  loss_cate_3: 0  loss_mask_3: 0.605  loss_dice_3: 1.059  loss_ce_4: 0.6423  loss_cate_4: 0  loss_mask_4: 0.6047  loss_dice_4: 0.9858  loss_ce_5: 0.6787  loss_cate_5: 0  loss_mask_5: 0.6119  loss_dice_5: 0.9381  loss_ce_6: 0.6263  loss_cate_6: 0  loss_mask_6: 0.6523  loss_dice_6: 0.9836  loss_ce_7: 0.6413  loss_cate_7: 0  loss_mask_7: 0.6078  loss_dice_7: 0.9861  loss_ce_8: 0.6171  loss_cate_8: 0  loss_mask_8: 0.5953  loss_dice_8: 0.9575  time: 1.3932  data_time: 0.0112  lr: 9.7368e-06  max_mem: 23800M
[01/01 10:58:45] d2.utils.events INFO:  eta: 14:33:26  iter: 1159  total_loss: 26.26  loss_ce: 0.6409  loss_cate: 0.224  loss_mask: 0.8323  loss_dice: 1.073  loss_ce_0: 1.182  loss_cate_0: 0  loss_mask_0: 0.78  loss_dice_0: 0.9189  loss_ce_1: 0.7254  loss_cate_1: 0  loss_mask_1: 0.8011  loss_dice_1: 1.006  loss_ce_2: 0.629  loss_cate_2: 0  loss_mask_2: 0.8009  loss_dice_2: 0.9878  loss_ce_3: 0.6027  loss_cate_3: 0  loss_mask_3: 0.764  loss_dice_3: 1.104  loss_ce_4: 0.6925  loss_cate_4: 0  loss_mask_4: 0.7729  loss_dice_4: 0.9941  loss_ce_5: 0.6455  loss_cate_5: 0  loss_mask_5: 0.8261  loss_dice_5: 1.037  loss_ce_6: 0.6401  loss_cate_6: 0  loss_mask_6: 0.8041  loss_dice_6: 1.05  loss_ce_7: 0.6086  loss_cate_7: 0  loss_mask_7: 0.8058  loss_dice_7: 1  loss_ce_8: 0.6555  loss_cate_8: 0  loss_mask_8: 0.8148  loss_dice_8: 1.067  time: 1.3931  data_time: 0.0116  lr: 9.7321e-06  max_mem: 23800M
[01/01 10:59:13] d2.utils.events INFO:  eta: 14:32:55  iter: 1179  total_loss: 26.52  loss_ce: 0.6797  loss_cate: 0.175  loss_mask: 0.714  loss_dice: 1.098  loss_ce_0: 1.147  loss_cate_0: 0  loss_mask_0: 0.6933  loss_dice_0: 1.073  loss_ce_1: 0.7528  loss_cate_1: 0  loss_mask_1: 0.6822  loss_dice_1: 1.06  loss_ce_2: 0.7121  loss_cate_2: 0  loss_mask_2: 0.7042  loss_dice_2: 1.057  loss_ce_3: 0.7187  loss_cate_3: 0  loss_mask_3: 0.7176  loss_dice_3: 1.045  loss_ce_4: 0.65  loss_cate_4: 0  loss_mask_4: 0.7526  loss_dice_4: 1.074  loss_ce_5: 0.6172  loss_cate_5: 0  loss_mask_5: 0.7234  loss_dice_5: 1.029  loss_ce_6: 0.6626  loss_cate_6: 0  loss_mask_6: 0.7148  loss_dice_6: 1.046  loss_ce_7: 0.7121  loss_cate_7: 0  loss_mask_7: 0.704  loss_dice_7: 1.084  loss_ce_8: 0.6077  loss_cate_8: 0  loss_mask_8: 0.712  loss_dice_8: 1.101  time: 1.3929  data_time: 0.0103  lr: 9.7275e-06  max_mem: 23800M
[01/01 10:59:41] d2.utils.events INFO:  eta: 14:32:24  iter: 1199  total_loss: 26.13  loss_ce: 0.7569  loss_cate: 0.2488  loss_mask: 0.771  loss_dice: 1.124  loss_ce_0: 1.081  loss_cate_0: 0  loss_mask_0: 0.7919  loss_dice_0: 1.139  loss_ce_1: 0.7536  loss_cate_1: 0  loss_mask_1: 0.7395  loss_dice_1: 1.13  loss_ce_2: 0.749  loss_cate_2: 0  loss_mask_2: 0.7363  loss_dice_2: 1.085  loss_ce_3: 0.7331  loss_cate_3: 0  loss_mask_3: 0.7698  loss_dice_3: 1.062  loss_ce_4: 0.7748  loss_cate_4: 0  loss_mask_4: 0.7705  loss_dice_4: 1.194  loss_ce_5: 0.7514  loss_cate_5: 0  loss_mask_5: 0.7944  loss_dice_5: 1.144  loss_ce_6: 0.7619  loss_cate_6: 0  loss_mask_6: 0.791  loss_dice_6: 1.084  loss_ce_7: 0.7242  loss_cate_7: 0  loss_mask_7: 0.7907  loss_dice_7: 1.105  loss_ce_8: 0.8072  loss_cate_8: 0  loss_mask_8: 0.7544  loss_dice_8: 1.113  time: 1.3926  data_time: 0.0108  lr: 9.7229e-06  max_mem: 23813M
[01/01 11:00:08] d2.utils.events INFO:  eta: 14:31:56  iter: 1219  total_loss: 26.24  loss_ce: 0.6503  loss_cate: 0.1846  loss_mask: 0.7982  loss_dice: 0.9859  loss_ce_0: 1.023  loss_cate_0: 0  loss_mask_0: 0.7446  loss_dice_0: 1.071  loss_ce_1: 0.7008  loss_cate_1: 0  loss_mask_1: 0.7915  loss_dice_1: 1.059  loss_ce_2: 0.6821  loss_cate_2: 0  loss_mask_2: 0.7534  loss_dice_2: 1.018  loss_ce_3: 0.6739  loss_cate_3: 0  loss_mask_3: 0.7229  loss_dice_3: 1.045  loss_ce_4: 0.6008  loss_cate_4: 0  loss_mask_4: 0.7688  loss_dice_4: 1.09  loss_ce_5: 0.7448  loss_cate_5: 0  loss_mask_5: 0.696  loss_dice_5: 1.077  loss_ce_6: 0.6798  loss_cate_6: 0  loss_mask_6: 0.7862  loss_dice_6: 0.9953  loss_ce_7: 0.6061  loss_cate_7: 0  loss_mask_7: 0.8193  loss_dice_7: 0.9669  loss_ce_8: 0.655  loss_cate_8: 0  loss_mask_8: 0.8154  loss_dice_8: 1.011  time: 1.3924  data_time: 0.0121  lr: 9.7182e-06  max_mem: 23813M
[01/01 11:00:36] d2.utils.events INFO:  eta: 14:31:33  iter: 1239  total_loss: 26.61  loss_ce: 0.6845  loss_cate: 0.2296  loss_mask: 0.741  loss_dice: 1.208  loss_ce_0: 1.104  loss_cate_0: 0  loss_mask_0: 0.761  loss_dice_0: 1.123  loss_ce_1: 0.611  loss_cate_1: 0  loss_mask_1: 0.7715  loss_dice_1: 1.207  loss_ce_2: 0.6378  loss_cate_2: 0  loss_mask_2: 0.77  loss_dice_2: 1.159  loss_ce_3: 0.6848  loss_cate_3: 0  loss_mask_3: 0.7413  loss_dice_3: 1.194  loss_ce_4: 0.6604  loss_cate_4: 0  loss_mask_4: 0.7586  loss_dice_4: 1.182  loss_ce_5: 0.6382  loss_cate_5: 0  loss_mask_5: 0.7605  loss_dice_5: 1.167  loss_ce_6: 0.7422  loss_cate_6: 0  loss_mask_6: 0.7314  loss_dice_6: 1.122  loss_ce_7: 0.6823  loss_cate_7: 0  loss_mask_7: 0.7315  loss_dice_7: 1.204  loss_ce_8: 0.6837  loss_cate_8: 0  loss_mask_8: 0.7385  loss_dice_8: 1.214  time: 1.3921  data_time: 0.0122  lr: 9.7136e-06  max_mem: 23813M
[01/01 11:01:04] d2.utils.events INFO:  eta: 14:31:14  iter: 1259  total_loss: 25.63  loss_ce: 0.6726  loss_cate: 0.1769  loss_mask: 0.8104  loss_dice: 0.997  loss_ce_0: 1.104  loss_cate_0: 0  loss_mask_0: 0.745  loss_dice_0: 0.9472  loss_ce_1: 0.7835  loss_cate_1: 0  loss_mask_1: 0.7307  loss_dice_1: 0.9472  loss_ce_2: 0.7557  loss_cate_2: 0  loss_mask_2: 0.8503  loss_dice_2: 1.083  loss_ce_3: 0.6897  loss_cate_3: 0  loss_mask_3: 0.7442  loss_dice_3: 1.012  loss_ce_4: 0.7755  loss_cate_4: 0  loss_mask_4: 0.6887  loss_dice_4: 0.9731  loss_ce_5: 0.749  loss_cate_5: 0  loss_mask_5: 0.751  loss_dice_5: 0.9314  loss_ce_6: 0.676  loss_cate_6: 0  loss_mask_6: 0.7582  loss_dice_6: 0.9869  loss_ce_7: 0.6829  loss_cate_7: 0  loss_mask_7: 0.787  loss_dice_7: 0.9877  loss_ce_8: 0.666  loss_cate_8: 0  loss_mask_8: 0.7285  loss_dice_8: 0.9874  time: 1.3921  data_time: 0.0127  lr: 9.709e-06  max_mem: 23813M
[01/01 11:01:32] d2.utils.events INFO:  eta: 14:30:58  iter: 1279  total_loss: 23.59  loss_ce: 0.535  loss_cate: 0.2121  loss_mask: 0.7879  loss_dice: 1.128  loss_ce_0: 1.015  loss_cate_0: 0  loss_mask_0: 0.7965  loss_dice_0: 1.066  loss_ce_1: 0.817  loss_cate_1: 0  loss_mask_1: 0.8457  loss_dice_1: 0.9714  loss_ce_2: 0.7424  loss_cate_2: 0  loss_mask_2: 0.8845  loss_dice_2: 1.047  loss_ce_3: 0.729  loss_cate_3: 0  loss_mask_3: 0.7799  loss_dice_3: 0.997  loss_ce_4: 0.6467  loss_cate_4: 0  loss_mask_4: 0.8102  loss_dice_4: 1.05  loss_ce_5: 0.6083  loss_cate_5: 0  loss_mask_5: 0.8302  loss_dice_5: 0.9234  loss_ce_6: 0.635  loss_cate_6: 0  loss_mask_6: 0.8289  loss_dice_6: 0.8978  loss_ce_7: 0.6231  loss_cate_7: 0  loss_mask_7: 0.8692  loss_dice_7: 0.9477  loss_ce_8: 0.5953  loss_cate_8: 0  loss_mask_8: 0.7718  loss_dice_8: 0.9553  time: 1.3923  data_time: 0.0120  lr: 9.7044e-06  max_mem: 23813M
[01/01 11:02:00] d2.data.datasets.coco INFO: Loaded 653 images with semantic segmentation from /opt/ml/input/data/images/val_0
[01/01 11:02:00] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=1280, sample_style='choice')]
[01/01 11:02:00] d2.data.common INFO: Serializing 653 elements to byte tensors and concatenating them all ...
[01/01 11:02:00] d2.data.common INFO: Serialized dataset takes 0.09 MiB
[01/01 11:02:00] d2.data.datasets.coco INFO: Loaded 653 images with semantic segmentation from /opt/ml/input/data/images/val_0
[01/01 11:02:00] d2.evaluation.evaluator INFO: Start inference on 653 batches
[01/01 11:02:01] d2.evaluation.evaluator INFO: Inference done 11/653. Dataloading: 0.0010 s/iter. Inference: 0.1053 s/iter. Eval: 0.0116 s/iter. Total: 0.1179 s/iter. ETA=0:01:15
[01/01 11:02:06] d2.evaluation.evaluator INFO: Inference done 53/653. Dataloading: 0.0016 s/iter. Inference: 0.1059 s/iter. Eval: 0.0115 s/iter. Total: 0.1191 s/iter. ETA=0:01:11
[01/01 11:02:11] d2.evaluation.evaluator INFO: Inference done 96/653. Dataloading: 0.0016 s/iter. Inference: 0.1055 s/iter. Eval: 0.0114 s/iter. Total: 0.1186 s/iter. ETA=0:01:06
[01/01 11:02:16] d2.evaluation.evaluator INFO: Inference done 139/653. Dataloading: 0.0016 s/iter. Inference: 0.1054 s/iter. Eval: 0.0112 s/iter. Total: 0.1183 s/iter. ETA=0:01:00
[01/01 11:02:22] d2.evaluation.evaluator INFO: Inference done 182/653. Dataloading: 0.0016 s/iter. Inference: 0.1053 s/iter. Eval: 0.0112 s/iter. Total: 0.1182 s/iter. ETA=0:00:55
[01/01 11:02:27] d2.evaluation.evaluator INFO: Inference done 225/653. Dataloading: 0.0015 s/iter. Inference: 0.1053 s/iter. Eval: 0.0112 s/iter. Total: 0.1181 s/iter. ETA=0:00:50
[01/01 11:02:32] d2.evaluation.evaluator INFO: Inference done 268/653. Dataloading: 0.0016 s/iter. Inference: 0.1053 s/iter. Eval: 0.0113 s/iter. Total: 0.1182 s/iter. ETA=0:00:45
[01/01 11:02:37] d2.evaluation.evaluator INFO: Inference done 310/653. Dataloading: 0.0016 s/iter. Inference: 0.1054 s/iter. Eval: 0.0114 s/iter. Total: 0.1184 s/iter. ETA=0:00:40
[01/01 11:02:42] d2.evaluation.evaluator INFO: Inference done 352/653. Dataloading: 0.0016 s/iter. Inference: 0.1054 s/iter. Eval: 0.0115 s/iter. Total: 0.1186 s/iter. ETA=0:00:35
[01/01 11:02:47] d2.evaluation.evaluator INFO: Inference done 394/653. Dataloading: 0.0016 s/iter. Inference: 0.1055 s/iter. Eval: 0.0116 s/iter. Total: 0.1187 s/iter. ETA=0:00:30
[01/01 11:02:52] d2.evaluation.evaluator INFO: Inference done 436/653. Dataloading: 0.0016 s/iter. Inference: 0.1055 s/iter. Eval: 0.0116 s/iter. Total: 0.1188 s/iter. ETA=0:00:25
[01/01 11:02:57] d2.evaluation.evaluator INFO: Inference done 478/653. Dataloading: 0.0016 s/iter. Inference: 0.1056 s/iter. Eval: 0.0117 s/iter. Total: 0.1189 s/iter. ETA=0:00:20
[01/01 11:03:02] d2.evaluation.evaluator INFO: Inference done 521/653. Dataloading: 0.0016 s/iter. Inference: 0.1056 s/iter. Eval: 0.0116 s/iter. Total: 0.1189 s/iter. ETA=0:00:15
[01/01 11:03:07] d2.evaluation.evaluator INFO: Inference done 563/653. Dataloading: 0.0016 s/iter. Inference: 0.1056 s/iter. Eval: 0.0116 s/iter. Total: 0.1189 s/iter. ETA=0:00:10
[01/01 11:03:12] d2.evaluation.evaluator INFO: Inference done 606/653. Dataloading: 0.0017 s/iter. Inference: 0.1056 s/iter. Eval: 0.0116 s/iter. Total: 0.1189 s/iter. ETA=0:00:05
[01/01 11:03:17] d2.evaluation.evaluator INFO: Inference done 649/653. Dataloading: 0.0016 s/iter. Inference: 0.1056 s/iter. Eval: 0.0116 s/iter. Total: 0.1189 s/iter. ETA=0:00:00
[01/01 11:03:18] d2.evaluation.evaluator INFO: Total inference time: 0:01:17.100066 (0.118982 s / iter per device, on 1 devices)
[01/01 11:03:18] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:01:08 (0.105617 s / iter per device, on 1 devices)
[01/01 11:03:18] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 44.99854153559936, 'fwIoU': 87.22783135629543, 'IoU-Backgroud': 96.57319906315638, 'IoU-General trash': 26.56381594210992, 'IoU-Paper': 71.48436897586467, 'IoU-Paper pack': 13.965031357518342, 'IoU-Metal': 44.26313527812986, 'IoU-Glass': 53.106934782329695, 'IoU-Plastic': 44.413365623601486, 'IoU-Styrofoam': 63.37983939968702, 'IoU-Plastic bag': 81.04409162618063, 'IoU-Battery': 0.0, 'IoU-Clothing': 0.19017484301486673, 'mACC': 55.81265780964853, 'pACC': 91.79688668360572, 'ACC-Backgroud': 97.81476280643717, 'ACC-General trash': 51.30554975538957, 'ACC-Paper': 79.08319513924545, 'ACC-Paper pack': 14.660487748985796, 'ACC-Metal': 58.833621555363095, 'ACC-Glass': 58.5447604828745, 'ACC-Plastic': 76.80577355779513, 'ACC-Styrofoam': 86.26925349443619, 'ACC-Plastic bag': 90.43152459158898, 'ACC-Battery': 0.0, 'ACC-Clothing': 0.19030677401791918})])
[01/01 11:03:18] d2.engine.defaults INFO: Evaluation results for trash_recycle_sem_seg_val_0 in csv format:
[01/01 11:03:18] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/01 11:03:18] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/01 11:03:18] d2.evaluation.testing INFO: copypaste: 44.9985,87.2278,55.8127,91.7969
[01/01 11:03:18] fvcore.common.checkpoint INFO: Saving checkpoint to ./trash_dataV1_WarmupPolyLR_1e-5/model_best_1299iter.pth
[01/01 11:03:20] d2.engine.hooks INFO: Saved first model at 44.99854 @ 1299 steps
[01/01 11:03:20] d2.utils.events INFO:  eta: 14:30:35  iter: 1299  total_loss: 27.01  loss_ce: 0.7113  loss_cate: 0.2026  loss_mask: 0.7931  loss_dice: 1.276  loss_ce_0: 1.153  loss_cate_0: 0  loss_mask_0: 0.8909  loss_dice_0: 1.226  loss_ce_1: 0.7471  loss_cate_1: 0  loss_mask_1: 0.8602  loss_dice_1: 1.176  loss_ce_2: 0.7497  loss_cate_2: 0  loss_mask_2: 0.8144  loss_dice_2: 1.208  loss_ce_3: 0.7202  loss_cate_3: 0  loss_mask_3: 0.7873  loss_dice_3: 1.247  loss_ce_4: 0.7631  loss_cate_4: 0  loss_mask_4: 0.8378  loss_dice_4: 1.248  loss_ce_5: 0.7824  loss_cate_5: 0  loss_mask_5: 0.8  loss_dice_5: 1.156  loss_ce_6: 0.7927  loss_cate_6: 0  loss_mask_6: 0.8165  loss_dice_6: 1.19  loss_ce_7: 0.7474  loss_cate_7: 0  loss_mask_7: 0.873  loss_dice_7: 1.176  loss_ce_8: 0.7275  loss_cate_8: 0  loss_mask_8: 0.85  loss_dice_8: 1.2  time: 1.3923  data_time: 0.0147  lr: 9.6997e-06  max_mem: 23813M
[01/01 11:03:48] d2.utils.events INFO:  eta: 14:30:23  iter: 1319  total_loss: 28.46  loss_ce: 0.8296  loss_cate: 0.2078  loss_mask: 0.693  loss_dice: 1.197  loss_ce_0: 1.26  loss_cate_0: 0  loss_mask_0: 0.6651  loss_dice_0: 1.247  loss_ce_1: 0.8333  loss_cate_1: 0  loss_mask_1: 0.6056  loss_dice_1: 1.261  loss_ce_2: 0.8504  loss_cate_2: 0  loss_mask_2: 0.6032  loss_dice_2: 1.301  loss_ce_3: 0.8076  loss_cate_3: 0  loss_mask_3: 0.6268  loss_dice_3: 1.256  loss_ce_4: 0.7996  loss_cate_4: 0  loss_mask_4: 0.6714  loss_dice_4: 1.251  loss_ce_5: 0.7924  loss_cate_5: 0  loss_mask_5: 0.6255  loss_dice_5: 1.266  loss_ce_6: 0.8459  loss_cate_6: 0  loss_mask_6: 0.6717  loss_dice_6: 1.19  loss_ce_7: 0.8189  loss_cate_7: 0  loss_mask_7: 0.6781  loss_dice_7: 1.205  loss_ce_8: 0.7991  loss_cate_8: 0  loss_mask_8: 0.6684  loss_dice_8: 1.189  time: 1.3923  data_time: 0.0131  lr: 9.6951e-06  max_mem: 23813M
[01/01 11:04:16] d2.utils.events INFO:  eta: 14:30:19  iter: 1339  total_loss: 25.45  loss_ce: 0.7672  loss_cate: 0.2122  loss_mask: 0.7097  loss_dice: 1.01  loss_ce_0: 1.167  loss_cate_0: 0  loss_mask_0: 0.6847  loss_dice_0: 1.097  loss_ce_1: 0.856  loss_cate_1: 0  loss_mask_1: 0.6078  loss_dice_1: 1.135  loss_ce_2: 0.8235  loss_cate_2: 0  loss_mask_2: 0.6521  loss_dice_2: 1.023  loss_ce_3: 0.8157  loss_cate_3: 0  loss_mask_3: 0.6278  loss_dice_3: 1.017  loss_ce_4: 0.7679  loss_cate_4: 0  loss_mask_4: 0.6141  loss_dice_4: 1.044  loss_ce_5: 0.7938  loss_cate_5: 0  loss_mask_5: 0.677  loss_dice_5: 1.015  loss_ce_6: 0.713  loss_cate_6: 0  loss_mask_6: 0.6798  loss_dice_6: 0.9909  loss_ce_7: 0.7512  loss_cate_7: 0  loss_mask_7: 0.6762  loss_dice_7: 0.9897  loss_ce_8: 0.7584  loss_cate_8: 0  loss_mask_8: 0.6693  loss_dice_8: 1.034  time: 1.3924  data_time: 0.0133  lr: 9.6905e-06  max_mem: 23813M
[01/01 11:04:43] d2.utils.events INFO:  eta: 14:30:06  iter: 1359  total_loss: 25.72  loss_ce: 0.5179  loss_cate: 0.2155  loss_mask: 0.714  loss_dice: 0.9358  loss_ce_0: 0.9163  loss_cate_0: 0  loss_mask_0: 0.7541  loss_dice_0: 1.059  loss_ce_1: 0.6202  loss_cate_1: 0  loss_mask_1: 0.7662  loss_dice_1: 1.019  loss_ce_2: 0.4974  loss_cate_2: 0  loss_mask_2: 0.7371  loss_dice_2: 0.9552  loss_ce_3: 0.5886  loss_cate_3: 0  loss_mask_3: 0.7373  loss_dice_3: 0.9554  loss_ce_4: 0.5935  loss_cate_4: 0  loss_mask_4: 0.6798  loss_dice_4: 0.8041  loss_ce_5: 0.544  loss_cate_5: 0  loss_mask_5: 0.7034  loss_dice_5: 0.9251  loss_ce_6: 0.5834  loss_cate_6: 0  loss_mask_6: 0.705  loss_dice_6: 0.9242  loss_ce_7: 0.6079  loss_cate_7: 0  loss_mask_7: 0.7553  loss_dice_7: 0.8712  loss_ce_8: 0.5813  loss_cate_8: 0  loss_mask_8: 0.7141  loss_dice_8: 0.8909  time: 1.3923  data_time: 0.0137  lr: 9.6858e-06  max_mem: 23813M
[01/01 11:05:11] d2.utils.events INFO:  eta: 14:29:53  iter: 1379  total_loss: 23.84  loss_ce: 0.6569  loss_cate: 0.1875  loss_mask: 0.6385  loss_dice: 1.04  loss_ce_0: 1.075  loss_cate_0: 0  loss_mask_0: 0.5806  loss_dice_0: 1.091  loss_ce_1: 0.7242  loss_cate_1: 0  loss_mask_1: 0.5941  loss_dice_1: 1.066  loss_ce_2: 0.6288  loss_cate_2: 0  loss_mask_2: 0.6068  loss_dice_2: 1.012  loss_ce_3: 0.6101  loss_cate_3: 0  loss_mask_3: 0.5853  loss_dice_3: 1.127  loss_ce_4: 0.6406  loss_cate_4: 0  loss_mask_4: 0.6092  loss_dice_4: 1.135  loss_ce_5: 0.6255  loss_cate_5: 0  loss_mask_5: 0.6315  loss_dice_5: 1.076  loss_ce_6: 0.6792  loss_cate_6: 0  loss_mask_6: 0.592  loss_dice_6: 1.018  loss_ce_7: 0.7196  loss_cate_7: 0  loss_mask_7: 0.6273  loss_dice_7: 1.017  loss_ce_8: 0.6898  loss_cate_8: 0  loss_mask_8: 0.625  loss_dice_8: 1.038  time: 1.3923  data_time: 0.0106  lr: 9.6812e-06  max_mem: 23813M
[01/01 11:05:39] d2.utils.events INFO:  eta: 14:29:42  iter: 1399  total_loss: 26.16  loss_ce: 0.5968  loss_cate: 0.1942  loss_mask: 0.6176  loss_dice: 1.32  loss_ce_0: 1.14  loss_cate_0: 0  loss_mask_0: 0.6358  loss_dice_0: 1.284  loss_ce_1: 0.7364  loss_cate_1: 0  loss_mask_1: 0.7069  loss_dice_1: 1.333  loss_ce_2: 0.6327  loss_cate_2: 0  loss_mask_2: 0.6918  loss_dice_2: 1.384  loss_ce_3: 0.7494  loss_cate_3: 0  loss_mask_3: 0.6555  loss_dice_3: 1.389  loss_ce_4: 0.6606  loss_cate_4: 0  loss_mask_4: 0.6589  loss_dice_4: 1.351  loss_ce_5: 0.6671  loss_cate_5: 0  loss_mask_5: 0.6621  loss_dice_5: 1.378  loss_ce_6: 0.763  loss_cate_6: 0  loss_mask_6: 0.6259  loss_dice_6: 1.339  loss_ce_7: 0.6386  loss_cate_7: 0  loss_mask_7: 0.6466  loss_dice_7: 1.382  loss_ce_8: 0.6709  loss_cate_8: 0  loss_mask_8: 0.63  loss_dice_8: 1.289  time: 1.3923  data_time: 0.0109  lr: 9.6766e-06  max_mem: 23813M
[01/01 11:06:07] d2.utils.events INFO:  eta: 14:29:29  iter: 1419  total_loss: 24.16  loss_ce: 0.5831  loss_cate: 0.171  loss_mask: 0.7808  loss_dice: 0.9904  loss_ce_0: 0.9348  loss_cate_0: 0  loss_mask_0: 0.8138  loss_dice_0: 0.9877  loss_ce_1: 0.5831  loss_cate_1: 0  loss_mask_1: 0.7187  loss_dice_1: 1.002  loss_ce_2: 0.6335  loss_cate_2: 0  loss_mask_2: 0.6961  loss_dice_2: 0.9461  loss_ce_3: 0.6774  loss_cate_3: 0  loss_mask_3: 0.7309  loss_dice_3: 0.9435  loss_ce_4: 0.5975  loss_cate_4: 0  loss_mask_4: 0.7454  loss_dice_4: 0.9588  loss_ce_5: 0.6332  loss_cate_5: 0  loss_mask_5: 0.7839  loss_dice_5: 0.9646  loss_ce_6: 0.5943  loss_cate_6: 0  loss_mask_6: 0.7781  loss_dice_6: 0.9328  loss_ce_7: 0.5915  loss_cate_7: 0  loss_mask_7: 0.8  loss_dice_7: 0.9371  loss_ce_8: 0.6283  loss_cate_8: 0  loss_mask_8: 0.8241  loss_dice_8: 0.9101  time: 1.3922  data_time: 0.0133  lr: 9.6719e-06  max_mem: 23813M
[01/01 11:06:35] d2.utils.events INFO:  eta: 14:29:07  iter: 1439  total_loss: 27.58  loss_ce: 0.6561  loss_cate: 0.1709  loss_mask: 0.769  loss_dice: 1.217  loss_ce_0: 1.162  loss_cate_0: 0  loss_mask_0: 0.637  loss_dice_0: 1.267  loss_ce_1: 0.6747  loss_cate_1: 0  loss_mask_1: 0.6779  loss_dice_1: 1.292  loss_ce_2: 0.6966  loss_cate_2: 0  loss_mask_2: 0.8222  loss_dice_2: 1.197  loss_ce_3: 0.7111  loss_cate_3: 0  loss_mask_3: 0.739  loss_dice_3: 1.205  loss_ce_4: 0.7116  loss_cate_4: 0  loss_mask_4: 0.7781  loss_dice_4: 1.24  loss_ce_5: 0.707  loss_cate_5: 0  loss_mask_5: 0.8355  loss_dice_5: 1.212  loss_ce_6: 0.7375  loss_cate_6: 0  loss_mask_6: 0.7478  loss_dice_6: 1.214  loss_ce_7: 0.6991  loss_cate_7: 0  loss_mask_7: 0.7441  loss_dice_7: 1.236  loss_ce_8: 0.6751  loss_cate_8: 0  loss_mask_8: 0.7559  loss_dice_8: 1.226  time: 1.3920  data_time: 0.0124  lr: 9.6673e-06  max_mem: 23813M
[01/01 11:07:03] d2.utils.events INFO:  eta: 14:28:54  iter: 1459  total_loss: 27.33  loss_ce: 0.6713  loss_cate: 0.2125  loss_mask: 0.7114  loss_dice: 1.147  loss_ce_0: 1.04  loss_cate_0: 0  loss_mask_0: 0.8497  loss_dice_0: 0.9823  loss_ce_1: 0.686  loss_cate_1: 0  loss_mask_1: 0.8401  loss_dice_1: 1.178  loss_ce_2: 0.5737  loss_cate_2: 0  loss_mask_2: 0.7687  loss_dice_2: 1.028  loss_ce_3: 0.6211  loss_cate_3: 0  loss_mask_3: 0.7862  loss_dice_3: 1.094  loss_ce_4: 0.5925  loss_cate_4: 0  loss_mask_4: 0.7753  loss_dice_4: 1.121  loss_ce_5: 0.6087  loss_cate_5: 0  loss_mask_5: 0.7677  loss_dice_5: 1.092  loss_ce_6: 0.5099  loss_cate_6: 0  loss_mask_6: 0.8212  loss_dice_6: 1.118  loss_ce_7: 0.5467  loss_cate_7: 0  loss_mask_7: 0.8069  loss_dice_7: 1.109  loss_ce_8: 0.6169  loss_cate_8: 0  loss_mask_8: 0.7714  loss_dice_8: 1.173  time: 1.3922  data_time: 0.0131  lr: 9.6627e-06  max_mem: 23813M
[01/01 11:07:31] d2.utils.events INFO:  eta: 14:28:39  iter: 1479  total_loss: 24.8  loss_ce: 0.7104  loss_cate: 0.1673  loss_mask: 0.4985  loss_dice: 1.085  loss_ce_0: 1.044  loss_cate_0: 0  loss_mask_0: 0.4846  loss_dice_0: 1.062  loss_ce_1: 0.7825  loss_cate_1: 0  loss_mask_1: 0.5229  loss_dice_1: 1.047  loss_ce_2: 0.6994  loss_cate_2: 0  loss_mask_2: 0.5138  loss_dice_2: 1.031  loss_ce_3: 0.7792  loss_cate_3: 0  loss_mask_3: 0.511  loss_dice_3: 1.031  loss_ce_4: 0.6417  loss_cate_4: 0  loss_mask_4: 0.5012  loss_dice_4: 1.023  loss_ce_5: 0.6224  loss_cate_5: 0  loss_mask_5: 0.5081  loss_dice_5: 1.084  loss_ce_6: 0.7023  loss_cate_6: 0  loss_mask_6: 0.5136  loss_dice_6: 1.099  loss_ce_7: 0.6647  loss_cate_7: 0  loss_mask_7: 0.5022  loss_dice_7: 1.081  loss_ce_8: 0.6545  loss_cate_8: 0  loss_mask_8: 0.5118  loss_dice_8: 1.105  time: 1.3921  data_time: 0.0162  lr: 9.658e-06  max_mem: 23813M
[01/01 11:07:58] d2.utils.events INFO:  eta: 14:28:12  iter: 1499  total_loss: 25.13  loss_ce: 0.6994  loss_cate: 0.1947  loss_mask: 0.7821  loss_dice: 0.9739  loss_ce_0: 0.9872  loss_cate_0: 0  loss_mask_0: 0.7167  loss_dice_0: 0.9682  loss_ce_1: 0.696  loss_cate_1: 0  loss_mask_1: 0.7  loss_dice_1: 0.9947  loss_ce_2: 0.6452  loss_cate_2: 0  loss_mask_2: 0.7473  loss_dice_2: 0.9614  loss_ce_3: 0.7295  loss_cate_3: 0  loss_mask_3: 0.6984  loss_dice_3: 0.9208  loss_ce_4: 0.6686  loss_cate_4: 0  loss_mask_4: 0.6962  loss_dice_4: 0.9717  loss_ce_5: 0.6571  loss_cate_5: 0  loss_mask_5: 0.8337  loss_dice_5: 0.9584  loss_ce_6: 0.6324  loss_cate_6: 0  loss_mask_6: 0.788  loss_dice_6: 0.9513  loss_ce_7: 0.645  loss_cate_7: 0  loss_mask_7: 0.7483  loss_dice_7: 1.005  loss_ce_8: 0.7934  loss_cate_8: 0  loss_mask_8: 0.7846  loss_dice_8: 0.9503  time: 1.3920  data_time: 0.0121  lr: 9.6534e-06  max_mem: 23813M
[01/01 11:08:26] d2.utils.events INFO:  eta: 14:27:27  iter: 1519  total_loss: 28.93  loss_ce: 0.7025  loss_cate: 0.2002  loss_mask: 0.692  loss_dice: 1.208  loss_ce_0: 1.117  loss_cate_0: 0  loss_mask_0: 0.6758  loss_dice_0: 1.218  loss_ce_1: 0.7541  loss_cate_1: 0  loss_mask_1: 0.6892  loss_dice_1: 1.202  loss_ce_2: 0.7141  loss_cate_2: 0  loss_mask_2: 0.6567  loss_dice_2: 1.163  loss_ce_3: 0.7691  loss_cate_3: 0  loss_mask_3: 0.6926  loss_dice_3: 1.144  loss_ce_4: 0.682  loss_cate_4: 0  loss_mask_4: 0.6983  loss_dice_4: 1.182  loss_ce_5: 0.7727  loss_cate_5: 0  loss_mask_5: 0.6468  loss_dice_5: 1.165  loss_ce_6: 0.7845  loss_cate_6: 0  loss_mask_6: 0.6798  loss_dice_6: 1.17  loss_ce_7: 0.7912  loss_cate_7: 0  loss_mask_7: 0.6906  loss_dice_7: 1.17  loss_ce_8: 0.7215  loss_cate_8: 0  loss_mask_8: 0.6888  loss_dice_8: 1.136  time: 1.3918  data_time: 0.0102  lr: 9.6488e-06  max_mem: 23813M
[01/01 11:08:53] d2.utils.events INFO:  eta: 14:26:51  iter: 1539  total_loss: 23.47  loss_ce: 0.5281  loss_cate: 0.1909  loss_mask: 0.5956  loss_dice: 0.9106  loss_ce_0: 0.9655  loss_cate_0: 0  loss_mask_0: 0.6961  loss_dice_0: 0.9402  loss_ce_1: 0.5657  loss_cate_1: 0  loss_mask_1: 0.7677  loss_dice_1: 0.905  loss_ce_2: 0.5887  loss_cate_2: 0  loss_mask_2: 0.6956  loss_dice_2: 0.9309  loss_ce_3: 0.597  loss_cate_3: 0  loss_mask_3: 0.6196  loss_dice_3: 0.8946  loss_ce_4: 0.5718  loss_cate_4: 0  loss_mask_4: 0.6091  loss_dice_4: 0.9402  loss_ce_5: 0.5524  loss_cate_5: 0  loss_mask_5: 0.5867  loss_dice_5: 0.9035  loss_ce_6: 0.5704  loss_cate_6: 0  loss_mask_6: 0.6575  loss_dice_6: 0.922  loss_ce_7: 0.5057  loss_cate_7: 0  loss_mask_7: 0.6497  loss_dice_7: 0.8853  loss_ce_8: 0.5861  loss_cate_8: 0  loss_mask_8: 0.5786  loss_dice_8: 0.8632  time: 1.3916  data_time: 0.0100  lr: 9.6441e-06  max_mem: 23813M
[01/01 11:09:21] d2.utils.events INFO:  eta: 14:26:15  iter: 1559  total_loss: 25.29  loss_ce: 0.6506  loss_cate: 0.1918  loss_mask: 0.6364  loss_dice: 0.9235  loss_ce_0: 1.154  loss_cate_0: 0  loss_mask_0: 0.7304  loss_dice_0: 1.064  loss_ce_1: 0.7027  loss_cate_1: 0  loss_mask_1: 0.7666  loss_dice_1: 1  loss_ce_2: 0.6267  loss_cate_2: 0  loss_mask_2: 0.7087  loss_dice_2: 1.004  loss_ce_3: 0.722  loss_cate_3: 0  loss_mask_3: 0.6973  loss_dice_3: 0.9631  loss_ce_4: 0.6662  loss_cate_4: 0  loss_mask_4: 0.6533  loss_dice_4: 1.044  loss_ce_5: 0.7533  loss_cate_5: 0  loss_mask_5: 0.7023  loss_dice_5: 0.9929  loss_ce_6: 0.7637  loss_cate_6: 0  loss_mask_6: 0.6207  loss_dice_6: 0.9537  loss_ce_7: 0.6666  loss_cate_7: 0  loss_mask_7: 0.6235  loss_dice_7: 0.9658  loss_ce_8: 0.681  loss_cate_8: 0  loss_mask_8: 0.6335  loss_dice_8: 1.009  time: 1.3914  data_time: 0.0111  lr: 9.6395e-06  max_mem: 23813M
[01/01 11:09:49] d2.utils.events INFO:  eta: 14:25:24  iter: 1579  total_loss: 25.93  loss_ce: 0.6711  loss_cate: 0.1652  loss_mask: 0.6258  loss_dice: 1.014  loss_ce_0: 1.053  loss_cate_0: 0  loss_mask_0: 0.6856  loss_dice_0: 0.9919  loss_ce_1: 0.6206  loss_cate_1: 0  loss_mask_1: 0.6798  loss_dice_1: 1.023  loss_ce_2: 0.6573  loss_cate_2: 0  loss_mask_2: 0.6278  loss_dice_2: 0.9961  loss_ce_3: 0.6654  loss_cate_3: 0  loss_mask_3: 0.7365  loss_dice_3: 1  loss_ce_4: 0.6362  loss_cate_4: 0  loss_mask_4: 0.6689  loss_dice_4: 1.025  loss_ce_5: 0.616  loss_cate_5: 0  loss_mask_5: 0.6469  loss_dice_5: 0.976  loss_ce_6: 0.6106  loss_cate_6: 0  loss_mask_6: 0.629  loss_dice_6: 0.9613  loss_ce_7: 0.6171  loss_cate_7: 0  loss_mask_7: 0.6311  loss_dice_7: 0.9709  loss_ce_8: 0.596  loss_cate_8: 0  loss_mask_8: 0.6457  loss_dice_8: 0.9982  time: 1.3913  data_time: 0.0130  lr: 9.6349e-06  max_mem: 23813M
[01/01 11:10:16] d2.utils.events INFO:  eta: 14:24:41  iter: 1599  total_loss: 22.5  loss_ce: 0.5576  loss_cate: 0.1729  loss_mask: 0.8221  loss_dice: 0.7978  loss_ce_0: 0.987  loss_cate_0: 0  loss_mask_0: 0.6349  loss_dice_0: 0.8523  loss_ce_1: 0.6848  loss_cate_1: 0  loss_mask_1: 0.7114  loss_dice_1: 0.914  loss_ce_2: 0.5386  loss_cate_2: 0  loss_mask_2: 0.7506  loss_dice_2: 0.8146  loss_ce_3: 0.5687  loss_cate_3: 0  loss_mask_3: 0.7952  loss_dice_3: 0.9203  loss_ce_4: 0.5615  loss_cate_4: 0  loss_mask_4: 0.7905  loss_dice_4: 0.9281  loss_ce_5: 0.5819  loss_cate_5: 0  loss_mask_5: 0.832  loss_dice_5: 0.8951  loss_ce_6: 0.66  loss_cate_6: 0  loss_mask_6: 0.7144  loss_dice_6: 0.8274  loss_ce_7: 0.629  loss_cate_7: 0  loss_mask_7: 0.7523  loss_dice_7: 0.8368  loss_ce_8: 0.5951  loss_cate_8: 0  loss_mask_8: 0.7588  loss_dice_8: 0.8104  time: 1.3912  data_time: 0.0112  lr: 9.6302e-06  max_mem: 23813M
[01/01 11:10:44] d2.utils.events INFO:  eta: 14:23:39  iter: 1619  total_loss: 21.85  loss_ce: 0.6164  loss_cate: 0.1796  loss_mask: 0.5569  loss_dice: 0.9014  loss_ce_0: 1.076  loss_cate_0: 0  loss_mask_0: 0.535  loss_dice_0: 0.9159  loss_ce_1: 0.6297  loss_cate_1: 0  loss_mask_1: 0.529  loss_dice_1: 0.9344  loss_ce_2: 0.5839  loss_cate_2: 0  loss_mask_2: 0.5446  loss_dice_2: 0.8359  loss_ce_3: 0.6441  loss_cate_3: 0  loss_mask_3: 0.5217  loss_dice_3: 0.8431  loss_ce_4: 0.6125  loss_cate_4: 0  loss_mask_4: 0.5214  loss_dice_4: 0.841  loss_ce_5: 0.5785  loss_cate_5: 0  loss_mask_5: 0.5365  loss_dice_5: 0.8291  loss_ce_6: 0.6182  loss_cate_6: 0  loss_mask_6: 0.5485  loss_dice_6: 0.8634  loss_ce_7: 0.6537  loss_cate_7: 0  loss_mask_7: 0.5447  loss_dice_7: 0.8576  loss_ce_8: 0.6582  loss_cate_8: 0  loss_mask_8: 0.4982  loss_dice_8: 0.8397  time: 1.3910  data_time: 0.0108  lr: 9.6256e-06  max_mem: 23813M
[01/01 11:11:12] d2.utils.events INFO:  eta: 14:22:59  iter: 1639  total_loss: 24.74  loss_ce: 0.6368  loss_cate: 0.1744  loss_mask: 0.8175  loss_dice: 1.05  loss_ce_0: 0.875  loss_cate_0: 0  loss_mask_0: 0.7512  loss_dice_0: 1.065  loss_ce_1: 0.6702  loss_cate_1: 0  loss_mask_1: 0.8431  loss_dice_1: 1.071  loss_ce_2: 0.5673  loss_cate_2: 0  loss_mask_2: 0.8404  loss_dice_2: 1.009  loss_ce_3: 0.6626  loss_cate_3: 0  loss_mask_3: 0.8965  loss_dice_3: 1.006  loss_ce_4: 0.7006  loss_cate_4: 0  loss_mask_4: 0.8754  loss_dice_4: 1.034  loss_ce_5: 0.669  loss_cate_5: 0  loss_mask_5: 0.8715  loss_dice_5: 0.9838  loss_ce_6: 0.6625  loss_cate_6: 0  loss_mask_6: 0.852  loss_dice_6: 1.032  loss_ce_7: 0.6763  loss_cate_7: 0  loss_mask_7: 0.84  loss_dice_7: 1.095  loss_ce_8: 0.641  loss_cate_8: 0  loss_mask_8: 0.8192  loss_dice_8: 1.08  time: 1.3908  data_time: 0.0104  lr: 9.621e-06  max_mem: 23815M
[01/01 11:11:39] d2.utils.events INFO:  eta: 14:22:31  iter: 1659  total_loss: 24.33  loss_ce: 0.5158  loss_cate: 0.188  loss_mask: 0.6935  loss_dice: 0.9231  loss_ce_0: 1.052  loss_cate_0: 0  loss_mask_0: 0.7074  loss_dice_0: 0.8895  loss_ce_1: 0.5332  loss_cate_1: 0  loss_mask_1: 0.7133  loss_dice_1: 0.9359  loss_ce_2: 0.5614  loss_cate_2: 0  loss_mask_2: 0.6828  loss_dice_2: 0.8531  loss_ce_3: 0.6229  loss_cate_3: 0  loss_mask_3: 0.686  loss_dice_3: 0.9041  loss_ce_4: 0.5394  loss_cate_4: 0  loss_mask_4: 0.7008  loss_dice_4: 0.9099  loss_ce_5: 0.5494  loss_cate_5: 0  loss_mask_5: 0.7055  loss_dice_5: 0.9157  loss_ce_6: 0.5825  loss_cate_6: 0  loss_mask_6: 0.6889  loss_dice_6: 0.9419  loss_ce_7: 0.5326  loss_cate_7: 0  loss_mask_7: 0.6899  loss_dice_7: 0.9464  loss_ce_8: 0.4776  loss_cate_8: 0  loss_mask_8: 0.6601  loss_dice_8: 0.8956  time: 1.3907  data_time: 0.0123  lr: 9.6163e-06  max_mem: 23815M
[01/01 11:12:07] d2.utils.events INFO:  eta: 14:21:58  iter: 1679  total_loss: 22.35  loss_ce: 0.5246  loss_cate: 0.1487  loss_mask: 0.568  loss_dice: 0.9101  loss_ce_0: 0.9405  loss_cate_0: 0  loss_mask_0: 0.548  loss_dice_0: 0.9552  loss_ce_1: 0.5172  loss_cate_1: 0  loss_mask_1: 0.6398  loss_dice_1: 1.002  loss_ce_2: 0.4973  loss_cate_2: 0  loss_mask_2: 0.5865  loss_dice_2: 0.964  loss_ce_3: 0.579  loss_cate_3: 0  loss_mask_3: 0.5569  loss_dice_3: 0.9566  loss_ce_4: 0.5682  loss_cate_4: 0  loss_mask_4: 0.5798  loss_dice_4: 0.9625  loss_ce_5: 0.5842  loss_cate_5: 0  loss_mask_5: 0.5618  loss_dice_5: 0.94  loss_ce_6: 0.5642  loss_cate_6: 0  loss_mask_6: 0.5346  loss_dice_6: 0.9244  loss_ce_7: 0.567  loss_cate_7: 0  loss_mask_7: 0.5575  loss_dice_7: 0.9329  loss_ce_8: 0.5657  loss_cate_8: 0  loss_mask_8: 0.5382  loss_dice_8: 0.9967  time: 1.3906  data_time: 0.0109  lr: 9.6117e-06  max_mem: 23815M
[01/01 11:12:35] d2.utils.events INFO:  eta: 14:21:09  iter: 1699  total_loss: 22.65  loss_ce: 0.6084  loss_cate: 0.2344  loss_mask: 0.6494  loss_dice: 0.7898  loss_ce_0: 0.946  loss_cate_0: 0  loss_mask_0: 0.7331  loss_dice_0: 0.7982  loss_ce_1: 0.6889  loss_cate_1: 0  loss_mask_1: 0.7838  loss_dice_1: 0.8434  loss_ce_2: 0.6272  loss_cate_2: 0  loss_mask_2: 0.7284  loss_dice_2: 0.8884  loss_ce_3: 0.6611  loss_cate_3: 0  loss_mask_3: 0.6626  loss_dice_3: 0.8432  loss_ce_4: 0.6167  loss_cate_4: 0  loss_mask_4: 0.672  loss_dice_4: 0.8624  loss_ce_5: 0.611  loss_cate_5: 0  loss_mask_5: 0.6924  loss_dice_5: 0.8309  loss_ce_6: 0.6004  loss_cate_6: 0  loss_mask_6: 0.7387  loss_dice_6: 0.7945  loss_ce_7: 0.6449  loss_cate_7: 0  loss_mask_7: 0.6507  loss_dice_7: 0.7595  loss_ce_8: 0.5971  loss_cate_8: 0  loss_mask_8: 0.6506  loss_dice_8: 0.7928  time: 1.3904  data_time: 0.0103  lr: 9.6071e-06  max_mem: 23815M
[01/01 11:13:02] d2.utils.events INFO:  eta: 14:20:16  iter: 1719  total_loss: 20.82  loss_ce: 0.5564  loss_cate: 0.1416  loss_mask: 0.6059  loss_dice: 0.8159  loss_ce_0: 0.9104  loss_cate_0: 0  loss_mask_0: 0.5792  loss_dice_0: 0.8396  loss_ce_1: 0.6018  loss_cate_1: 0  loss_mask_1: 0.6206  loss_dice_1: 0.6778  loss_ce_2: 0.5299  loss_cate_2: 0  loss_mask_2: 0.5752  loss_dice_2: 0.7348  loss_ce_3: 0.508  loss_cate_3: 0  loss_mask_3: 0.6123  loss_dice_3: 0.8297  loss_ce_4: 0.5382  loss_cate_4: 0  loss_mask_4: 0.6331  loss_dice_4: 0.8283  loss_ce_5: 0.5265  loss_cate_5: 0  loss_mask_5: 0.638  loss_dice_5: 0.78  loss_ce_6: 0.571  loss_cate_6: 0  loss_mask_6: 0.6158  loss_dice_6: 0.7692  loss_ce_7: 0.5747  loss_cate_7: 0  loss_mask_7: 0.666  loss_dice_7: 0.8167  loss_ce_8: 0.5639  loss_cate_8: 0  loss_mask_8: 0.5983  loss_dice_8: 0.8105  time: 1.3902  data_time: 0.0101  lr: 9.6024e-06  max_mem: 23815M
[01/01 11:13:30] d2.utils.events INFO:  eta: 14:19:38  iter: 1739  total_loss: 25.85  loss_ce: 0.6692  loss_cate: 0.161  loss_mask: 0.7715  loss_dice: 0.9715  loss_ce_0: 0.9992  loss_cate_0: 0  loss_mask_0: 0.7542  loss_dice_0: 1.035  loss_ce_1: 0.6459  loss_cate_1: 0  loss_mask_1: 0.6989  loss_dice_1: 1.035  loss_ce_2: 0.7589  loss_cate_2: 0  loss_mask_2: 0.7172  loss_dice_2: 0.9316  loss_ce_3: 0.7061  loss_cate_3: 0  loss_mask_3: 0.7263  loss_dice_3: 0.951  loss_ce_4: 0.6536  loss_cate_4: 0  loss_mask_4: 0.7435  loss_dice_4: 1.033  loss_ce_5: 0.705  loss_cate_5: 0  loss_mask_5: 0.7503  loss_dice_5: 1.041  loss_ce_6: 0.6729  loss_cate_6: 0  loss_mask_6: 0.6862  loss_dice_6: 1.01  loss_ce_7: 0.6955  loss_cate_7: 0  loss_mask_7: 0.716  loss_dice_7: 1.003  loss_ce_8: 0.616  loss_cate_8: 0  loss_mask_8: 0.7057  loss_dice_8: 0.9682  time: 1.3901  data_time: 0.0101  lr: 9.5978e-06  max_mem: 23815M
[01/01 11:13:57] d2.utils.events INFO:  eta: 14:19:01  iter: 1759  total_loss: 23.26  loss_ce: 0.5561  loss_cate: 0.1519  loss_mask: 0.6394  loss_dice: 0.9105  loss_ce_0: 0.9061  loss_cate_0: 0  loss_mask_0: 0.6428  loss_dice_0: 0.8905  loss_ce_1: 0.5551  loss_cate_1: 0  loss_mask_1: 0.6872  loss_dice_1: 0.9165  loss_ce_2: 0.5169  loss_cate_2: 0  loss_mask_2: 0.6328  loss_dice_2: 0.9006  loss_ce_3: 0.571  loss_cate_3: 0  loss_mask_3: 0.7398  loss_dice_3: 0.879  loss_ce_4: 0.5262  loss_cate_4: 0  loss_mask_4: 0.7258  loss_dice_4: 0.9442  loss_ce_5: 0.5911  loss_cate_5: 0  loss_mask_5: 0.6703  loss_dice_5: 0.9023  loss_ce_6: 0.5929  loss_cate_6: 0  loss_mask_6: 0.677  loss_dice_6: 0.9292  loss_ce_7: 0.579  loss_cate_7: 0  loss_mask_7: 0.6513  loss_dice_7: 0.9454  loss_ce_8: 0.5695  loss_cate_8: 0  loss_mask_8: 0.7185  loss_dice_8: 0.8704  time: 1.3900  data_time: 0.0117  lr: 9.5931e-06  max_mem: 23815M
[01/01 11:14:25] d2.utils.events INFO:  eta: 14:18:20  iter: 1779  total_loss: 22.81  loss_ce: 0.5839  loss_cate: 0.168  loss_mask: 0.6182  loss_dice: 0.9022  loss_ce_0: 0.9584  loss_cate_0: 0  loss_mask_0: 0.5726  loss_dice_0: 0.9751  loss_ce_1: 0.6118  loss_cate_1: 0  loss_mask_1: 0.6482  loss_dice_1: 0.9533  loss_ce_2: 0.5673  loss_cate_2: 0  loss_mask_2: 0.6278  loss_dice_2: 0.9496  loss_ce_3: 0.5852  loss_cate_3: 0  loss_mask_3: 0.6661  loss_dice_3: 0.9348  loss_ce_4: 0.5304  loss_cate_4: 0  loss_mask_4: 0.6744  loss_dice_4: 0.9444  loss_ce_5: 0.5586  loss_cate_5: 0  loss_mask_5: 0.6518  loss_dice_5: 0.8771  loss_ce_6: 0.5302  loss_cate_6: 0  loss_mask_6: 0.6135  loss_dice_6: 0.9121  loss_ce_7: 0.5356  loss_cate_7: 0  loss_mask_7: 0.6536  loss_dice_7: 0.9115  loss_ce_8: 0.5974  loss_cate_8: 0  loss_mask_8: 0.627  loss_dice_8: 0.9362  time: 1.3900  data_time: 0.0135  lr: 9.5885e-06  max_mem: 23815M
[01/01 11:14:53] d2.utils.events INFO:  eta: 14:17:45  iter: 1799  total_loss: 21.43  loss_ce: 0.5013  loss_cate: 0.1437  loss_mask: 0.6015  loss_dice: 0.801  loss_ce_0: 1.034  loss_cate_0: 0  loss_mask_0: 0.5933  loss_dice_0: 0.8129  loss_ce_1: 0.5475  loss_cate_1: 0  loss_mask_1: 0.5678  loss_dice_1: 0.8261  loss_ce_2: 0.4997  loss_cate_2: 0  loss_mask_2: 0.6254  loss_dice_2: 0.8479  loss_ce_3: 0.5247  loss_cate_3: 0  loss_mask_3: 0.6735  loss_dice_3: 0.8493  loss_ce_4: 0.4796  loss_cate_4: 0  loss_mask_4: 0.5824  loss_dice_4: 0.9158  loss_ce_5: 0.4929  loss_cate_5: 0  loss_mask_5: 0.6216  loss_dice_5: 0.8348  loss_ce_6: 0.527  loss_cate_6: 0  loss_mask_6: 0.6779  loss_dice_6: 0.9106  loss_ce_7: 0.5356  loss_cate_7: 0  loss_mask_7: 0.5881  loss_dice_7: 0.8802  loss_ce_8: 0.544  loss_cate_8: 0  loss_mask_8: 0.5926  loss_dice_8: 0.785  time: 1.3900  data_time: 0.0125  lr: 9.5839e-06  max_mem: 23815M
[01/01 11:15:21] d2.utils.events INFO:  eta: 14:17:09  iter: 1819  total_loss: 20.37  loss_ce: 0.5509  loss_cate: 0.1583  loss_mask: 0.527  loss_dice: 0.8882  loss_ce_0: 1.012  loss_cate_0: 0  loss_mask_0: 0.4313  loss_dice_0: 0.856  loss_ce_1: 0.6848  loss_cate_1: 0  loss_mask_1: 0.5326  loss_dice_1: 0.8424  loss_ce_2: 0.6181  loss_cate_2: 0  loss_mask_2: 0.5624  loss_dice_2: 0.8819  loss_ce_3: 0.594  loss_cate_3: 0  loss_mask_3: 0.5426  loss_dice_3: 0.9018  loss_ce_4: 0.6022  loss_cate_4: 0  loss_mask_4: 0.5499  loss_dice_4: 0.879  loss_ce_5: 0.6533  loss_cate_5: 0  loss_mask_5: 0.525  loss_dice_5: 0.8262  loss_ce_6: 0.6293  loss_cate_6: 0  loss_mask_6: 0.4954  loss_dice_6: 0.8086  loss_ce_7: 0.6152  loss_cate_7: 0  loss_mask_7: 0.5286  loss_dice_7: 0.8965  loss_ce_8: 0.6018  loss_cate_8: 0  loss_mask_8: 0.5366  loss_dice_8: 0.8514  time: 1.3901  data_time: 0.0129  lr: 9.5792e-06  max_mem: 23815M
[01/01 11:15:49] d2.utils.events INFO:  eta: 14:16:29  iter: 1839  total_loss: 29.13  loss_ce: 0.6782  loss_cate: 0.1572  loss_mask: 0.6607  loss_dice: 1.221  loss_ce_0: 1.107  loss_cate_0: 0  loss_mask_0: 0.7441  loss_dice_0: 1.198  loss_ce_1: 0.7845  loss_cate_1: 0  loss_mask_1: 0.6816  loss_dice_1: 1.21  loss_ce_2: 0.7283  loss_cate_2: 0  loss_mask_2: 0.6902  loss_dice_2: 1.196  loss_ce_3: 0.6518  loss_cate_3: 0  loss_mask_3: 0.6425  loss_dice_3: 1.182  loss_ce_4: 0.6661  loss_cate_4: 0  loss_mask_4: 0.6128  loss_dice_4: 1.225  loss_ce_5: 0.6922  loss_cate_5: 0  loss_mask_5: 0.6627  loss_dice_5: 1.246  loss_ce_6: 0.681  loss_cate_6: 0  loss_mask_6: 0.6497  loss_dice_6: 1.279  loss_ce_7: 0.7101  loss_cate_7: 0  loss_mask_7: 0.6031  loss_dice_7: 1.25  loss_ce_8: 0.6701  loss_cate_8: 0  loss_mask_8: 0.6359  loss_dice_8: 1.239  time: 1.3900  data_time: 0.0148  lr: 9.5746e-06  max_mem: 23815M
[01/01 11:16:17] d2.utils.events INFO:  eta: 14:15:58  iter: 1859  total_loss: 16.95  loss_ce: 0.4806  loss_cate: 0.1589  loss_mask: 0.4522  loss_dice: 0.6981  loss_ce_0: 0.7849  loss_cate_0: 0  loss_mask_0: 0.4507  loss_dice_0: 0.8316  loss_ce_1: 0.4916  loss_cate_1: 0  loss_mask_1: 0.445  loss_dice_1: 0.8463  loss_ce_2: 0.4286  loss_cate_2: 0  loss_mask_2: 0.4863  loss_dice_2: 0.7336  loss_ce_3: 0.4405  loss_cate_3: 0  loss_mask_3: 0.4552  loss_dice_3: 0.7328  loss_ce_4: 0.4323  loss_cate_4: 0  loss_mask_4: 0.4506  loss_dice_4: 0.7281  loss_ce_5: 0.4513  loss_cate_5: 0  loss_mask_5: 0.4605  loss_dice_5: 0.714  loss_ce_6: 0.47  loss_cate_6: 0  loss_mask_6: 0.4492  loss_dice_6: 0.6885  loss_ce_7: 0.4627  loss_cate_7: 0  loss_mask_7: 0.4238  loss_dice_7: 0.7484  loss_ce_8: 0.4646  loss_cate_8: 0  loss_mask_8: 0.437  loss_dice_8: 0.7156  time: 1.3901  data_time: 0.0122  lr: 9.57e-06  max_mem: 23815M
[01/01 11:16:45] d2.utils.events INFO:  eta: 14:15:25  iter: 1879  total_loss: 24.49  loss_ce: 0.6577  loss_cate: 0.1895  loss_mask: 0.7883  loss_dice: 1.057  loss_ce_0: 1.021  loss_cate_0: 0  loss_mask_0: 0.687  loss_dice_0: 1.015  loss_ce_1: 0.6191  loss_cate_1: 0  loss_mask_1: 0.7347  loss_dice_1: 1.108  loss_ce_2: 0.5741  loss_cate_2: 0  loss_mask_2: 0.7234  loss_dice_2: 0.9881  loss_ce_3: 0.6441  loss_cate_3: 0  loss_mask_3: 0.7633  loss_dice_3: 1.046  loss_ce_4: 0.6067  loss_cate_4: 0  loss_mask_4: 0.7664  loss_dice_4: 1.071  loss_ce_5: 0.64  loss_cate_5: 0  loss_mask_5: 0.7187  loss_dice_5: 1.048  loss_ce_6: 0.6455  loss_cate_6: 0  loss_mask_6: 0.7874  loss_dice_6: 0.9926  loss_ce_7: 0.6602  loss_cate_7: 0  loss_mask_7: 0.7841  loss_dice_7: 1.046  loss_ce_8: 0.6523  loss_cate_8: 0  loss_mask_8: 0.8283  loss_dice_8: 1.045  time: 1.3900  data_time: 0.0150  lr: 9.5653e-06  max_mem: 23815M
[01/01 11:17:12] d2.utils.events INFO:  eta: 14:14:53  iter: 1899  total_loss: 21.41  loss_ce: 0.4552  loss_cate: 0.1343  loss_mask: 0.7225  loss_dice: 0.8798  loss_ce_0: 0.9002  loss_cate_0: 0  loss_mask_0: 0.7156  loss_dice_0: 0.7787  loss_ce_1: 0.5678  loss_cate_1: 0  loss_mask_1: 0.7119  loss_dice_1: 0.8214  loss_ce_2: 0.5056  loss_cate_2: 0  loss_mask_2: 0.669  loss_dice_2: 0.7782  loss_ce_3: 0.4633  loss_cate_3: 0  loss_mask_3: 0.7598  loss_dice_3: 0.8056  loss_ce_4: 0.462  loss_cate_4: 0  loss_mask_4: 0.7501  loss_dice_4: 0.8237  loss_ce_5: 0.4985  loss_cate_5: 0  loss_mask_5: 0.7426  loss_dice_5: 0.8272  loss_ce_6: 0.4924  loss_cate_6: 0  loss_mask_6: 0.7144  loss_dice_6: 0.7935  loss_ce_7: 0.5193  loss_cate_7: 0  loss_mask_7: 0.7344  loss_dice_7: 0.7812  loss_ce_8: 0.4535  loss_cate_8: 0  loss_mask_8: 0.7367  loss_dice_8: 0.8355  time: 1.3900  data_time: 0.0128  lr: 9.5607e-06  max_mem: 23815M
[01/01 11:17:40] d2.utils.events INFO:  eta: 14:14:18  iter: 1919  total_loss: 24.87  loss_ce: 0.6757  loss_cate: 0.1497  loss_mask: 0.6522  loss_dice: 0.874  loss_ce_0: 1.048  loss_cate_0: 0  loss_mask_0: 0.6587  loss_dice_0: 0.8709  loss_ce_1: 0.7194  loss_cate_1: 0  loss_mask_1: 0.6849  loss_dice_1: 0.9807  loss_ce_2: 0.7516  loss_cate_2: 0  loss_mask_2: 0.6559  loss_dice_2: 0.8528  loss_ce_3: 0.6596  loss_cate_3: 0  loss_mask_3: 0.6384  loss_dice_3: 0.911  loss_ce_4: 0.6533  loss_cate_4: 0  loss_mask_4: 0.6613  loss_dice_4: 0.9339  loss_ce_5: 0.6709  loss_cate_5: 0  loss_mask_5: 0.6561  loss_dice_5: 0.9477  loss_ce_6: 0.6724  loss_cate_6: 0  loss_mask_6: 0.6491  loss_dice_6: 0.9155  loss_ce_7: 0.7509  loss_cate_7: 0  loss_mask_7: 0.6529  loss_dice_7: 0.917  loss_ce_8: 0.6935  loss_cate_8: 0  loss_mask_8: 0.6352  loss_dice_8: 0.922  time: 1.3900  data_time: 0.0124  lr: 9.556e-06  max_mem: 23815M
[01/01 11:18:08] d2.utils.events INFO:  eta: 14:13:43  iter: 1939  total_loss: 25.5  loss_ce: 0.6518  loss_cate: 0.2032  loss_mask: 0.6677  loss_dice: 0.9563  loss_ce_0: 0.9736  loss_cate_0: 0  loss_mask_0: 0.5843  loss_dice_0: 0.9949  loss_ce_1: 0.6843  loss_cate_1: 0  loss_mask_1: 0.6332  loss_dice_1: 0.9019  loss_ce_2: 0.6112  loss_cate_2: 0  loss_mask_2: 0.6171  loss_dice_2: 0.9762  loss_ce_3: 0.583  loss_cate_3: 0  loss_mask_3: 0.6331  loss_dice_3: 1.02  loss_ce_4: 0.5689  loss_cate_4: 0  loss_mask_4: 0.6729  loss_dice_4: 1.029  loss_ce_5: 0.5768  loss_cate_5: 0  loss_mask_5: 0.615  loss_dice_5: 0.9486  loss_ce_6: 0.6068  loss_cate_6: 0  loss_mask_6: 0.699  loss_dice_6: 0.9113  loss_ce_7: 0.6426  loss_cate_7: 0  loss_mask_7: 0.6573  loss_dice_7: 0.9333  loss_ce_8: 0.6592  loss_cate_8: 0  loss_mask_8: 0.5719  loss_dice_8: 0.9513  time: 1.3899  data_time: 0.0111  lr: 9.5514e-06  max_mem: 23815M
[01/01 11:18:36] d2.utils.events INFO:  eta: 14:13:06  iter: 1959  total_loss: 26.41  loss_ce: 0.6407  loss_cate: 0.1481  loss_mask: 0.7035  loss_dice: 0.9863  loss_ce_0: 0.9369  loss_cate_0: 0  loss_mask_0: 0.7532  loss_dice_0: 0.972  loss_ce_1: 0.7064  loss_cate_1: 0  loss_mask_1: 0.6511  loss_dice_1: 0.9989  loss_ce_2: 0.7568  loss_cate_2: 0  loss_mask_2: 0.6269  loss_dice_2: 0.9384  loss_ce_3: 0.7673  loss_cate_3: 0  loss_mask_3: 0.6601  loss_dice_3: 0.9668  loss_ce_4: 0.6895  loss_cate_4: 0  loss_mask_4: 0.6573  loss_dice_4: 0.9231  loss_ce_5: 0.6762  loss_cate_5: 0  loss_mask_5: 0.6913  loss_dice_5: 1.034  loss_ce_6: 0.6522  loss_cate_6: 0  loss_mask_6: 0.6917  loss_dice_6: 1.041  loss_ce_7: 0.6459  loss_cate_7: 0  loss_mask_7: 0.6726  loss_dice_7: 1.092  loss_ce_8: 0.6804  loss_cate_8: 0  loss_mask_8: 0.7247  loss_dice_8: 0.991  time: 1.3898  data_time: 0.0125  lr: 9.5468e-06  max_mem: 23815M
[01/01 11:19:03] d2.utils.events INFO:  eta: 14:12:28  iter: 1979  total_loss: 20.98  loss_ce: 0.5214  loss_cate: 0.1486  loss_mask: 0.5723  loss_dice: 0.7453  loss_ce_0: 0.994  loss_cate_0: 0  loss_mask_0: 0.6089  loss_dice_0: 0.7262  loss_ce_1: 0.5944  loss_cate_1: 0  loss_mask_1: 0.6364  loss_dice_1: 0.8038  loss_ce_2: 0.5564  loss_cate_2: 0  loss_mask_2: 0.656  loss_dice_2: 0.7245  loss_ce_3: 0.566  loss_cate_3: 0  loss_mask_3: 0.6154  loss_dice_3: 0.7273  loss_ce_4: 0.5508  loss_cate_4: 0  loss_mask_4: 0.6087  loss_dice_4: 0.7872  loss_ce_5: 0.5183  loss_cate_5: 0  loss_mask_5: 0.6417  loss_dice_5: 0.8071  loss_ce_6: 0.5656  loss_cate_6: 0  loss_mask_6: 0.6072  loss_dice_6: 0.7025  loss_ce_7: 0.501  loss_cate_7: 0  loss_mask_7: 0.5597  loss_dice_7: 0.7713  loss_ce_8: 0.5135  loss_cate_8: 0  loss_mask_8: 0.6093  loss_dice_8: 0.7596  time: 1.3898  data_time: 0.0132  lr: 9.5421e-06  max_mem: 23815M
[01/01 11:19:31] d2.utils.events INFO:  eta: 14:11:50  iter: 1999  total_loss: 24.59  loss_ce: 0.5274  loss_cate: 0.1957  loss_mask: 0.7494  loss_dice: 1.015  loss_ce_0: 0.9532  loss_cate_0: 0  loss_mask_0: 0.7272  loss_dice_0: 1.036  loss_ce_1: 0.5947  loss_cate_1: 0  loss_mask_1: 0.8508  loss_dice_1: 1.057  loss_ce_2: 0.5695  loss_cate_2: 0  loss_mask_2: 0.7777  loss_dice_2: 1.023  loss_ce_3: 0.5136  loss_cate_3: 0  loss_mask_3: 0.7418  loss_dice_3: 1.01  loss_ce_4: 0.4927  loss_cate_4: 0  loss_mask_4: 0.7508  loss_dice_4: 1.029  loss_ce_5: 0.5803  loss_cate_5: 0  loss_mask_5: 0.789  loss_dice_5: 1.033  loss_ce_6: 0.5742  loss_cate_6: 0  loss_mask_6: 0.7798  loss_dice_6: 0.9653  loss_ce_7: 0.5554  loss_cate_7: 0  loss_mask_7: 0.7517  loss_dice_7: 1.042  loss_ce_8: 0.5671  loss_cate_8: 0  loss_mask_8: 0.759  loss_dice_8: 1.031  time: 1.3897  data_time: 0.0115  lr: 9.5375e-06  max_mem: 23815M
[01/01 11:19:59] d2.utils.events INFO:  eta: 14:11:15  iter: 2019  total_loss: 23.5  loss_ce: 0.5264  loss_cate: 0.1628  loss_mask: 0.6487  loss_dice: 0.8093  loss_ce_0: 1.023  loss_cate_0: 0  loss_mask_0: 0.6506  loss_dice_0: 0.7761  loss_ce_1: 0.5756  loss_cate_1: 0  loss_mask_1: 0.6483  loss_dice_1: 0.9785  loss_ce_2: 0.468  loss_cate_2: 0  loss_mask_2: 0.6651  loss_dice_2: 0.985  loss_ce_3: 0.4649  loss_cate_3: 0  loss_mask_3: 0.6732  loss_dice_3: 0.9341  loss_ce_4: 0.477  loss_cate_4: 0  loss_mask_4: 0.6962  loss_dice_4: 0.9433  loss_ce_5: 0.517  loss_cate_5: 0  loss_mask_5: 0.6775  loss_dice_5: 0.947  loss_ce_6: 0.5818  loss_cate_6: 0  loss_mask_6: 0.6865  loss_dice_6: 0.8203  loss_ce_7: 0.5749  loss_cate_7: 0  loss_mask_7: 0.6773  loss_dice_7: 0.8564  loss_ce_8: 0.5441  loss_cate_8: 0  loss_mask_8: 0.663  loss_dice_8: 0.8136  time: 1.3896  data_time: 0.0121  lr: 9.5328e-06  max_mem: 23815M
[01/01 11:20:27] d2.utils.events INFO:  eta: 14:10:44  iter: 2039  total_loss: 20.76  loss_ce: 0.4804  loss_cate: 0.1322  loss_mask: 0.6393  loss_dice: 0.8339  loss_ce_0: 0.8634  loss_cate_0: 0  loss_mask_0: 0.6593  loss_dice_0: 0.8033  loss_ce_1: 0.5512  loss_cate_1: 0  loss_mask_1: 0.5755  loss_dice_1: 0.7816  loss_ce_2: 0.5397  loss_cate_2: 0  loss_mask_2: 0.6395  loss_dice_2: 0.7893  loss_ce_3: 0.5425  loss_cate_3: 0  loss_mask_3: 0.6229  loss_dice_3: 0.841  loss_ce_4: 0.5624  loss_cate_4: 0  loss_mask_4: 0.6004  loss_dice_4: 0.8436  loss_ce_5: 0.4844  loss_cate_5: 0  loss_mask_5: 0.665  loss_dice_5: 0.8101  loss_ce_6: 0.5549  loss_cate_6: 0  loss_mask_6: 0.6118  loss_dice_6: 0.7948  loss_ce_7: 0.5191  loss_cate_7: 0  loss_mask_7: 0.6485  loss_dice_7: 0.7127  loss_ce_8: 0.4942  loss_cate_8: 0  loss_mask_8: 0.6521  loss_dice_8: 0.7948  time: 1.3897  data_time: 0.0112  lr: 9.5282e-06  max_mem: 23815M
[01/01 11:20:55] d2.utils.events INFO:  eta: 14:10:23  iter: 2059  total_loss: 20.7  loss_ce: 0.506  loss_cate: 0.1812  loss_mask: 0.6545  loss_dice: 0.8709  loss_ce_0: 0.8693  loss_cate_0: 0  loss_mask_0: 0.6463  loss_dice_0: 0.8606  loss_ce_1: 0.4895  loss_cate_1: 0  loss_mask_1: 0.6635  loss_dice_1: 0.9311  loss_ce_2: 0.412  loss_cate_2: 0  loss_mask_2: 0.6227  loss_dice_2: 0.877  loss_ce_3: 0.4529  loss_cate_3: 0  loss_mask_3: 0.6437  loss_dice_3: 0.9185  loss_ce_4: 0.4551  loss_cate_4: 0  loss_mask_4: 0.6723  loss_dice_4: 0.9003  loss_ce_5: 0.3577  loss_cate_5: 0  loss_mask_5: 0.6503  loss_dice_5: 0.8596  loss_ce_6: 0.4769  loss_cate_6: 0  loss_mask_6: 0.6493  loss_dice_6: 0.8765  loss_ce_7: 0.4793  loss_cate_7: 0  loss_mask_7: 0.6473  loss_dice_7: 0.8239  loss_ce_8: 0.4896  loss_cate_8: 0  loss_mask_8: 0.6945  loss_dice_8: 0.8278  time: 1.3898  data_time: 0.0131  lr: 9.5236e-06  max_mem: 23815M
[01/01 11:21:23] d2.utils.events INFO:  eta: 14:09:56  iter: 2079  total_loss: 25.3  loss_ce: 0.5993  loss_cate: 0.1349  loss_mask: 0.6879  loss_dice: 1.166  loss_ce_0: 1.072  loss_cate_0: 0  loss_mask_0: 0.66  loss_dice_0: 1.132  loss_ce_1: 0.6949  loss_cate_1: 0  loss_mask_1: 0.7529  loss_dice_1: 1.048  loss_ce_2: 0.5753  loss_cate_2: 0  loss_mask_2: 0.722  loss_dice_2: 1.139  loss_ce_3: 0.6094  loss_cate_3: 0  loss_mask_3: 0.6823  loss_dice_3: 1.197  loss_ce_4: 0.5663  loss_cate_4: 0  loss_mask_4: 0.7242  loss_dice_4: 1.18  loss_ce_5: 0.5347  loss_cate_5: 0  loss_mask_5: 0.6952  loss_dice_5: 1.137  loss_ce_6: 0.5501  loss_cate_6: 0  loss_mask_6: 0.6786  loss_dice_6: 1.104  loss_ce_7: 0.5839  loss_cate_7: 0  loss_mask_7: 0.703  loss_dice_7: 1.108  loss_ce_8: 0.6374  loss_cate_8: 0  loss_mask_8: 0.6794  loss_dice_8: 1.129  time: 1.3898  data_time: 0.0111  lr: 9.5189e-06  max_mem: 23815M
[01/01 11:21:51] d2.utils.events INFO:  eta: 14:09:34  iter: 2099  total_loss: 24.77  loss_ce: 0.758  loss_cate: 0.1757  loss_mask: 0.7336  loss_dice: 0.9172  loss_ce_0: 1.051  loss_cate_0: 0  loss_mask_0: 0.7009  loss_dice_0: 0.9032  loss_ce_1: 0.7011  loss_cate_1: 0  loss_mask_1: 0.8045  loss_dice_1: 0.9487  loss_ce_2: 0.7475  loss_cate_2: 0  loss_mask_2: 0.7334  loss_dice_2: 0.9767  loss_ce_3: 0.7294  loss_cate_3: 0  loss_mask_3: 0.711  loss_dice_3: 0.9016  loss_ce_4: 0.7019  loss_cate_4: 0  loss_mask_4: 0.77  loss_dice_4: 0.8981  loss_ce_5: 0.6552  loss_cate_5: 0  loss_mask_5: 0.7376  loss_dice_5: 0.911  loss_ce_6: 0.727  loss_cate_6: 0  loss_mask_6: 0.7355  loss_dice_6: 0.9422  loss_ce_7: 0.7159  loss_cate_7: 0  loss_mask_7: 0.7428  loss_dice_7: 0.9341  loss_ce_8: 0.6337  loss_cate_8: 0  loss_mask_8: 0.7433  loss_dice_8: 0.9108  time: 1.3900  data_time: 0.0146  lr: 9.5143e-06  max_mem: 23815M
[01/01 11:22:19] d2.utils.events INFO:  eta: 14:09:15  iter: 2119  total_loss: 27.72  loss_ce: 0.7652  loss_cate: 0.1784  loss_mask: 0.7747  loss_dice: 1.078  loss_ce_0: 1.203  loss_cate_0: 0  loss_mask_0: 0.797  loss_dice_0: 1.06  loss_ce_1: 0.8119  loss_cate_1: 0  loss_mask_1: 0.7485  loss_dice_1: 1.108  loss_ce_2: 0.7913  loss_cate_2: 0  loss_mask_2: 0.7152  loss_dice_2: 1.091  loss_ce_3: 0.8238  loss_cate_3: 0  loss_mask_3: 0.7105  loss_dice_3: 1.06  loss_ce_4: 0.8343  loss_cate_4: 0  loss_mask_4: 0.7954  loss_dice_4: 1.094  loss_ce_5: 0.7448  loss_cate_5: 0  loss_mask_5: 0.7507  loss_dice_5: 1.093  loss_ce_6: 0.8161  loss_cate_6: 0  loss_mask_6: 0.8483  loss_dice_6: 1.106  loss_ce_7: 0.7164  loss_cate_7: 0  loss_mask_7: 0.7972  loss_dice_7: 1.052  loss_ce_8: 0.7427  loss_cate_8: 0  loss_mask_8: 0.7744  loss_dice_8: 1.068  time: 1.3902  data_time: 0.0137  lr: 9.5096e-06  max_mem: 23815M
[01/01 11:22:47] d2.utils.events INFO:  eta: 14:08:55  iter: 2139  total_loss: 22.14  loss_ce: 0.6377  loss_cate: 0.1604  loss_mask: 0.7532  loss_dice: 0.8373  loss_ce_0: 1.017  loss_cate_0: 0  loss_mask_0: 0.6149  loss_dice_0: 0.8026  loss_ce_1: 0.7116  loss_cate_1: 0  loss_mask_1: 0.6337  loss_dice_1: 0.8607  loss_ce_2: 0.6586  loss_cate_2: 0  loss_mask_2: 0.6641  loss_dice_2: 0.8965  loss_ce_3: 0.5962  loss_cate_3: 0  loss_mask_3: 0.7043  loss_dice_3: 0.8924  loss_ce_4: 0.5475  loss_cate_4: 0  loss_mask_4: 0.6654  loss_dice_4: 0.8821  loss_ce_5: 0.5866  loss_cate_5: 0  loss_mask_5: 0.6854  loss_dice_5: 0.902  loss_ce_6: 0.6233  loss_cate_6: 0  loss_mask_6: 0.7563  loss_dice_6: 0.8702  loss_ce_7: 0.6447  loss_cate_7: 0  loss_mask_7: 0.7109  loss_dice_7: 0.8232  loss_ce_8: 0.6685  loss_cate_8: 0  loss_mask_8: 0.7347  loss_dice_8: 0.8544  time: 1.3903  data_time: 0.0120  lr: 9.505e-06  max_mem: 23815M
[01/01 11:23:15] d2.utils.events INFO:  eta: 14:08:33  iter: 2159  total_loss: 20.16  loss_ce: 0.5916  loss_cate: 0.1935  loss_mask: 0.692  loss_dice: 0.8935  loss_ce_0: 0.9103  loss_cate_0: 0  loss_mask_0: 0.6516  loss_dice_0: 0.8914  loss_ce_1: 0.6018  loss_cate_1: 0  loss_mask_1: 0.6944  loss_dice_1: 0.909  loss_ce_2: 0.5663  loss_cate_2: 0  loss_mask_2: 0.6735  loss_dice_2: 0.8968  loss_ce_3: 0.5735  loss_cate_3: 0  loss_mask_3: 0.6786  loss_dice_3: 0.8471  loss_ce_4: 0.5813  loss_cate_4: 0  loss_mask_4: 0.7204  loss_dice_4: 0.9069  loss_ce_5: 0.5556  loss_cate_5: 0  loss_mask_5: 0.6824  loss_dice_5: 0.915  loss_ce_6: 0.597  loss_cate_6: 0  loss_mask_6: 0.6538  loss_dice_6: 0.9103  loss_ce_7: 0.5818  loss_cate_7: 0  loss_mask_7: 0.7097  loss_dice_7: 0.888  loss_ce_8: 0.6104  loss_cate_8: 0  loss_mask_8: 0.6922  loss_dice_8: 0.8674  time: 1.3904  data_time: 0.0118  lr: 9.5004e-06  max_mem: 23815M
[01/01 11:23:44] d2.utils.events INFO:  eta: 14:08:21  iter: 2179  total_loss: 21.09  loss_ce: 0.5353  loss_cate: 0.1498  loss_mask: 0.5571  loss_dice: 0.9645  loss_ce_0: 0.9058  loss_cate_0: 0  loss_mask_0: 0.586  loss_dice_0: 0.9796  loss_ce_1: 0.5665  loss_cate_1: 0  loss_mask_1: 0.6235  loss_dice_1: 0.9306  loss_ce_2: 0.5834  loss_cate_2: 0  loss_mask_2: 0.6301  loss_dice_2: 0.9815  loss_ce_3: 0.538  loss_cate_3: 0  loss_mask_3: 0.6104  loss_dice_3: 0.9561  loss_ce_4: 0.5118  loss_cate_4: 0  loss_mask_4: 0.5793  loss_dice_4: 0.9895  loss_ce_5: 0.5611  loss_cate_5: 0  loss_mask_5: 0.581  loss_dice_5: 0.94  loss_ce_6: 0.5596  loss_cate_6: 0  loss_mask_6: 0.5729  loss_dice_6: 0.9506  loss_ce_7: 0.5647  loss_cate_7: 0  loss_mask_7: 0.569  loss_dice_7: 0.9415  loss_ce_8: 0.5318  loss_cate_8: 0  loss_mask_8: 0.5477  loss_dice_8: 0.9183  time: 1.3905  data_time: 0.0119  lr: 9.4957e-06  max_mem: 23815M
[01/01 11:24:12] d2.utils.events INFO:  eta: 14:08:09  iter: 2199  total_loss: 26.42  loss_ce: 0.6672  loss_cate: 0.1555  loss_mask: 0.6297  loss_dice: 0.993  loss_ce_0: 0.9838  loss_cate_0: 0  loss_mask_0: 0.6462  loss_dice_0: 0.9532  loss_ce_1: 0.7648  loss_cate_1: 0  loss_mask_1: 0.6587  loss_dice_1: 0.978  loss_ce_2: 0.7742  loss_cate_2: 0  loss_mask_2: 0.6582  loss_dice_2: 0.9357  loss_ce_3: 0.6722  loss_cate_3: 0  loss_mask_3: 0.674  loss_dice_3: 0.9327  loss_ce_4: 0.7332  loss_cate_4: 0  loss_mask_4: 0.6843  loss_dice_4: 0.952  loss_ce_5: 0.6431  loss_cate_5: 0  loss_mask_5: 0.6444  loss_dice_5: 0.9963  loss_ce_6: 0.7196  loss_cate_6: 0  loss_mask_6: 0.6256  loss_dice_6: 0.9813  loss_ce_7: 0.6415  loss_cate_7: 0  loss_mask_7: 0.6818  loss_dice_7: 0.9824  loss_ce_8: 0.7703  loss_cate_8: 0  loss_mask_8: 0.6262  loss_dice_8: 0.9899  time: 1.3908  data_time: 0.0127  lr: 9.4911e-06  max_mem: 23815M
[01/01 11:24:40] d2.utils.events INFO:  eta: 14:08:02  iter: 2219  total_loss: 23.1  loss_ce: 0.619  loss_cate: 0.1332  loss_mask: 0.5266  loss_dice: 0.9419  loss_ce_0: 0.9516  loss_cate_0: 0  loss_mask_0: 0.6105  loss_dice_0: 0.9839  loss_ce_1: 0.6916  loss_cate_1: 0  loss_mask_1: 0.5399  loss_dice_1: 0.8728  loss_ce_2: 0.6321  loss_cate_2: 0  loss_mask_2: 0.6062  loss_dice_2: 0.982  loss_ce_3: 0.6494  loss_cate_3: 0  loss_mask_3: 0.5472  loss_dice_3: 0.9195  loss_ce_4: 0.641  loss_cate_4: 0  loss_mask_4: 0.5994  loss_dice_4: 0.9601  loss_ce_5: 0.6014  loss_cate_5: 0  loss_mask_5: 0.5683  loss_dice_5: 0.9645  loss_ce_6: 0.6132  loss_cate_6: 0  loss_mask_6: 0.5729  loss_dice_6: 0.9074  loss_ce_7: 0.5869  loss_cate_7: 0  loss_mask_7: 0.5657  loss_dice_7: 0.9316  loss_ce_8: 0.5697  loss_cate_8: 0  loss_mask_8: 0.5259  loss_dice_8: 0.8831  time: 1.3909  data_time: 0.0124  lr: 9.4864e-06  max_mem: 23815M
[01/01 11:25:08] d2.utils.events INFO:  eta: 14:07:41  iter: 2239  total_loss: 21.56  loss_ce: 0.5271  loss_cate: 0.1387  loss_mask: 0.5083  loss_dice: 0.9489  loss_ce_0: 0.9121  loss_cate_0: 0  loss_mask_0: 0.527  loss_dice_0: 0.9202  loss_ce_1: 0.5888  loss_cate_1: 0  loss_mask_1: 0.609  loss_dice_1: 1.06  loss_ce_2: 0.5846  loss_cate_2: 0  loss_mask_2: 0.5212  loss_dice_2: 1.008  loss_ce_3: 0.5907  loss_cate_3: 0  loss_mask_3: 0.4825  loss_dice_3: 0.9482  loss_ce_4: 0.5517  loss_cate_4: 0  loss_mask_4: 0.4928  loss_dice_4: 0.9539  loss_ce_5: 0.595  loss_cate_5: 0  loss_mask_5: 0.5142  loss_dice_5: 0.9691  loss_ce_6: 0.5755  loss_cate_6: 0  loss_mask_6: 0.4981  loss_dice_6: 0.9888  loss_ce_7: 0.5399  loss_cate_7: 0  loss_mask_7: 0.4998  loss_dice_7: 0.9493  loss_ce_8: 0.5376  loss_cate_8: 0  loss_mask_8: 0.5114  loss_dice_8: 0.9652  time: 1.3909  data_time: 0.0118  lr: 9.4818e-06  max_mem: 23815M
[01/01 11:25:36] d2.utils.events INFO:  eta: 14:07:18  iter: 2259  total_loss: 22.36  loss_ce: 0.6514  loss_cate: 0.1392  loss_mask: 0.6011  loss_dice: 0.9924  loss_ce_0: 0.99  loss_cate_0: 0  loss_mask_0: 0.5523  loss_dice_0: 0.9408  loss_ce_1: 0.6774  loss_cate_1: 0  loss_mask_1: 0.6086  loss_dice_1: 1.051  loss_ce_2: 0.7132  loss_cate_2: 0  loss_mask_2: 0.5635  loss_dice_2: 0.9596  loss_ce_3: 0.7286  loss_cate_3: 0  loss_mask_3: 0.5937  loss_dice_3: 0.9728  loss_ce_4: 0.7764  loss_cate_4: 0  loss_mask_4: 0.6241  loss_dice_4: 1.058  loss_ce_5: 0.708  loss_cate_5: 0  loss_mask_5: 0.5988  loss_dice_5: 1.01  loss_ce_6: 0.6231  loss_cate_6: 0  loss_mask_6: 0.5592  loss_dice_6: 0.9858  loss_ce_7: 0.7096  loss_cate_7: 0  loss_mask_7: 0.6086  loss_dice_7: 0.973  loss_ce_8: 0.7016  loss_cate_8: 0  loss_mask_8: 0.5765  loss_dice_8: 1.024  time: 1.3910  data_time: 0.0128  lr: 9.4771e-06  max_mem: 23815M
[01/01 11:26:04] d2.utils.events INFO:  eta: 14:06:39  iter: 2279  total_loss: 18.53  loss_ce: 0.484  loss_cate: 0.1374  loss_mask: 0.6262  loss_dice: 0.7112  loss_ce_0: 0.8657  loss_cate_0: 0  loss_mask_0: 0.6633  loss_dice_0: 0.7853  loss_ce_1: 0.5205  loss_cate_1: 0  loss_mask_1: 0.6352  loss_dice_1: 0.7002  loss_ce_2: 0.4757  loss_cate_2: 0  loss_mask_2: 0.6071  loss_dice_2: 0.7494  loss_ce_3: 0.43  loss_cate_3: 0  loss_mask_3: 0.6559  loss_dice_3: 0.7443  loss_ce_4: 0.4769  loss_cate_4: 0  loss_mask_4: 0.6548  loss_dice_4: 0.7352  loss_ce_5: 0.4621  loss_cate_5: 0  loss_mask_5: 0.6504  loss_dice_5: 0.72  loss_ce_6: 0.4624  loss_cate_6: 0  loss_mask_6: 0.6263  loss_dice_6: 0.7342  loss_ce_7: 0.4773  loss_cate_7: 0  loss_mask_7: 0.6153  loss_dice_7: 0.7409  loss_ce_8: 0.4804  loss_cate_8: 0  loss_mask_8: 0.6292  loss_dice_8: 0.7046  time: 1.3910  data_time: 0.0119  lr: 9.4725e-06  max_mem: 23815M
[01/01 11:26:32] d2.utils.events INFO:  eta: 14:06:01  iter: 2299  total_loss: 23.46  loss_ce: 0.5524  loss_cate: 0.1739  loss_mask: 0.6407  loss_dice: 0.967  loss_ce_0: 0.9821  loss_cate_0: 0  loss_mask_0: 0.6166  loss_dice_0: 0.9302  loss_ce_1: 0.6503  loss_cate_1: 0  loss_mask_1: 0.6166  loss_dice_1: 0.9204  loss_ce_2: 0.537  loss_cate_2: 0  loss_mask_2: 0.6405  loss_dice_2: 0.9293  loss_ce_3: 0.4948  loss_cate_3: 0  loss_mask_3: 0.6819  loss_dice_3: 0.9472  loss_ce_4: 0.5347  loss_cate_4: 0  loss_mask_4: 0.6854  loss_dice_4: 0.9438  loss_ce_5: 0.4301  loss_cate_5: 0  loss_mask_5: 0.678  loss_dice_5: 0.9575  loss_ce_6: 0.5217  loss_cate_6: 0  loss_mask_6: 0.6389  loss_dice_6: 0.9362  loss_ce_7: 0.5011  loss_cate_7: 0  loss_mask_7: 0.5792  loss_dice_7: 0.874  loss_ce_8: 0.5737  loss_cate_8: 0  loss_mask_8: 0.638  loss_dice_8: 0.8903  time: 1.3909  data_time: 0.0129  lr: 9.4679e-06  max_mem: 23815M
[01/01 11:26:59] d2.utils.events INFO:  eta: 14:05:21  iter: 2319  total_loss: 22.86  loss_ce: 0.6583  loss_cate: 0.1308  loss_mask: 0.6376  loss_dice: 1.012  loss_ce_0: 0.9908  loss_cate_0: 0  loss_mask_0: 0.6229  loss_dice_0: 0.9191  loss_ce_1: 0.7917  loss_cate_1: 0  loss_mask_1: 0.6258  loss_dice_1: 0.9491  loss_ce_2: 0.6691  loss_cate_2: 0  loss_mask_2: 0.652  loss_dice_2: 0.9417  loss_ce_3: 0.6337  loss_cate_3: 0  loss_mask_3: 0.6667  loss_dice_3: 0.9524  loss_ce_4: 0.6253  loss_cate_4: 0  loss_mask_4: 0.6454  loss_dice_4: 1.005  loss_ce_5: 0.6225  loss_cate_5: 0  loss_mask_5: 0.6835  loss_dice_5: 1.011  loss_ce_6: 0.6344  loss_cate_6: 0  loss_mask_6: 0.6251  loss_dice_6: 0.9456  loss_ce_7: 0.636  loss_cate_7: 0  loss_mask_7: 0.6361  loss_dice_7: 0.9564  loss_ce_8: 0.6445  loss_cate_8: 0  loss_mask_8: 0.6336  loss_dice_8: 0.9455  time: 1.3908  data_time: 0.0103  lr: 9.4632e-06  max_mem: 23815M
[01/01 11:27:27] d2.utils.events INFO:  eta: 14:04:53  iter: 2339  total_loss: 24.47  loss_ce: 0.6845  loss_cate: 0.1731  loss_mask: 0.716  loss_dice: 0.956  loss_ce_0: 1.043  loss_cate_0: 0  loss_mask_0: 0.6519  loss_dice_0: 0.9648  loss_ce_1: 0.7444  loss_cate_1: 0  loss_mask_1: 0.6801  loss_dice_1: 0.9498  loss_ce_2: 0.6203  loss_cate_2: 0  loss_mask_2: 0.6906  loss_dice_2: 0.9507  loss_ce_3: 0.7067  loss_cate_3: 0  loss_mask_3: 0.6845  loss_dice_3: 1.031  loss_ce_4: 0.6708  loss_cate_4: 0  loss_mask_4: 0.6591  loss_dice_4: 1.079  loss_ce_5: 0.6521  loss_cate_5: 0  loss_mask_5: 0.6742  loss_dice_5: 0.9812  loss_ce_6: 0.7439  loss_cate_6: 0  loss_mask_6: 0.6187  loss_dice_6: 0.9399  loss_ce_7: 0.7282  loss_cate_7: 0  loss_mask_7: 0.6695  loss_dice_7: 0.9393  loss_ce_8: 0.6603  loss_cate_8: 0  loss_mask_8: 0.6676  loss_dice_8: 0.9119  time: 1.3908  data_time: 0.0105  lr: 9.4586e-06  max_mem: 23815M
[01/01 11:27:55] d2.utils.events INFO:  eta: 14:04:17  iter: 2359  total_loss: 21.91  loss_ce: 0.5943  loss_cate: 0.1655  loss_mask: 0.6432  loss_dice: 0.9862  loss_ce_0: 0.9487  loss_cate_0: 0  loss_mask_0: 0.6045  loss_dice_0: 1.016  loss_ce_1: 0.6328  loss_cate_1: 0  loss_mask_1: 0.5333  loss_dice_1: 1.083  loss_ce_2: 0.6439  loss_cate_2: 0  loss_mask_2: 0.5303  loss_dice_2: 1.034  loss_ce_3: 0.6519  loss_cate_3: 0  loss_mask_3: 0.5413  loss_dice_3: 1.037  loss_ce_4: 0.6031  loss_cate_4: 0  loss_mask_4: 0.5291  loss_dice_4: 0.9878  loss_ce_5: 0.569  loss_cate_5: 0  loss_mask_5: 0.6313  loss_dice_5: 1.05  loss_ce_6: 0.5727  loss_cate_6: 0  loss_mask_6: 0.6187  loss_dice_6: 1.067  loss_ce_7: 0.5881  loss_cate_7: 0  loss_mask_7: 0.6234  loss_dice_7: 1.02  loss_ce_8: 0.5952  loss_cate_8: 0  loss_mask_8: 0.6522  loss_dice_8: 1.004  time: 1.3907  data_time: 0.0105  lr: 9.4539e-06  max_mem: 23815M
[01/01 11:28:22] d2.utils.events INFO:  eta: 14:03:43  iter: 2379  total_loss: 21.28  loss_ce: 0.4442  loss_cate: 0.1332  loss_mask: 0.5418  loss_dice: 0.7942  loss_ce_0: 0.8277  loss_cate_0: 0  loss_mask_0: 0.5228  loss_dice_0: 0.8392  loss_ce_1: 0.4893  loss_cate_1: 0  loss_mask_1: 0.5547  loss_dice_1: 0.743  loss_ce_2: 0.5225  loss_cate_2: 0  loss_mask_2: 0.5241  loss_dice_2: 0.8126  loss_ce_3: 0.4805  loss_cate_3: 0  loss_mask_3: 0.5405  loss_dice_3: 0.8062  loss_ce_4: 0.4203  loss_cate_4: 0  loss_mask_4: 0.5718  loss_dice_4: 0.758  loss_ce_5: 0.437  loss_cate_5: 0  loss_mask_5: 0.579  loss_dice_5: 0.8331  loss_ce_6: 0.4502  loss_cate_6: 0  loss_mask_6: 0.5663  loss_dice_6: 0.8035  loss_ce_7: 0.3785  loss_cate_7: 0  loss_mask_7: 0.5586  loss_dice_7: 0.8233  loss_ce_8: 0.4643  loss_cate_8: 0  loss_mask_8: 0.5385  loss_dice_8: 0.8784  time: 1.3906  data_time: 0.0113  lr: 9.4493e-06  max_mem: 23815M
[01/01 11:28:51] d2.utils.events INFO:  eta: 14:03:15  iter: 2399  total_loss: 24.03  loss_ce: 0.5915  loss_cate: 0.1555  loss_mask: 0.6005  loss_dice: 0.8804  loss_ce_0: 0.8988  loss_cate_0: 0  loss_mask_0: 0.6675  loss_dice_0: 0.9764  loss_ce_1: 0.65  loss_cate_1: 0  loss_mask_1: 0.5856  loss_dice_1: 1.073  loss_ce_2: 0.5586  loss_cate_2: 0  loss_mask_2: 0.592  loss_dice_2: 0.9424  loss_ce_3: 0.5383  loss_cate_3: 0  loss_mask_3: 0.5701  loss_dice_3: 0.95  loss_ce_4: 0.6065  loss_cate_4: 0  loss_mask_4: 0.5941  loss_dice_4: 0.896  loss_ce_5: 0.5788  loss_cate_5: 0  loss_mask_5: 0.5763  loss_dice_5: 0.9749  loss_ce_6: 0.614  loss_cate_6: 0  loss_mask_6: 0.5726  loss_dice_6: 1.028  loss_ce_7: 0.6208  loss_cate_7: 0  loss_mask_7: 0.5498  loss_dice_7: 0.9255  loss_ce_8: 0.5733  loss_cate_8: 0  loss_mask_8: 0.5737  loss_dice_8: 0.9545  time: 1.3907  data_time: 0.0127  lr: 9.4446e-06  max_mem: 23815M
[01/01 11:29:19] d2.utils.events INFO:  eta: 14:02:57  iter: 2419  total_loss: 23.57  loss_ce: 0.6277  loss_cate: 0.1489  loss_mask: 0.6016  loss_dice: 0.9185  loss_ce_0: 0.8939  loss_cate_0: 0  loss_mask_0: 0.6175  loss_dice_0: 0.9567  loss_ce_1: 0.6454  loss_cate_1: 0  loss_mask_1: 0.526  loss_dice_1: 0.9109  loss_ce_2: 0.6461  loss_cate_2: 0  loss_mask_2: 0.5948  loss_dice_2: 0.8409  loss_ce_3: 0.6143  loss_cate_3: 0  loss_mask_3: 0.5994  loss_dice_3: 0.8983  loss_ce_4: 0.6507  loss_cate_4: 0  loss_mask_4: 0.58  loss_dice_4: 0.9306  loss_ce_5: 0.5695  loss_cate_5: 0  loss_mask_5: 0.5604  loss_dice_5: 0.9529  loss_ce_6: 0.6024  loss_cate_6: 0  loss_mask_6: 0.5757  loss_dice_6: 0.9227  loss_ce_7: 0.5384  loss_cate_7: 0  loss_mask_7: 0.5986  loss_dice_7: 0.9401  loss_ce_8: 0.6023  loss_cate_8: 0  loss_mask_8: 0.5981  loss_dice_8: 0.919  time: 1.3910  data_time: 0.0152  lr: 9.44e-06  max_mem: 23815M
[01/01 11:29:47] d2.utils.events INFO:  eta: 14:02:32  iter: 2439  total_loss: 22.98  loss_ce: 0.5258  loss_cate: 0.1684  loss_mask: 0.5657  loss_dice: 0.846  loss_ce_0: 0.845  loss_cate_0: 0  loss_mask_0: 0.5958  loss_dice_0: 0.8801  loss_ce_1: 0.5497  loss_cate_1: 0  loss_mask_1: 0.6338  loss_dice_1: 0.8178  loss_ce_2: 0.4958  loss_cate_2: 0  loss_mask_2: 0.5972  loss_dice_2: 0.9139  loss_ce_3: 0.4569  loss_cate_3: 0  loss_mask_3: 0.5435  loss_dice_3: 0.8077  loss_ce_4: 0.3956  loss_cate_4: 0  loss_mask_4: 0.5396  loss_dice_4: 0.8363  loss_ce_5: 0.4591  loss_cate_5: 0  loss_mask_5: 0.5714  loss_dice_5: 0.8503  loss_ce_6: 0.5212  loss_cate_6: 0  loss_mask_6: 0.5625  loss_dice_6: 0.8762  loss_ce_7: 0.5029  loss_cate_7: 0  loss_mask_7: 0.5849  loss_dice_7: 0.8763  loss_ce_8: 0.5265  loss_cate_8: 0  loss_mask_8: 0.5423  loss_dice_8: 0.8601  time: 1.3910  data_time: 0.0116  lr: 9.4354e-06  max_mem: 23815M
[01/01 11:30:15] d2.utils.events INFO:  eta: 14:01:54  iter: 2459  total_loss: 21.53  loss_ce: 0.5338  loss_cate: 0.1328  loss_mask: 0.6296  loss_dice: 0.8477  loss_ce_0: 0.9263  loss_cate_0: 0  loss_mask_0: 0.5863  loss_dice_0: 0.8513  loss_ce_1: 0.5061  loss_cate_1: 0  loss_mask_1: 0.6467  loss_dice_1: 0.8967  loss_ce_2: 0.5726  loss_cate_2: 0  loss_mask_2: 0.5807  loss_dice_2: 0.8561  loss_ce_3: 0.5985  loss_cate_3: 0  loss_mask_3: 0.5363  loss_dice_3: 0.8365  loss_ce_4: 0.6025  loss_cate_4: 0  loss_mask_4: 0.6024  loss_dice_4: 0.8896  loss_ce_5: 0.5635  loss_cate_5: 0  loss_mask_5: 0.5879  loss_dice_5: 0.8765  loss_ce_6: 0.5696  loss_cate_6: 0  loss_mask_6: 0.6207  loss_dice_6: 0.9225  loss_ce_7: 0.601  loss_cate_7: 0  loss_mask_7: 0.6164  loss_dice_7: 0.8535  loss_ce_8: 0.6012  loss_cate_8: 0  loss_mask_8: 0.5971  loss_dice_8: 0.8632  time: 1.3910  data_time: 0.0147  lr: 9.4307e-06  max_mem: 23815M
[01/01 11:30:42] d2.utils.events INFO:  eta: 14:01:19  iter: 2479  total_loss: 22.38  loss_ce: 0.4611  loss_cate: 0.1455  loss_mask: 0.6233  loss_dice: 0.9297  loss_ce_0: 0.9054  loss_cate_0: 0  loss_mask_0: 0.6436  loss_dice_0: 0.8417  loss_ce_1: 0.5181  loss_cate_1: 0  loss_mask_1: 0.7084  loss_dice_1: 0.8147  loss_ce_2: 0.436  loss_cate_2: 0  loss_mask_2: 0.6389  loss_dice_2: 0.8906  loss_ce_3: 0.4411  loss_cate_3: 0  loss_mask_3: 0.6268  loss_dice_3: 0.8467  loss_ce_4: 0.4111  loss_cate_4: 0  loss_mask_4: 0.6571  loss_dice_4: 0.8779  loss_ce_5: 0.4258  loss_cate_5: 0  loss_mask_5: 0.6495  loss_dice_5: 0.9027  loss_ce_6: 0.4328  loss_cate_6: 0  loss_mask_6: 0.6362  loss_dice_6: 0.9152  loss_ce_7: 0.4254  loss_cate_7: 0  loss_mask_7: 0.6062  loss_dice_7: 0.9803  loss_ce_8: 0.4246  loss_cate_8: 0  loss_mask_8: 0.6153  loss_dice_8: 0.9323  time: 1.3909  data_time: 0.0112  lr: 9.4261e-06  max_mem: 23815M
[01/01 11:31:10] d2.utils.events INFO:  eta: 14:00:52  iter: 2499  total_loss: 27.06  loss_ce: 0.657  loss_cate: 0.155  loss_mask: 0.7795  loss_dice: 1.129  loss_ce_0: 1.101  loss_cate_0: 0  loss_mask_0: 0.6522  loss_dice_0: 1.019  loss_ce_1: 0.7064  loss_cate_1: 0  loss_mask_1: 0.7044  loss_dice_1: 1.059  loss_ce_2: 0.7104  loss_cate_2: 0  loss_mask_2: 0.7528  loss_dice_2: 1.072  loss_ce_3: 0.7684  loss_cate_3: 0  loss_mask_3: 0.7572  loss_dice_3: 1.048  loss_ce_4: 0.7158  loss_cate_4: 0  loss_mask_4: 0.7244  loss_dice_4: 0.9947  loss_ce_5: 0.6972  loss_cate_5: 0  loss_mask_5: 0.7978  loss_dice_5: 0.9894  loss_ce_6: 0.7045  loss_cate_6: 0  loss_mask_6: 0.7179  loss_dice_6: 0.9876  loss_ce_7: 0.7014  loss_cate_7: 0  loss_mask_7: 0.83  loss_dice_7: 1.084  loss_ce_8: 0.6827  loss_cate_8: 0  loss_mask_8: 0.8427  loss_dice_8: 1.057  time: 1.3908  data_time: 0.0140  lr: 9.4214e-06  max_mem: 23815M
[01/01 11:31:38] d2.utils.events INFO:  eta: 14:00:39  iter: 2519  total_loss: 23.54  loss_ce: 0.6071  loss_cate: 0.1281  loss_mask: 0.6175  loss_dice: 1.198  loss_ce_0: 1.054  loss_cate_0: 0  loss_mask_0: 0.4735  loss_dice_0: 1.082  loss_ce_1: 0.7267  loss_cate_1: 0  loss_mask_1: 0.5027  loss_dice_1: 1.119  loss_ce_2: 0.6753  loss_cate_2: 0  loss_mask_2: 0.5618  loss_dice_2: 1.123  loss_ce_3: 0.6857  loss_cate_3: 0  loss_mask_3: 0.5883  loss_dice_3: 1.077  loss_ce_4: 0.654  loss_cate_4: 0  loss_mask_4: 0.5574  loss_dice_4: 1.149  loss_ce_5: 0.6615  loss_cate_5: 0  loss_mask_5: 0.5963  loss_dice_5: 1.178  loss_ce_6: 0.6668  loss_cate_6: 0  loss_mask_6: 0.6513  loss_dice_6: 1.302  loss_ce_7: 0.6544  loss_cate_7: 0  loss_mask_7: 0.5751  loss_dice_7: 1.17  loss_ce_8: 0.6284  loss_cate_8: 0  loss_mask_8: 0.6679  loss_dice_8: 1.167  time: 1.3909  data_time: 0.0117  lr: 9.4168e-06  max_mem: 23815M
[01/01 11:32:06] d2.utils.events INFO:  eta: 14:00:18  iter: 2539  total_loss: 21.08  loss_ce: 0.444  loss_cate: 0.1391  loss_mask: 0.8034  loss_dice: 0.8809  loss_ce_0: 1.006  loss_cate_0: 0  loss_mask_0: 0.6735  loss_dice_0: 0.8105  loss_ce_1: 0.6422  loss_cate_1: 0  loss_mask_1: 0.6438  loss_dice_1: 0.8893  loss_ce_2: 0.5374  loss_cate_2: 0  loss_mask_2: 0.7419  loss_dice_2: 0.8685  loss_ce_3: 0.5154  loss_cate_3: 0  loss_mask_3: 0.718  loss_dice_3: 0.8619  loss_ce_4: 0.4702  loss_cate_4: 0  loss_mask_4: 0.7439  loss_dice_4: 0.8677  loss_ce_5: 0.4864  loss_cate_5: 0  loss_mask_5: 0.7867  loss_dice_5: 0.8517  loss_ce_6: 0.5234  loss_cate_6: 0  loss_mask_6: 0.7457  loss_dice_6: 0.8432  loss_ce_7: 0.4797  loss_cate_7: 0  loss_mask_7: 0.7563  loss_dice_7: 0.8669  loss_ce_8: 0.44  loss_cate_8: 0  loss_mask_8: 0.7744  loss_dice_8: 0.9017  time: 1.3908  data_time: 0.0110  lr: 9.4121e-06  max_mem: 23815M
[01/01 11:32:34] d2.utils.events INFO:  eta: 14:00:00  iter: 2559  total_loss: 21.34  loss_ce: 0.6099  loss_cate: 0.1309  loss_mask: 0.6693  loss_dice: 0.8999  loss_ce_0: 0.8476  loss_cate_0: 0  loss_mask_0: 0.5286  loss_dice_0: 0.9642  loss_ce_1: 0.5829  loss_cate_1: 0  loss_mask_1: 0.5547  loss_dice_1: 0.9195  loss_ce_2: 0.5578  loss_cate_2: 0  loss_mask_2: 0.5202  loss_dice_2: 0.9244  loss_ce_3: 0.6252  loss_cate_3: 0  loss_mask_3: 0.5763  loss_dice_3: 0.9141  loss_ce_4: 0.5565  loss_cate_4: 0  loss_mask_4: 0.6487  loss_dice_4: 0.952  loss_ce_5: 0.6152  loss_cate_5: 0  loss_mask_5: 0.5841  loss_dice_5: 0.9046  loss_ce_6: 0.5747  loss_cate_6: 0  loss_mask_6: 0.5808  loss_dice_6: 0.9292  loss_ce_7: 0.5343  loss_cate_7: 0  loss_mask_7: 0.625  loss_dice_7: 0.9255  loss_ce_8: 0.5994  loss_cate_8: 0  loss_mask_8: 0.637  loss_dice_8: 0.9351  time: 1.3908  data_time: 0.0122  lr: 9.4075e-06  max_mem: 23815M
[01/01 11:33:02] d2.utils.events INFO:  eta: 13:59:38  iter: 2579  total_loss: 25.48  loss_ce: 0.673  loss_cate: 0.1858  loss_mask: 0.6798  loss_dice: 0.9593  loss_ce_0: 0.9286  loss_cate_0: 0  loss_mask_0: 0.5761  loss_dice_0: 1.023  loss_ce_1: 0.6634  loss_cate_1: 0  loss_mask_1: 0.603  loss_dice_1: 1.098  loss_ce_2: 0.6082  loss_cate_2: 0  loss_mask_2: 0.6709  loss_dice_2: 1.035  loss_ce_3: 0.6654  loss_cate_3: 0  loss_mask_3: 0.6359  loss_dice_3: 1.017  loss_ce_4: 0.6707  loss_cate_4: 0  loss_mask_4: 0.6851  loss_dice_4: 1.001  loss_ce_5: 0.5704  loss_cate_5: 0  loss_mask_5: 0.6383  loss_dice_5: 0.914  loss_ce_6: 0.658  loss_cate_6: 0  loss_mask_6: 0.6433  loss_dice_6: 0.9592  loss_ce_7: 0.6018  loss_cate_7: 0  loss_mask_7: 0.6326  loss_dice_7: 0.9493  loss_ce_8: 0.5947  loss_cate_8: 0  loss_mask_8: 0.6077  loss_dice_8: 0.9535  time: 1.3908  data_time: 0.0126  lr: 9.4028e-06  max_mem: 23815M
[01/01 11:33:29] d2.data.datasets.coco INFO: Loaded 653 images with semantic segmentation from /opt/ml/input/data/images/val_0
[01/01 11:33:29] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=1280, sample_style='choice')]
[01/01 11:33:29] d2.data.common INFO: Serializing 653 elements to byte tensors and concatenating them all ...
[01/01 11:33:29] d2.data.common INFO: Serialized dataset takes 0.09 MiB
[01/01 11:33:29] d2.data.datasets.coco INFO: Loaded 653 images with semantic segmentation from /opt/ml/input/data/images/val_0
[01/01 11:33:29] d2.evaluation.evaluator INFO: Start inference on 653 batches
[01/01 11:33:31] d2.evaluation.evaluator INFO: Inference done 11/653. Dataloading: 0.0012 s/iter. Inference: 0.1072 s/iter. Eval: 0.0128 s/iter. Total: 0.1211 s/iter. ETA=0:01:17
[01/01 11:33:36] d2.evaluation.evaluator INFO: Inference done 53/653. Dataloading: 0.0016 s/iter. Inference: 0.1063 s/iter. Eval: 0.0126 s/iter. Total: 0.1206 s/iter. ETA=0:01:12
[01/01 11:33:41] d2.evaluation.evaluator INFO: Inference done 95/653. Dataloading: 0.0017 s/iter. Inference: 0.1059 s/iter. Eval: 0.0125 s/iter. Total: 0.1201 s/iter. ETA=0:01:07
[01/01 11:33:46] d2.evaluation.evaluator INFO: Inference done 137/653. Dataloading: 0.0017 s/iter. Inference: 0.1059 s/iter. Eval: 0.0124 s/iter. Total: 0.1201 s/iter. ETA=0:01:01
[01/01 11:33:51] d2.evaluation.evaluator INFO: Inference done 179/653. Dataloading: 0.0017 s/iter. Inference: 0.1059 s/iter. Eval: 0.0125 s/iter. Total: 0.1202 s/iter. ETA=0:00:56
[01/01 11:33:56] d2.evaluation.evaluator INFO: Inference done 221/653. Dataloading: 0.0017 s/iter. Inference: 0.1059 s/iter. Eval: 0.0126 s/iter. Total: 0.1203 s/iter. ETA=0:00:51
[01/01 11:34:01] d2.evaluation.evaluator INFO: Inference done 263/653. Dataloading: 0.0017 s/iter. Inference: 0.1060 s/iter. Eval: 0.0127 s/iter. Total: 0.1205 s/iter. ETA=0:00:47
[01/01 11:34:07] d2.evaluation.evaluator INFO: Inference done 306/653. Dataloading: 0.0017 s/iter. Inference: 0.1058 s/iter. Eval: 0.0127 s/iter. Total: 0.1203 s/iter. ETA=0:00:41
[01/01 11:34:12] d2.evaluation.evaluator INFO: Inference done 347/653. Dataloading: 0.0017 s/iter. Inference: 0.1061 s/iter. Eval: 0.0127 s/iter. Total: 0.1205 s/iter. ETA=0:00:36
[01/01 11:34:17] d2.evaluation.evaluator INFO: Inference done 389/653. Dataloading: 0.0017 s/iter. Inference: 0.1060 s/iter. Eval: 0.0127 s/iter. Total: 0.1205 s/iter. ETA=0:00:31
[01/01 11:34:22] d2.evaluation.evaluator INFO: Inference done 431/653. Dataloading: 0.0017 s/iter. Inference: 0.1060 s/iter. Eval: 0.0128 s/iter. Total: 0.1206 s/iter. ETA=0:00:26
[01/01 11:34:27] d2.evaluation.evaluator INFO: Inference done 473/653. Dataloading: 0.0017 s/iter. Inference: 0.1060 s/iter. Eval: 0.0128 s/iter. Total: 0.1206 s/iter. ETA=0:00:21
[01/01 11:34:32] d2.evaluation.evaluator INFO: Inference done 515/653. Dataloading: 0.0017 s/iter. Inference: 0.1060 s/iter. Eval: 0.0128 s/iter. Total: 0.1206 s/iter. ETA=0:00:16
[01/01 11:34:37] d2.evaluation.evaluator INFO: Inference done 557/653. Dataloading: 0.0017 s/iter. Inference: 0.1061 s/iter. Eval: 0.0128 s/iter. Total: 0.1206 s/iter. ETA=0:00:11
[01/01 11:34:42] d2.evaluation.evaluator INFO: Inference done 599/653. Dataloading: 0.0017 s/iter. Inference: 0.1060 s/iter. Eval: 0.0127 s/iter. Total: 0.1205 s/iter. ETA=0:00:06
[01/01 11:34:47] d2.evaluation.evaluator INFO: Inference done 641/653. Dataloading: 0.0017 s/iter. Inference: 0.1060 s/iter. Eval: 0.0127 s/iter. Total: 0.1205 s/iter. ETA=0:00:01
[01/01 11:34:48] d2.evaluation.evaluator INFO: Total inference time: 0:01:18.109339 (0.120539 s / iter per device, on 1 devices)
[01/01 11:34:48] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:01:08 (0.106040 s / iter per device, on 1 devices)
[01/01 11:34:48] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 57.290279894435436, 'fwIoU': 88.50361079088131, 'IoU-Backgroud': 96.83585051941898, 'IoU-General trash': 29.656160397144323, 'IoU-Paper': 73.4466719741499, 'IoU-Paper pack': 31.565074701074032, 'IoU-Metal': 51.21073219899686, 'IoU-Glass': 61.84854946806018, 'IoU-Plastic': 46.860649864806255, 'IoU-Styrofoam': 70.58174509548783, 'IoU-Plastic bag': 81.20099139227423, 'IoU-Battery': 0.0, 'IoU-Clothing': 86.9866532273772, 'mACC': 67.9091784657024, 'pACC': 92.70580577996387, 'ACC-Backgroud': 98.66139364836957, 'ACC-General trash': 53.26550768675005, 'ACC-Paper': 77.35002159900634, 'ACC-Paper pack': 59.21645785815821, 'ACC-Metal': 61.401363323770696, 'ACC-Glass': 64.01476003821386, 'ACC-Plastic': 76.8184090316318, 'ACC-Styrofoam': 78.2918316436037, 'ACC-Plastic bag': 88.33869896773857, 'ACC-Battery': 0.0, 'ACC-Clothing': 89.64251932548355})])
[01/01 11:34:48] d2.engine.defaults INFO: Evaluation results for trash_recycle_sem_seg_val_0 in csv format:
[01/01 11:34:48] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/01 11:34:48] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/01 11:34:48] d2.evaluation.testing INFO: copypaste: 57.2903,88.5036,67.9092,92.7058
[01/01 11:34:48] fvcore.common.checkpoint INFO: Saving checkpoint to ./trash_dataV1_WarmupPolyLR_1e-5/model_best_2599iter.pth
[01/01 11:34:50] d2.engine.hooks INFO: Saved best model as latest eval score for mIoU is57.29028, better than last best score 44.99854 @ iteration 1299.
[01/01 11:34:50] d2.utils.events INFO:  eta: 13:59:18  iter: 2599  total_loss: 24.12  loss_ce: 0.6622  loss_cate: 0.1976  loss_mask: 0.6705  loss_dice: 1.077  loss_ce_0: 0.9603  loss_cate_0: 0  loss_mask_0: 0.6764  loss_dice_0: 0.9624  loss_ce_1: 0.6523  loss_cate_1: 0  loss_mask_1: 0.7012  loss_dice_1: 0.9985  loss_ce_2: 0.6287  loss_cate_2: 0  loss_mask_2: 0.6888  loss_dice_2: 1.072  loss_ce_3: 0.6244  loss_cate_3: 0  loss_mask_3: 0.7266  loss_dice_3: 1.024  loss_ce_4: 0.6366  loss_cate_4: 0  loss_mask_4: 0.6862  loss_dice_4: 1.044  loss_ce_5: 0.5733  loss_cate_5: 0  loss_mask_5: 0.6748  loss_dice_5: 0.9839  loss_ce_6: 0.5668  loss_cate_6: 0  loss_mask_6: 0.7402  loss_dice_6: 1.004  loss_ce_7: 0.589  loss_cate_7: 0  loss_mask_7: 0.6865  loss_dice_7: 0.9716  loss_ce_8: 0.6442  loss_cate_8: 0  loss_mask_8: 0.7248  loss_dice_8: 0.958  time: 1.3908  data_time: 0.0131  lr: 9.3982e-06  max_mem: 23815M
[01/01 11:35:18] d2.utils.events INFO:  eta: 13:58:57  iter: 2619  total_loss: 24.06  loss_ce: 0.6093  loss_cate: 0.1629  loss_mask: 0.7987  loss_dice: 0.8882  loss_ce_0: 0.9144  loss_cate_0: 0  loss_mask_0: 0.6647  loss_dice_0: 0.8476  loss_ce_1: 0.7064  loss_cate_1: 0  loss_mask_1: 0.6959  loss_dice_1: 0.8684  loss_ce_2: 0.5906  loss_cate_2: 0  loss_mask_2: 0.7465  loss_dice_2: 0.8832  loss_ce_3: 0.6593  loss_cate_3: 0  loss_mask_3: 0.7153  loss_dice_3: 0.8309  loss_ce_4: 0.5973  loss_cate_4: 0  loss_mask_4: 0.7798  loss_dice_4: 0.8207  loss_ce_5: 0.6035  loss_cate_5: 0  loss_mask_5: 0.8212  loss_dice_5: 0.8359  loss_ce_6: 0.5293  loss_cate_6: 0  loss_mask_6: 0.8877  loss_dice_6: 0.8964  loss_ce_7: 0.5816  loss_cate_7: 0  loss_mask_7: 0.8268  loss_dice_7: 0.9136  loss_ce_8: 0.5789  loss_cate_8: 0  loss_mask_8: 0.8311  loss_dice_8: 0.9162  time: 1.3907  data_time: 0.0118  lr: 9.3935e-06  max_mem: 23815M
[01/01 11:35:46] d2.utils.events INFO:  eta: 13:58:38  iter: 2639  total_loss: 22.08  loss_ce: 0.5117  loss_cate: 0.1814  loss_mask: 0.7518  loss_dice: 1.025  loss_ce_0: 0.9951  loss_cate_0: 0  loss_mask_0: 0.7006  loss_dice_0: 1.029  loss_ce_1: 0.6035  loss_cate_1: 0  loss_mask_1: 0.7369  loss_dice_1: 0.9889  loss_ce_2: 0.4939  loss_cate_2: 0  loss_mask_2: 0.6912  loss_dice_2: 1.044  loss_ce_3: 0.5012  loss_cate_3: 0  loss_mask_3: 0.731  loss_dice_3: 0.9723  loss_ce_4: 0.5783  loss_cate_4: 0  loss_mask_4: 0.7083  loss_dice_4: 1  loss_ce_5: 0.488  loss_cate_5: 0  loss_mask_5: 0.7405  loss_dice_5: 1.035  loss_ce_6: 0.5089  loss_cate_6: 0  loss_mask_6: 0.6288  loss_dice_6: 0.9912  loss_ce_7: 0.5576  loss_cate_7: 0  loss_mask_7: 0.6318  loss_dice_7: 1.008  loss_ce_8: 0.5126  loss_cate_8: 0  loss_mask_8: 0.7082  loss_dice_8: 1.048  time: 1.3908  data_time: 0.0131  lr: 9.3889e-06  max_mem: 23815M
[01/01 11:36:14] d2.utils.events INFO:  eta: 13:58:21  iter: 2659  total_loss: 25.73  loss_ce: 0.5536  loss_cate: 0.1898  loss_mask: 0.8315  loss_dice: 0.9648  loss_ce_0: 0.9991  loss_cate_0: 0  loss_mask_0: 0.8117  loss_dice_0: 0.9409  loss_ce_1: 0.7227  loss_cate_1: 0  loss_mask_1: 0.8251  loss_dice_1: 0.9467  loss_ce_2: 0.6176  loss_cate_2: 0  loss_mask_2: 0.7777  loss_dice_2: 0.9342  loss_ce_3: 0.6479  loss_cate_3: 0  loss_mask_3: 0.8455  loss_dice_3: 0.9328  loss_ce_4: 0.5887  loss_cate_4: 0  loss_mask_4: 0.7747  loss_dice_4: 0.9389  loss_ce_5: 0.5725  loss_cate_5: 0  loss_mask_5: 0.8213  loss_dice_5: 0.9351  loss_ce_6: 0.5596  loss_cate_6: 0  loss_mask_6: 0.8293  loss_dice_6: 0.9389  loss_ce_7: 0.5599  loss_cate_7: 0  loss_mask_7: 0.8249  loss_dice_7: 0.9245  loss_ce_8: 0.5677  loss_cate_8: 0  loss_mask_8: 0.8328  loss_dice_8: 0.9662  time: 1.3908  data_time: 0.0121  lr: 9.3842e-06  max_mem: 23815M
[01/01 11:36:42] d2.utils.events INFO:  eta: 13:57:55  iter: 2679  total_loss: 20.62  loss_ce: 0.5036  loss_cate: 0.155  loss_mask: 0.5121  loss_dice: 0.8229  loss_ce_0: 0.9264  loss_cate_0: 0  loss_mask_0: 0.4682  loss_dice_0: 0.8656  loss_ce_1: 0.6678  loss_cate_1: 0  loss_mask_1: 0.5431  loss_dice_1: 0.88  loss_ce_2: 0.5455  loss_cate_2: 0  loss_mask_2: 0.4986  loss_dice_2: 0.9003  loss_ce_3: 0.5548  loss_cate_3: 0  loss_mask_3: 0.5338  loss_dice_3: 0.8662  loss_ce_4: 0.4948  loss_cate_4: 0  loss_mask_4: 0.4864  loss_dice_4: 0.8683  loss_ce_5: 0.5593  loss_cate_5: 0  loss_mask_5: 0.5197  loss_dice_5: 0.8036  loss_ce_6: 0.4965  loss_cate_6: 0  loss_mask_6: 0.5165  loss_dice_6: 0.8542  loss_ce_7: 0.5026  loss_cate_7: 0  loss_mask_7: 0.5364  loss_dice_7: 0.8639  loss_ce_8: 0.5092  loss_cate_8: 0  loss_mask_8: 0.4964  loss_dice_8: 0.8728  time: 1.3908  data_time: 0.0124  lr: 9.3796e-06  max_mem: 23815M
[01/01 11:37:10] d2.utils.events INFO:  eta: 13:57:34  iter: 2699  total_loss: 17.56  loss_ce: 0.497  loss_cate: 0.121  loss_mask: 0.5253  loss_dice: 0.6922  loss_ce_0: 0.8285  loss_cate_0: 0  loss_mask_0: 0.5527  loss_dice_0: 0.6302  loss_ce_1: 0.6244  loss_cate_1: 0  loss_mask_1: 0.5003  loss_dice_1: 0.6917  loss_ce_2: 0.5562  loss_cate_2: 0  loss_mask_2: 0.491  loss_dice_2: 0.6775  loss_ce_3: 0.5163  loss_cate_3: 0  loss_mask_3: 0.473  loss_dice_3: 0.6264  loss_ce_4: 0.5164  loss_cate_4: 0  loss_mask_4: 0.4987  loss_dice_4: 0.6493  loss_ce_5: 0.4934  loss_cate_5: 0  loss_mask_5: 0.4964  loss_dice_5: 0.6429  loss_ce_6: 0.5262  loss_cate_6: 0  loss_mask_6: 0.5253  loss_dice_6: 0.6587  loss_ce_7: 0.4911  loss_cate_7: 0  loss_mask_7: 0.5124  loss_dice_7: 0.6773  loss_ce_8: 0.4909  loss_cate_8: 0  loss_mask_8: 0.5046  loss_dice_8: 0.6678  time: 1.3907  data_time: 0.0109  lr: 9.3749e-06  max_mem: 23815M
[01/01 11:37:37] d2.utils.events INFO:  eta: 13:57:11  iter: 2719  total_loss: 22.98  loss_ce: 0.473  loss_cate: 0.1742  loss_mask: 0.75  loss_dice: 0.8807  loss_ce_0: 0.9208  loss_cate_0: 0  loss_mask_0: 0.8647  loss_dice_0: 0.9011  loss_ce_1: 0.6174  loss_cate_1: 0  loss_mask_1: 0.7743  loss_dice_1: 0.8266  loss_ce_2: 0.5754  loss_cate_2: 0  loss_mask_2: 0.7967  loss_dice_2: 0.8392  loss_ce_3: 0.4323  loss_cate_3: 0  loss_mask_3: 0.7648  loss_dice_3: 0.8476  loss_ce_4: 0.4363  loss_cate_4: 0  loss_mask_4: 0.7384  loss_dice_4: 0.8816  loss_ce_5: 0.5449  loss_cate_5: 0  loss_mask_5: 0.7544  loss_dice_5: 0.8565  loss_ce_6: 0.4986  loss_cate_6: 0  loss_mask_6: 0.7456  loss_dice_6: 0.8381  loss_ce_7: 0.5869  loss_cate_7: 0  loss_mask_7: 0.7525  loss_dice_7: 0.8427  loss_ce_8: 0.4796  loss_cate_8: 0  loss_mask_8: 0.7331  loss_dice_8: 0.878  time: 1.3906  data_time: 0.0103  lr: 9.3703e-06  max_mem: 23815M
[01/01 11:38:05] d2.utils.events INFO:  eta: 13:56:46  iter: 2739  total_loss: 22.95  loss_ce: 0.7037  loss_cate: 0.1626  loss_mask: 0.7301  loss_dice: 1.138  loss_ce_0: 1.062  loss_cate_0: 0  loss_mask_0: 0.6615  loss_dice_0: 1.166  loss_ce_1: 0.641  loss_cate_1: 0  loss_mask_1: 0.6487  loss_dice_1: 1.158  loss_ce_2: 0.5907  loss_cate_2: 0  loss_mask_2: 0.613  loss_dice_2: 1.059  loss_ce_3: 0.5797  loss_cate_3: 0  loss_mask_3: 0.6964  loss_dice_3: 0.9911  loss_ce_4: 0.5892  loss_cate_4: 0  loss_mask_4: 0.6815  loss_dice_4: 1.126  loss_ce_5: 0.7044  loss_cate_5: 0  loss_mask_5: 0.6203  loss_dice_5: 1.189  loss_ce_6: 0.6975  loss_cate_6: 0  loss_mask_6: 0.6975  loss_dice_6: 1.162  loss_ce_7: 0.6834  loss_cate_7: 0  loss_mask_7: 0.6293  loss_dice_7: 1.103  loss_ce_8: 0.6184  loss_cate_8: 0  loss_mask_8: 0.6551  loss_dice_8: 1.117  time: 1.3906  data_time: 0.0102  lr: 9.3656e-06  max_mem: 23815M
[01/01 11:38:33] d2.utils.events INFO:  eta: 13:56:27  iter: 2759  total_loss: 20.11  loss_ce: 0.416  loss_cate: 0.1464  loss_mask: 0.5879  loss_dice: 0.8665  loss_ce_0: 0.7925  loss_cate_0: 0  loss_mask_0: 0.5514  loss_dice_0: 0.7923  loss_ce_1: 0.5279  loss_cate_1: 0  loss_mask_1: 0.6132  loss_dice_1: 0.7526  loss_ce_2: 0.4854  loss_cate_2: 0  loss_mask_2: 0.6298  loss_dice_2: 0.7477  loss_ce_3: 0.5467  loss_cate_3: 0  loss_mask_3: 0.5446  loss_dice_3: 0.7595  loss_ce_4: 0.5112  loss_cate_4: 0  loss_mask_4: 0.6249  loss_dice_4: 0.7343  loss_ce_5: 0.44  loss_cate_5: 0  loss_mask_5: 0.6241  loss_dice_5: 0.8772  loss_ce_6: 0.4898  loss_cate_6: 0  loss_mask_6: 0.5533  loss_dice_6: 0.8343  loss_ce_7: 0.4888  loss_cate_7: 0  loss_mask_7: 0.54  loss_dice_7: 0.7719  loss_ce_8: 0.4747  loss_cate_8: 0  loss_mask_8: 0.6182  loss_dice_8: 0.8333  time: 1.3908  data_time: 0.0107  lr: 9.361e-06  max_mem: 23815M
[01/01 11:39:01] d2.utils.events INFO:  eta: 13:55:54  iter: 2779  total_loss: 20.56  loss_ce: 0.4376  loss_cate: 0.1339  loss_mask: 0.5657  loss_dice: 0.894  loss_ce_0: 0.8435  loss_cate_0: 0  loss_mask_0: 0.5054  loss_dice_0: 0.8276  loss_ce_1: 0.5874  loss_cate_1: 0  loss_mask_1: 0.542  loss_dice_1: 0.9308  loss_ce_2: 0.4738  loss_cate_2: 0  loss_mask_2: 0.5422  loss_dice_2: 0.9106  loss_ce_3: 0.4448  loss_cate_3: 0  loss_mask_3: 0.5382  loss_dice_3: 0.8562  loss_ce_4: 0.4877  loss_cate_4: 0  loss_mask_4: 0.5757  loss_dice_4: 0.8526  loss_ce_5: 0.4606  loss_cate_5: 0  loss_mask_5: 0.5538  loss_dice_5: 0.859  loss_ce_6: 0.4872  loss_cate_6: 0  loss_mask_6: 0.566  loss_dice_6: 0.86  loss_ce_7: 0.4345  loss_cate_7: 0  loss_mask_7: 0.5862  loss_dice_7: 0.8695  loss_ce_8: 0.4293  loss_cate_8: 0  loss_mask_8: 0.5587  loss_dice_8: 0.8719  time: 1.3907  data_time: 0.0108  lr: 9.3563e-06  max_mem: 23815M
[01/01 11:39:29] d2.utils.events INFO:  eta: 13:55:25  iter: 2799  total_loss: 21.72  loss_ce: 0.5666  loss_cate: 0.1379  loss_mask: 0.5256  loss_dice: 0.8457  loss_ce_0: 1.024  loss_cate_0: 0  loss_mask_0: 0.5248  loss_dice_0: 0.7736  loss_ce_1: 0.6313  loss_cate_1: 0  loss_mask_1: 0.5461  loss_dice_1: 0.8165  loss_ce_2: 0.531  loss_cate_2: 0  loss_mask_2: 0.5409  loss_dice_2: 0.8512  loss_ce_3: 0.4968  loss_cate_3: 0  loss_mask_3: 0.4992  loss_dice_3: 0.8226  loss_ce_4: 0.5105  loss_cate_4: 0  loss_mask_4: 0.5171  loss_dice_4: 0.8528  loss_ce_5: 0.4955  loss_cate_5: 0  loss_mask_5: 0.575  loss_dice_5: 0.861  loss_ce_6: 0.4882  loss_cate_6: 0  loss_mask_6: 0.5544  loss_dice_6: 0.8297  loss_ce_7: 0.4792  loss_cate_7: 0  loss_mask_7: 0.5249  loss_dice_7: 0.8474  loss_ce_8: 0.4974  loss_cate_8: 0  loss_mask_8: 0.5562  loss_dice_8: 0.8366  time: 1.3906  data_time: 0.0141  lr: 9.3517e-06  max_mem: 23815M
[01/01 11:39:56] d2.utils.events INFO:  eta: 13:54:56  iter: 2819  total_loss: 17.52  loss_ce: 0.4866  loss_cate: 0.1347  loss_mask: 0.4703  loss_dice: 0.7947  loss_ce_0: 0.8848  loss_cate_0: 0  loss_mask_0: 0.5034  loss_dice_0: 0.7923  loss_ce_1: 0.5599  loss_cate_1: 0  loss_mask_1: 0.5502  loss_dice_1: 0.7822  loss_ce_2: 0.4924  loss_cate_2: 0  loss_mask_2: 0.5929  loss_dice_2: 0.7944  loss_ce_3: 0.5108  loss_cate_3: 0  loss_mask_3: 0.5106  loss_dice_3: 0.7635  loss_ce_4: 0.4853  loss_cate_4: 0  loss_mask_4: 0.5771  loss_dice_4: 0.8004  loss_ce_5: 0.4704  loss_cate_5: 0  loss_mask_5: 0.5117  loss_dice_5: 0.7597  loss_ce_6: 0.4559  loss_cate_6: 0  loss_mask_6: 0.5593  loss_dice_6: 0.8816  loss_ce_7: 0.4915  loss_cate_7: 0  loss_mask_7: 0.4968  loss_dice_7: 0.8422  loss_ce_8: 0.4684  loss_cate_8: 0  loss_mask_8: 0.4996  loss_dice_8: 0.8133  time: 1.3906  data_time: 0.0128  lr: 9.347e-06  max_mem: 23815M
[01/01 11:40:24] d2.utils.events INFO:  eta: 13:54:24  iter: 2839  total_loss: 18.36  loss_ce: 0.4724  loss_cate: 0.159  loss_mask: 0.5598  loss_dice: 0.7126  loss_ce_0: 0.7818  loss_cate_0: 0  loss_mask_0: 0.5481  loss_dice_0: 0.7398  loss_ce_1: 0.4967  loss_cate_1: 0  loss_mask_1: 0.583  loss_dice_1: 0.6968  loss_ce_2: 0.4441  loss_cate_2: 0  loss_mask_2: 0.6063  loss_dice_2: 0.7174  loss_ce_3: 0.4155  loss_cate_3: 0  loss_mask_3: 0.5891  loss_dice_3: 0.7515  loss_ce_4: 0.4886  loss_cate_4: 0  loss_mask_4: 0.574  loss_dice_4: 0.7107  loss_ce_5: 0.4241  loss_cate_5: 0  loss_mask_5: 0.5705  loss_dice_5: 0.7142  loss_ce_6: 0.4367  loss_cate_6: 0  loss_mask_6: 0.5896  loss_dice_6: 0.697  loss_ce_7: 0.3725  loss_cate_7: 0  loss_mask_7: 0.5752  loss_dice_7: 0.7488  loss_ce_8: 0.365  loss_cate_8: 0  loss_mask_8: 0.551  loss_dice_8: 0.7367  time: 1.3905  data_time: 0.0119  lr: 9.3424e-06  max_mem: 23815M
[01/01 11:40:52] d2.utils.events INFO:  eta: 13:53:52  iter: 2859  total_loss: 19.78  loss_ce: 0.551  loss_cate: 0.131  loss_mask: 0.4875  loss_dice: 0.8907  loss_ce_0: 0.7869  loss_cate_0: 0  loss_mask_0: 0.523  loss_dice_0: 0.9544  loss_ce_1: 0.6151  loss_cate_1: 0  loss_mask_1: 0.5396  loss_dice_1: 0.8612  loss_ce_2: 0.544  loss_cate_2: 0  loss_mask_2: 0.5444  loss_dice_2: 0.8512  loss_ce_3: 0.4925  loss_cate_3: 0  loss_mask_3: 0.5181  loss_dice_3: 0.8603  loss_ce_4: 0.5302  loss_cate_4: 0  loss_mask_4: 0.5229  loss_dice_4: 0.8786  loss_ce_5: 0.5387  loss_cate_5: 0  loss_mask_5: 0.5547  loss_dice_5: 0.8782  loss_ce_6: 0.516  loss_cate_6: 0  loss_mask_6: 0.5497  loss_dice_6: 0.8229  loss_ce_7: 0.5014  loss_cate_7: 0  loss_mask_7: 0.5134  loss_dice_7: 0.8591  loss_ce_8: 0.5463  loss_cate_8: 0  loss_mask_8: 0.5067  loss_dice_8: 0.8266  time: 1.3906  data_time: 0.0134  lr: 9.3377e-06  max_mem: 23815M
[01/01 11:41:20] d2.utils.events INFO:  eta: 13:53:17  iter: 2879  total_loss: 19.46  loss_ce: 0.4495  loss_cate: 0.1295  loss_mask: 0.6587  loss_dice: 0.7242  loss_ce_0: 0.6865  loss_cate_0: 0  loss_mask_0: 0.5092  loss_dice_0: 0.7764  loss_ce_1: 0.648  loss_cate_1: 0  loss_mask_1: 0.594  loss_dice_1: 0.681  loss_ce_2: 0.5281  loss_cate_2: 0  loss_mask_2: 0.5956  loss_dice_2: 0.7296  loss_ce_3: 0.4704  loss_cate_3: 0  loss_mask_3: 0.5981  loss_dice_3: 0.6906  loss_ce_4: 0.4821  loss_cate_4: 0  loss_mask_4: 0.5566  loss_dice_4: 0.7249  loss_ce_5: 0.4488  loss_cate_5: 0  loss_mask_5: 0.5981  loss_dice_5: 0.7059  loss_ce_6: 0.4733  loss_cate_6: 0  loss_mask_6: 0.5958  loss_dice_6: 0.6773  loss_ce_7: 0.4938  loss_cate_7: 0  loss_mask_7: 0.5956  loss_dice_7: 0.713  loss_ce_8: 0.4701  loss_cate_8: 0  loss_mask_8: 0.6434  loss_dice_8: 0.7002  time: 1.3905  data_time: 0.0109  lr: 9.3331e-06  max_mem: 23815M
[01/01 11:41:47] d2.utils.events INFO:  eta: 13:52:45  iter: 2899  total_loss: 18.93  loss_ce: 0.5511  loss_cate: 0.1172  loss_mask: 0.5532  loss_dice: 0.7073  loss_ce_0: 0.9879  loss_cate_0: 0  loss_mask_0: 0.5336  loss_dice_0: 0.7066  loss_ce_1: 0.5868  loss_cate_1: 0  loss_mask_1: 0.6056  loss_dice_1: 0.7464  loss_ce_2: 0.5277  loss_cate_2: 0  loss_mask_2: 0.5409  loss_dice_2: 0.7745  loss_ce_3: 0.5622  loss_cate_3: 0  loss_mask_3: 0.5437  loss_dice_3: 0.6997  loss_ce_4: 0.5509  loss_cate_4: 0  loss_mask_4: 0.5426  loss_dice_4: 0.6953  loss_ce_5: 0.5105  loss_cate_5: 0  loss_mask_5: 0.5785  loss_dice_5: 0.7014  loss_ce_6: 0.572  loss_cate_6: 0  loss_mask_6: 0.6024  loss_dice_6: 0.6995  loss_ce_7: 0.5444  loss_cate_7: 0  loss_mask_7: 0.5794  loss_dice_7: 0.7081  loss_ce_8: 0.5469  loss_cate_8: 0  loss_mask_8: 0.5557  loss_dice_8: 0.7408  time: 1.3904  data_time: 0.0132  lr: 9.3284e-06  max_mem: 23815M
[01/01 11:42:15] d2.utils.events INFO:  eta: 13:52:09  iter: 2919  total_loss: 23.98  loss_ce: 0.6221  loss_cate: 0.1467  loss_mask: 0.7226  loss_dice: 1.006  loss_ce_0: 1.028  loss_cate_0: 0  loss_mask_0: 0.7686  loss_dice_0: 0.9585  loss_ce_1: 0.68  loss_cate_1: 0  loss_mask_1: 0.711  loss_dice_1: 1.018  loss_ce_2: 0.6055  loss_cate_2: 0  loss_mask_2: 0.6915  loss_dice_2: 1.112  loss_ce_3: 0.6385  loss_cate_3: 0  loss_mask_3: 0.6694  loss_dice_3: 1.052  loss_ce_4: 0.6434  loss_cate_4: 0  loss_mask_4: 0.6812  loss_dice_4: 1.047  loss_ce_5: 0.631  loss_cate_5: 0  loss_mask_5: 0.7076  loss_dice_5: 1.052  loss_ce_6: 0.6619  loss_cate_6: 0  loss_mask_6: 0.7298  loss_dice_6: 1.032  loss_ce_7: 0.655  loss_cate_7: 0  loss_mask_7: 0.77  loss_dice_7: 1.054  loss_ce_8: 0.6896  loss_cate_8: 0  loss_mask_8: 0.7029  loss_dice_8: 1.006  time: 1.3904  data_time: 0.0108  lr: 9.3238e-06  max_mem: 23815M
[01/01 11:42:43] d2.utils.events INFO:  eta: 13:51:41  iter: 2939  total_loss: 22.86  loss_ce: 0.625  loss_cate: 0.1649  loss_mask: 0.5429  loss_dice: 0.9377  loss_ce_0: 0.8904  loss_cate_0: 0  loss_mask_0: 0.5177  loss_dice_0: 0.8868  loss_ce_1: 0.6751  loss_cate_1: 0  loss_mask_1: 0.5238  loss_dice_1: 0.9148  loss_ce_2: 0.5717  loss_cate_2: 0  loss_mask_2: 0.4866  loss_dice_2: 0.9457  loss_ce_3: 0.5929  loss_cate_3: 0  loss_mask_3: 0.4985  loss_dice_3: 0.8848  loss_ce_4: 0.6785  loss_cate_4: 0  loss_mask_4: 0.5535  loss_dice_4: 0.8503  loss_ce_5: 0.6406  loss_cate_5: 0  loss_mask_5: 0.5093  loss_dice_5: 0.8914  loss_ce_6: 0.6301  loss_cate_6: 0  loss_mask_6: 0.5266  loss_dice_6: 0.9049  loss_ce_7: 0.6918  loss_cate_7: 0  loss_mask_7: 0.545  loss_dice_7: 0.8875  loss_ce_8: 0.6493  loss_cate_8: 0  loss_mask_8: 0.5134  loss_dice_8: 0.9091  time: 1.3903  data_time: 0.0147  lr: 9.3191e-06  max_mem: 23815M
[01/01 11:43:11] d2.utils.events INFO:  eta: 13:51:21  iter: 2959  total_loss: 24.51  loss_ce: 0.7043  loss_cate: 0.1654  loss_mask: 0.5768  loss_dice: 0.9522  loss_ce_0: 0.9996  loss_cate_0: 0  loss_mask_0: 0.6406  loss_dice_0: 0.9931  loss_ce_1: 0.7413  loss_cate_1: 0  loss_mask_1: 0.5256  loss_dice_1: 1.043  loss_ce_2: 0.6604  loss_cate_2: 0  loss_mask_2: 0.5298  loss_dice_2: 0.9563  loss_ce_3: 0.6783  loss_cate_3: 0  loss_mask_3: 0.5264  loss_dice_3: 0.9181  loss_ce_4: 0.7359  loss_cate_4: 0  loss_mask_4: 0.5304  loss_dice_4: 0.9225  loss_ce_5: 0.7327  loss_cate_5: 0  loss_mask_5: 0.5574  loss_dice_5: 0.9483  loss_ce_6: 0.7369  loss_cate_6: 0  loss_mask_6: 0.5746  loss_dice_6: 0.9447  loss_ce_7: 0.6904  loss_cate_7: 0  loss_mask_7: 0.5377  loss_dice_7: 0.9591  loss_ce_8: 0.7185  loss_cate_8: 0  loss_mask_8: 0.57  loss_dice_8: 0.9813  time: 1.3904  data_time: 0.0119  lr: 9.3145e-06  max_mem: 23815M
[01/01 11:43:39] d2.utils.events INFO:  eta: 13:50:58  iter: 2979  total_loss: 20.36  loss_ce: 0.6405  loss_cate: 0.1669  loss_mask: 0.4669  loss_dice: 0.8829  loss_ce_0: 1.124  loss_cate_0: 0  loss_mask_0: 0.4358  loss_dice_0: 0.9143  loss_ce_1: 0.7022  loss_cate_1: 0  loss_mask_1: 0.458  loss_dice_1: 0.7843  loss_ce_2: 0.6026  loss_cate_2: 0  loss_mask_2: 0.4506  loss_dice_2: 0.7287  loss_ce_3: 0.6371  loss_cate_3: 0  loss_mask_3: 0.5442  loss_dice_3: 0.8199  loss_ce_4: 0.6472  loss_cate_4: 0  loss_mask_4: 0.4702  loss_dice_4: 0.8059  loss_ce_5: 0.6237  loss_cate_5: 0  loss_mask_5: 0.4771  loss_dice_5: 0.8147  loss_ce_6: 0.6438  loss_cate_6: 0  loss_mask_6: 0.4902  loss_dice_6: 0.7893  loss_ce_7: 0.6204  loss_cate_7: 0  loss_mask_7: 0.4433  loss_dice_7: 0.8748  loss_ce_8: 0.63  loss_cate_8: 0  loss_mask_8: 0.4731  loss_dice_8: 0.8395  time: 1.3904  data_time: 0.0133  lr: 9.3098e-06  max_mem: 23815M
[01/01 11:44:07] d2.utils.events INFO:  eta: 13:50:36  iter: 2999  total_loss: 25.15  loss_ce: 0.4509  loss_cate: 0.1152  loss_mask: 0.7242  loss_dice: 1.085  loss_ce_0: 0.8883  loss_cate_0: 0  loss_mask_0: 0.6771  loss_dice_0: 0.9364  loss_ce_1: 0.5586  loss_cate_1: 0  loss_mask_1: 0.7575  loss_dice_1: 0.9979  loss_ce_2: 0.5819  loss_cate_2: 0  loss_mask_2: 0.7629  loss_dice_2: 0.9416  loss_ce_3: 0.5517  loss_cate_3: 0  loss_mask_3: 0.7152  loss_dice_3: 0.8896  loss_ce_4: 0.5371  loss_cate_4: 0  loss_mask_4: 0.7263  loss_dice_4: 0.9072  loss_ce_5: 0.523  loss_cate_5: 0  loss_mask_5: 0.7047  loss_dice_5: 0.9178  loss_ce_6: 0.5065  loss_cate_6: 0  loss_mask_6: 0.7213  loss_dice_6: 1.014  loss_ce_7: 0.495  loss_cate_7: 0  loss_mask_7: 0.7479  loss_dice_7: 1.041  loss_ce_8: 0.4975  loss_cate_8: 0  loss_mask_8: 0.7214  loss_dice_8: 1.029  time: 1.3904  data_time: 0.0113  lr: 9.3052e-06  max_mem: 23815M
[01/01 11:44:34] d2.utils.events INFO:  eta: 13:50:17  iter: 3019  total_loss: 22.77  loss_ce: 0.6043  loss_cate: 0.1213  loss_mask: 0.6272  loss_dice: 0.9487  loss_ce_0: 0.8861  loss_cate_0: 0  loss_mask_0: 0.6053  loss_dice_0: 1.072  loss_ce_1: 0.6509  loss_cate_1: 0  loss_mask_1: 0.6552  loss_dice_1: 1.059  loss_ce_2: 0.5967  loss_cate_2: 0  loss_mask_2: 0.6677  loss_dice_2: 1.021  loss_ce_3: 0.5952  loss_cate_3: 0  loss_mask_3: 0.665  loss_dice_3: 0.9989  loss_ce_4: 0.5895  loss_cate_4: 0  loss_mask_4: 0.6374  loss_dice_4: 1.011  loss_ce_5: 0.6293  loss_cate_5: 0  loss_mask_5: 0.6552  loss_dice_5: 0.932  loss_ce_6: 0.655  loss_cate_6: 0  loss_mask_6: 0.599  loss_dice_6: 0.9671  loss_ce_7: 0.6276  loss_cate_7: 0  loss_mask_7: 0.6028  loss_dice_7: 0.9962  loss_ce_8: 0.6438  loss_cate_8: 0  loss_mask_8: 0.5895  loss_dice_8: 0.9839  time: 1.3904  data_time: 0.0125  lr: 9.3005e-06  max_mem: 23815M
[01/01 11:45:02] d2.utils.events INFO:  eta: 13:49:41  iter: 3039  total_loss: 23.16  loss_ce: 0.6307  loss_cate: 0.1464  loss_mask: 0.6504  loss_dice: 0.9272  loss_ce_0: 0.9398  loss_cate_0: 0  loss_mask_0: 0.6672  loss_dice_0: 1.034  loss_ce_1: 0.5892  loss_cate_1: 0  loss_mask_1: 0.6529  loss_dice_1: 1.043  loss_ce_2: 0.4933  loss_cate_2: 0  loss_mask_2: 0.6329  loss_dice_2: 1.001  loss_ce_3: 0.5455  loss_cate_3: 0  loss_mask_3: 0.6414  loss_dice_3: 0.9519  loss_ce_4: 0.6337  loss_cate_4: 0  loss_mask_4: 0.5856  loss_dice_4: 0.9826  loss_ce_5: 0.5246  loss_cate_5: 0  loss_mask_5: 0.5978  loss_dice_5: 0.9561  loss_ce_6: 0.5405  loss_cate_6: 0  loss_mask_6: 0.5606  loss_dice_6: 0.9764  loss_ce_7: 0.5411  loss_cate_7: 0  loss_mask_7: 0.5383  loss_dice_7: 1.003  loss_ce_8: 0.5532  loss_cate_8: 0  loss_mask_8: 0.5893  loss_dice_8: 0.9248  time: 1.3903  data_time: 0.0139  lr: 9.2959e-06  max_mem: 23815M
[01/01 11:45:30] d2.utils.events INFO:  eta: 13:49:05  iter: 3059  total_loss: 21.68  loss_ce: 0.5431  loss_cate: 0.1375  loss_mask: 0.5002  loss_dice: 1.007  loss_ce_0: 0.9848  loss_cate_0: 0  loss_mask_0: 0.5831  loss_dice_0: 0.9981  loss_ce_1: 0.6091  loss_cate_1: 0  loss_mask_1: 0.4836  loss_dice_1: 1.026  loss_ce_2: 0.6251  loss_cate_2: 0  loss_mask_2: 0.5053  loss_dice_2: 0.9646  loss_ce_3: 0.5638  loss_cate_3: 0  loss_mask_3: 0.5085  loss_dice_3: 0.9292  loss_ce_4: 0.6015  loss_cate_4: 0  loss_mask_4: 0.4962  loss_dice_4: 0.9212  loss_ce_5: 0.5695  loss_cate_5: 0  loss_mask_5: 0.5005  loss_dice_5: 0.9833  loss_ce_6: 0.5345  loss_cate_6: 0  loss_mask_6: 0.495  loss_dice_6: 0.9892  loss_ce_7: 0.5296  loss_cate_7: 0  loss_mask_7: 0.469  loss_dice_7: 1.008  loss_ce_8: 0.6029  loss_cate_8: 0  loss_mask_8: 0.5229  loss_dice_8: 1.043  time: 1.3904  data_time: 0.0111  lr: 9.2912e-06  max_mem: 23815M
[01/01 11:45:58] d2.utils.events INFO:  eta: 13:48:27  iter: 3079  total_loss: 19.82  loss_ce: 0.5312  loss_cate: 0.1196  loss_mask: 0.5321  loss_dice: 0.7126  loss_ce_0: 0.857  loss_cate_0: 0  loss_mask_0: 0.5144  loss_dice_0: 0.899  loss_ce_1: 0.5128  loss_cate_1: 0  loss_mask_1: 0.5466  loss_dice_1: 0.7812  loss_ce_2: 0.5613  loss_cate_2: 0  loss_mask_2: 0.5164  loss_dice_2: 0.7785  loss_ce_3: 0.5211  loss_cate_3: 0  loss_mask_3: 0.5256  loss_dice_3: 0.7497  loss_ce_4: 0.526  loss_cate_4: 0  loss_mask_4: 0.5382  loss_dice_4: 0.8068  loss_ce_5: 0.547  loss_cate_5: 0  loss_mask_5: 0.5656  loss_dice_5: 0.7873  loss_ce_6: 0.6586  loss_cate_6: 0  loss_mask_6: 0.5633  loss_dice_6: 0.698  loss_ce_7: 0.6761  loss_cate_7: 0  loss_mask_7: 0.5259  loss_dice_7: 0.7362  loss_ce_8: 0.5746  loss_cate_8: 0  loss_mask_8: 0.4939  loss_dice_8: 0.6977  time: 1.3903  data_time: 0.0143  lr: 9.2866e-06  max_mem: 23815M
[01/01 11:46:26] d2.utils.events INFO:  eta: 13:47:52  iter: 3099  total_loss: 19.74  loss_ce: 0.5025  loss_cate: 0.1256  loss_mask: 0.5355  loss_dice: 0.8602  loss_ce_0: 0.7772  loss_cate_0: 0  loss_mask_0: 0.5625  loss_dice_0: 0.8883  loss_ce_1: 0.4537  loss_cate_1: 0  loss_mask_1: 0.5926  loss_dice_1: 0.7847  loss_ce_2: 0.4358  loss_cate_2: 0  loss_mask_2: 0.5555  loss_dice_2: 0.9184  loss_ce_3: 0.4113  loss_cate_3: 0  loss_mask_3: 0.5656  loss_dice_3: 0.8907  loss_ce_4: 0.3928  loss_cate_4: 0  loss_mask_4: 0.5904  loss_dice_4: 0.9001  loss_ce_5: 0.508  loss_cate_5: 0  loss_mask_5: 0.5771  loss_dice_5: 0.8292  loss_ce_6: 0.398  loss_cate_6: 0  loss_mask_6: 0.6065  loss_dice_6: 0.842  loss_ce_7: 0.4327  loss_cate_7: 0  loss_mask_7: 0.5956  loss_dice_7: 0.8449  loss_ce_8: 0.4943  loss_cate_8: 0  loss_mask_8: 0.5867  loss_dice_8: 0.8285  time: 1.3903  data_time: 0.0118  lr: 9.2819e-06  max_mem: 23815M
[01/01 11:46:54] d2.utils.events INFO:  eta: 13:47:17  iter: 3119  total_loss: 17.81  loss_ce: 0.434  loss_cate: 0.1629  loss_mask: 0.4843  loss_dice: 0.7552  loss_ce_0: 0.7806  loss_cate_0: 0  loss_mask_0: 0.5088  loss_dice_0: 0.8063  loss_ce_1: 0.5014  loss_cate_1: 0  loss_mask_1: 0.4798  loss_dice_1: 0.8317  loss_ce_2: 0.4374  loss_cate_2: 0  loss_mask_2: 0.472  loss_dice_2: 0.8039  loss_ce_3: 0.4974  loss_cate_3: 0  loss_mask_3: 0.4514  loss_dice_3: 0.7494  loss_ce_4: 0.4599  loss_cate_4: 0  loss_mask_4: 0.4633  loss_dice_4: 0.7418  loss_ce_5: 0.4155  loss_cate_5: 0  loss_mask_5: 0.4409  loss_dice_5: 0.7836  loss_ce_6: 0.4531  loss_cate_6: 0  loss_mask_6: 0.4763  loss_dice_6: 0.7407  loss_ce_7: 0.466  loss_cate_7: 0  loss_mask_7: 0.4388  loss_dice_7: 0.7639  loss_ce_8: 0.4262  loss_cate_8: 0  loss_mask_8: 0.4894  loss_dice_8: 0.7496  time: 1.3904  data_time: 0.0128  lr: 9.2773e-06  max_mem: 23815M
[01/01 11:47:22] d2.utils.events INFO:  eta: 13:46:43  iter: 3139  total_loss: 24.09  loss_ce: 0.5828  loss_cate: 0.1183  loss_mask: 0.5526  loss_dice: 0.9673  loss_ce_0: 0.9017  loss_cate_0: 0  loss_mask_0: 0.5761  loss_dice_0: 1.022  loss_ce_1: 0.5927  loss_cate_1: 0  loss_mask_1: 0.5673  loss_dice_1: 0.9465  loss_ce_2: 0.6557  loss_cate_2: 0  loss_mask_2: 0.5691  loss_dice_2: 0.9727  loss_ce_3: 0.6637  loss_cate_3: 0  loss_mask_3: 0.5782  loss_dice_3: 0.8889  loss_ce_4: 0.5588  loss_cate_4: 0  loss_mask_4: 0.5702  loss_dice_4: 0.963  loss_ce_5: 0.608  loss_cate_5: 0  loss_mask_5: 0.5517  loss_dice_5: 0.9999  loss_ce_6: 0.6023  loss_cate_6: 0  loss_mask_6: 0.5717  loss_dice_6: 0.9884  loss_ce_7: 0.5912  loss_cate_7: 0  loss_mask_7: 0.567  loss_dice_7: 1.021  loss_ce_8: 0.5951  loss_cate_8: 0  loss_mask_8: 0.5661  loss_dice_8: 0.9728  time: 1.3904  data_time: 0.0138  lr: 9.2726e-06  max_mem: 23815M
[01/01 11:47:49] d2.utils.events INFO:  eta: 13:46:04  iter: 3159  total_loss: 20.84  loss_ce: 0.484  loss_cate: 0.1777  loss_mask: 0.6342  loss_dice: 0.7812  loss_ce_0: 0.8873  loss_cate_0: 0  loss_mask_0: 0.659  loss_dice_0: 0.7378  loss_ce_1: 0.57  loss_cate_1: 0  loss_mask_1: 0.6278  loss_dice_1: 0.7685  loss_ce_2: 0.5162  loss_cate_2: 0  loss_mask_2: 0.6423  loss_dice_2: 0.7711  loss_ce_3: 0.5363  loss_cate_3: 0  loss_mask_3: 0.5834  loss_dice_3: 0.7713  loss_ce_4: 0.5177  loss_cate_4: 0  loss_mask_4: 0.5894  loss_dice_4: 0.8246  loss_ce_5: 0.516  loss_cate_5: 0  loss_mask_5: 0.5675  loss_dice_5: 0.8226  loss_ce_6: 0.5212  loss_cate_6: 0  loss_mask_6: 0.5859  loss_dice_6: 0.7858  loss_ce_7: 0.4977  loss_cate_7: 0  loss_mask_7: 0.6087  loss_dice_7: 0.7367  loss_ce_8: 0.5087  loss_cate_8: 0  loss_mask_8: 0.6126  loss_dice_8: 0.802  time: 1.3904  data_time: 0.0122  lr: 9.268e-06  max_mem: 23815M
[01/01 11:48:17] d2.utils.events INFO:  eta: 13:45:31  iter: 3179  total_loss: 22.92  loss_ce: 0.6293  loss_cate: 0.1486  loss_mask: 0.6865  loss_dice: 0.8647  loss_ce_0: 0.9266  loss_cate_0: 0  loss_mask_0: 0.6629  loss_dice_0: 0.8701  loss_ce_1: 0.6312  loss_cate_1: 0  loss_mask_1: 0.591  loss_dice_1: 0.9161  loss_ce_2: 0.6156  loss_cate_2: 0  loss_mask_2: 0.6586  loss_dice_2: 0.8963  loss_ce_3: 0.6014  loss_cate_3: 0  loss_mask_3: 0.6665  loss_dice_3: 0.9338  loss_ce_4: 0.6466  loss_cate_4: 0  loss_mask_4: 0.6231  loss_dice_4: 0.9363  loss_ce_5: 0.6054  loss_cate_5: 0  loss_mask_5: 0.6436  loss_dice_5: 0.8955  loss_ce_6: 0.6197  loss_cate_6: 0  loss_mask_6: 0.6785  loss_dice_6: 0.8737  loss_ce_7: 0.6124  loss_cate_7: 0  loss_mask_7: 0.6983  loss_dice_7: 0.8933  loss_ce_8: 0.6073  loss_cate_8: 0  loss_mask_8: 0.7133  loss_dice_8: 0.8815  time: 1.3903  data_time: 0.0134  lr: 9.2633e-06  max_mem: 23815M
[01/01 11:48:46] d2.utils.events INFO:  eta: 13:45:02  iter: 3199  total_loss: 23.36  loss_ce: 0.5858  loss_cate: 0.1587  loss_mask: 0.6903  loss_dice: 0.969  loss_ce_0: 1.028  loss_cate_0: 0  loss_mask_0: 0.7616  loss_dice_0: 0.8675  loss_ce_1: 0.5806  loss_cate_1: 0  loss_mask_1: 0.7181  loss_dice_1: 0.979  loss_ce_2: 0.5906  loss_cate_2: 0  loss_mask_2: 0.7865  loss_dice_2: 0.9595  loss_ce_3: 0.5829  loss_cate_3: 0  loss_mask_3: 0.7777  loss_dice_3: 1.007  loss_ce_4: 0.6939  loss_cate_4: 0  loss_mask_4: 0.7471  loss_dice_4: 1.046  loss_ce_5: 0.6376  loss_cate_5: 0  loss_mask_5: 0.7068  loss_dice_5: 1.045  loss_ce_6: 0.6611  loss_cate_6: 0  loss_mask_6: 0.7195  loss_dice_6: 0.9454  loss_ce_7: 0.6445  loss_cate_7: 0  loss_mask_7: 0.7036  loss_dice_7: 0.942  loss_ce_8: 0.6091  loss_cate_8: 0  loss_mask_8: 0.6883  loss_dice_8: 0.9887  time: 1.3905  data_time: 0.0141  lr: 9.2586e-06  max_mem: 23815M
[01/01 11:49:14] d2.utils.events INFO:  eta: 13:44:33  iter: 3219  total_loss: 21.21  loss_ce: 0.4972  loss_cate: 0.1542  loss_mask: 0.6308  loss_dice: 0.9095  loss_ce_0: 0.8479  loss_cate_0: 0  loss_mask_0: 0.6528  loss_dice_0: 0.9009  loss_ce_1: 0.596  loss_cate_1: 0  loss_mask_1: 0.624  loss_dice_1: 0.8617  loss_ce_2: 0.5396  loss_cate_2: 0  loss_mask_2: 0.6122  loss_dice_2: 0.9034  loss_ce_3: 0.513  loss_cate_3: 0  loss_mask_3: 0.6137  loss_dice_3: 0.9092  loss_ce_4: 0.4856  loss_cate_4: 0  loss_mask_4: 0.6112  loss_dice_4: 0.8966  loss_ce_5: 0.4535  loss_cate_5: 0  loss_mask_5: 0.6002  loss_dice_5: 0.8448  loss_ce_6: 0.4805  loss_cate_6: 0  loss_mask_6: 0.5867  loss_dice_6: 0.8491  loss_ce_7: 0.4876  loss_cate_7: 0  loss_mask_7: 0.63  loss_dice_7: 0.8444  loss_ce_8: 0.4818  loss_cate_8: 0  loss_mask_8: 0.6155  loss_dice_8: 0.8207  time: 1.3906  data_time: 0.0129  lr: 9.254e-06  max_mem: 23815M
[01/01 11:49:42] d2.utils.events INFO:  eta: 13:44:00  iter: 3239  total_loss: 24.64  loss_ce: 0.4647  loss_cate: 0.1603  loss_mask: 0.73  loss_dice: 0.8995  loss_ce_0: 0.9144  loss_cate_0: 0  loss_mask_0: 0.8005  loss_dice_0: 0.9999  loss_ce_1: 0.6153  loss_cate_1: 0  loss_mask_1: 0.7574  loss_dice_1: 0.9637  loss_ce_2: 0.5687  loss_cate_2: 0  loss_mask_2: 0.7715  loss_dice_2: 0.9143  loss_ce_3: 0.5037  loss_cate_3: 0  loss_mask_3: 0.7729  loss_dice_3: 0.8593  loss_ce_4: 0.545  loss_cate_4: 0  loss_mask_4: 0.7387  loss_dice_4: 0.8981  loss_ce_5: 0.4724  loss_cate_5: 0  loss_mask_5: 0.7606  loss_dice_5: 0.8801  loss_ce_6: 0.503  loss_cate_6: 0  loss_mask_6: 0.7834  loss_dice_6: 0.8705  loss_ce_7: 0.5278  loss_cate_7: 0  loss_mask_7: 0.7929  loss_dice_7: 0.8686  loss_ce_8: 0.4851  loss_cate_8: 0  loss_mask_8: 0.7265  loss_dice_8: 0.8956  time: 1.3906  data_time: 0.0112  lr: 9.2493e-06  max_mem: 23815M
[01/01 11:50:09] d2.utils.events INFO:  eta: 13:43:29  iter: 3259  total_loss: 17.29  loss_ce: 0.4236  loss_cate: 0.1579  loss_mask: 0.4741  loss_dice: 0.7262  loss_ce_0: 0.8168  loss_cate_0: 0  loss_mask_0: 0.4215  loss_dice_0: 0.7125  loss_ce_1: 0.5054  loss_cate_1: 0  loss_mask_1: 0.4446  loss_dice_1: 0.7336  loss_ce_2: 0.4072  loss_cate_2: 0  loss_mask_2: 0.5053  loss_dice_2: 0.7536  loss_ce_3: 0.474  loss_cate_3: 0  loss_mask_3: 0.478  loss_dice_3: 0.7064  loss_ce_4: 0.4006  loss_cate_4: 0  loss_mask_4: 0.5117  loss_dice_4: 0.7502  loss_ce_5: 0.4456  loss_cate_5: 0  loss_mask_5: 0.4921  loss_dice_5: 0.7377  loss_ce_6: 0.3951  loss_cate_6: 0  loss_mask_6: 0.4874  loss_dice_6: 0.7141  loss_ce_7: 0.3788  loss_cate_7: 0  loss_mask_7: 0.5066  loss_dice_7: 0.6944  loss_ce_8: 0.3772  loss_cate_8: 0  loss_mask_8: 0.4789  loss_dice_8: 0.7219  time: 1.3905  data_time: 0.0113  lr: 9.2447e-06  max_mem: 23815M
[01/01 11:50:37] d2.utils.events INFO:  eta: 13:43:07  iter: 3279  total_loss: 20.41  loss_ce: 0.5666  loss_cate: 0.1255  loss_mask: 0.7411  loss_dice: 0.6757  loss_ce_0: 0.8465  loss_cate_0: 0  loss_mask_0: 0.7729  loss_dice_0: 0.7241  loss_ce_1: 0.5468  loss_cate_1: 0  loss_mask_1: 0.7441  loss_dice_1: 0.697  loss_ce_2: 0.5508  loss_cate_2: 0  loss_mask_2: 0.7278  loss_dice_2: 0.6473  loss_ce_3: 0.5218  loss_cate_3: 0  loss_mask_3: 0.7566  loss_dice_3: 0.6643  loss_ce_4: 0.5329  loss_cate_4: 0  loss_mask_4: 0.7364  loss_dice_4: 0.6087  loss_ce_5: 0.5125  loss_cate_5: 0  loss_mask_5: 0.7608  loss_dice_5: 0.622  loss_ce_6: 0.5503  loss_cate_6: 0  loss_mask_6: 0.7529  loss_dice_6: 0.6072  loss_ce_7: 0.5258  loss_cate_7: 0  loss_mask_7: 0.7637  loss_dice_7: 0.6257  loss_ce_8: 0.5316  loss_cate_8: 0  loss_mask_8: 0.7228  loss_dice_8: 0.6426  time: 1.3906  data_time: 0.0113  lr: 9.24e-06  max_mem: 23815M
[01/01 11:51:05] d2.utils.events INFO:  eta: 13:42:44  iter: 3299  total_loss: 18.85  loss_ce: 0.439  loss_cate: 0.1105  loss_mask: 0.4225  loss_dice: 0.7949  loss_ce_0: 0.7532  loss_cate_0: 0  loss_mask_0: 0.4418  loss_dice_0: 0.7828  loss_ce_1: 0.5141  loss_cate_1: 0  loss_mask_1: 0.426  loss_dice_1: 0.8249  loss_ce_2: 0.5117  loss_cate_2: 0  loss_mask_2: 0.381  loss_dice_2: 0.7413  loss_ce_3: 0.483  loss_cate_3: 0  loss_mask_3: 0.3868  loss_dice_3: 0.7915  loss_ce_4: 0.468  loss_cate_4: 0  loss_mask_4: 0.4121  loss_dice_4: 0.7975  loss_ce_5: 0.4727  loss_cate_5: 0  loss_mask_5: 0.4459  loss_dice_5: 0.7565  loss_ce_6: 0.464  loss_cate_6: 0  loss_mask_6: 0.4222  loss_dice_6: 0.7122  loss_ce_7: 0.4649  loss_cate_7: 0  loss_mask_7: 0.4427  loss_dice_7: 0.8221  loss_ce_8: 0.4549  loss_cate_8: 0  loss_mask_8: 0.4372  loss_dice_8: 0.7738  time: 1.3906  data_time: 0.0128  lr: 9.2354e-06  max_mem: 23815M
[01/01 11:51:33] d2.utils.events INFO:  eta: 13:42:23  iter: 3319  total_loss: 21.63  loss_ce: 0.4817  loss_cate: 0.1499  loss_mask: 0.6531  loss_dice: 0.8804  loss_ce_0: 0.9173  loss_cate_0: 0  loss_mask_0: 0.5739  loss_dice_0: 0.7506  loss_ce_1: 0.61  loss_cate_1: 0  loss_mask_1: 0.581  loss_dice_1: 0.8792  loss_ce_2: 0.5708  loss_cate_2: 0  loss_mask_2: 0.6113  loss_dice_2: 0.8934  loss_ce_3: 0.5387  loss_cate_3: 0  loss_mask_3: 0.6634  loss_dice_3: 0.8903  loss_ce_4: 0.5396  loss_cate_4: 0  loss_mask_4: 0.6783  loss_dice_4: 0.8313  loss_ce_5: 0.5548  loss_cate_5: 0  loss_mask_5: 0.6843  loss_dice_5: 0.856  loss_ce_6: 0.5419  loss_cate_6: 0  loss_mask_6: 0.6781  loss_dice_6: 0.837  loss_ce_7: 0.4794  loss_cate_7: 0  loss_mask_7: 0.6053  loss_dice_7: 0.8644  loss_ce_8: 0.4986  loss_cate_8: 0  loss_mask_8: 0.6579  loss_dice_8: 0.8701  time: 1.3906  data_time: 0.0148  lr: 9.2307e-06  max_mem: 23815M
[01/01 11:52:01] d2.utils.events INFO:  eta: 13:41:54  iter: 3339  total_loss: 25.3  loss_ce: 0.5754  loss_cate: 0.1505  loss_mask: 0.681  loss_dice: 1.116  loss_ce_0: 0.92  loss_cate_0: 0  loss_mask_0: 0.5649  loss_dice_0: 1.074  loss_ce_1: 0.6246  loss_cate_1: 0  loss_mask_1: 0.5903  loss_dice_1: 1.119  loss_ce_2: 0.582  loss_cate_2: 0  loss_mask_2: 0.6149  loss_dice_2: 1.168  loss_ce_3: 0.5386  loss_cate_3: 0  loss_mask_3: 0.6343  loss_dice_3: 1.155  loss_ce_4: 0.6021  loss_cate_4: 0  loss_mask_4: 0.6252  loss_dice_4: 1.16  loss_ce_5: 0.5993  loss_cate_5: 0  loss_mask_5: 0.6393  loss_dice_5: 1.14  loss_ce_6: 0.5902  loss_cate_6: 0  loss_mask_6: 0.6376  loss_dice_6: 1.202  loss_ce_7: 0.5578  loss_cate_7: 0  loss_mask_7: 0.6431  loss_dice_7: 1.125  loss_ce_8: 0.5298  loss_cate_8: 0  loss_mask_8: 0.6431  loss_dice_8: 1.141  time: 1.3906  data_time: 0.0135  lr: 9.2261e-06  max_mem: 23815M
[01/01 11:52:29] d2.utils.events INFO:  eta: 13:41:37  iter: 3359  total_loss: 23  loss_ce: 0.5596  loss_cate: 0.152  loss_mask: 0.52  loss_dice: 0.9497  loss_ce_0: 0.8653  loss_cate_0: 0  loss_mask_0: 0.5557  loss_dice_0: 0.9338  loss_ce_1: 0.6752  loss_cate_1: 0  loss_mask_1: 0.5462  loss_dice_1: 0.9688  loss_ce_2: 0.5691  loss_cate_2: 0  loss_mask_2: 0.5799  loss_dice_2: 0.9773  loss_ce_3: 0.5493  loss_cate_3: 0  loss_mask_3: 0.5106  loss_dice_3: 0.9525  loss_ce_4: 0.5058  loss_cate_4: 0  loss_mask_4: 0.4948  loss_dice_4: 0.9381  loss_ce_5: 0.5049  loss_cate_5: 0  loss_mask_5: 0.525  loss_dice_5: 0.9179  loss_ce_6: 0.5283  loss_cate_6: 0  loss_mask_6: 0.5162  loss_dice_6: 0.9631  loss_ce_7: 0.5397  loss_cate_7: 0  loss_mask_7: 0.514  loss_dice_7: 0.9514  loss_ce_8: 0.5228  loss_cate_8: 0  loss_mask_8: 0.5269  loss_dice_8: 0.9192  time: 1.3906  data_time: 0.0134  lr: 9.2214e-06  max_mem: 23815M
[01/01 11:52:57] d2.utils.events INFO:  eta: 13:41:11  iter: 3379  total_loss: 22.75  loss_ce: 0.584  loss_cate: 0.1202  loss_mask: 0.5928  loss_dice: 0.8864  loss_ce_0: 0.8921  loss_cate_0: 0  loss_mask_0: 0.5718  loss_dice_0: 0.9252  loss_ce_1: 0.6891  loss_cate_1: 0  loss_mask_1: 0.5091  loss_dice_1: 0.8686  loss_ce_2: 0.6974  loss_cate_2: 0  loss_mask_2: 0.5445  loss_dice_2: 0.8895  loss_ce_3: 0.5722  loss_cate_3: 0  loss_mask_3: 0.5605  loss_dice_3: 0.93  loss_ce_4: 0.6267  loss_cate_4: 0  loss_mask_4: 0.5417  loss_dice_4: 0.8555  loss_ce_5: 0.5503  loss_cate_5: 0  loss_mask_5: 0.5457  loss_dice_5: 0.896  loss_ce_6: 0.637  loss_cate_6: 0  loss_mask_6: 0.58  loss_dice_6: 0.894  loss_ce_7: 0.5329  loss_cate_7: 0  loss_mask_7: 0.5581  loss_dice_7: 0.8831  loss_ce_8: 0.6337  loss_cate_8: 0  loss_mask_8: 0.5682  loss_dice_8: 0.8381  time: 1.3906  data_time: 0.0142  lr: 9.2167e-06  max_mem: 23815M
[01/01 11:53:24] d2.utils.events INFO:  eta: 13:40:32  iter: 3399  total_loss: 14.22  loss_ce: 0.4558  loss_cate: 0.1276  loss_mask: 0.3952  loss_dice: 0.502  loss_ce_0: 0.785  loss_cate_0: 0  loss_mask_0: 0.4159  loss_dice_0: 0.5067  loss_ce_1: 0.5246  loss_cate_1: 0  loss_mask_1: 0.3967  loss_dice_1: 0.545  loss_ce_2: 0.437  loss_cate_2: 0  loss_mask_2: 0.4007  loss_dice_2: 0.5311  loss_ce_3: 0.4802  loss_cate_3: 0  loss_mask_3: 0.4329  loss_dice_3: 0.5264  loss_ce_4: 0.4728  loss_cate_4: 0  loss_mask_4: 0.4161  loss_dice_4: 0.5235  loss_ce_5: 0.5011  loss_cate_5: 0  loss_mask_5: 0.4035  loss_dice_5: 0.5096  loss_ce_6: 0.4554  loss_cate_6: 0  loss_mask_6: 0.4362  loss_dice_6: 0.5096  loss_ce_7: 0.4699  loss_cate_7: 0  loss_mask_7: 0.4165  loss_dice_7: 0.5455  loss_ce_8: 0.4288  loss_cate_8: 0  loss_mask_8: 0.3918  loss_dice_8: 0.5197  time: 1.3905  data_time: 0.0113  lr: 9.2121e-06  max_mem: 23815M
[01/01 11:53:52] d2.utils.events INFO:  eta: 13:39:58  iter: 3419  total_loss: 23.15  loss_ce: 0.5913  loss_cate: 0.124  loss_mask: 0.6031  loss_dice: 0.9115  loss_ce_0: 0.943  loss_cate_0: 0  loss_mask_0: 0.5876  loss_dice_0: 0.9512  loss_ce_1: 0.6846  loss_cate_1: 0  loss_mask_1: 0.6227  loss_dice_1: 0.9121  loss_ce_2: 0.6008  loss_cate_2: 0  loss_mask_2: 0.6085  loss_dice_2: 0.9041  loss_ce_3: 0.6248  loss_cate_3: 0  loss_mask_3: 0.6314  loss_dice_3: 0.9371  loss_ce_4: 0.6045  loss_cate_4: 0  loss_mask_4: 0.5942  loss_dice_4: 0.9095  loss_ce_5: 0.5678  loss_cate_5: 0  loss_mask_5: 0.6199  loss_dice_5: 0.8906  loss_ce_6: 0.618  loss_cate_6: 0  loss_mask_6: 0.5959  loss_dice_6: 0.8631  loss_ce_7: 0.609  loss_cate_7: 0  loss_mask_7: 0.5898  loss_dice_7: 0.9172  loss_ce_8: 0.6173  loss_cate_8: 0  loss_mask_8: 0.5992  loss_dice_8: 0.9082  time: 1.3905  data_time: 0.0110  lr: 9.2074e-06  max_mem: 23815M
[01/01 11:54:20] d2.utils.events INFO:  eta: 13:39:33  iter: 3439  total_loss: 20.46  loss_ce: 0.4981  loss_cate: 0.1571  loss_mask: 0.5846  loss_dice: 0.9274  loss_ce_0: 0.8805  loss_cate_0: 0  loss_mask_0: 0.5895  loss_dice_0: 0.832  loss_ce_1: 0.6067  loss_cate_1: 0  loss_mask_1: 0.5348  loss_dice_1: 0.8766  loss_ce_2: 0.4703  loss_cate_2: 0  loss_mask_2: 0.5491  loss_dice_2: 0.8591  loss_ce_3: 0.5128  loss_cate_3: 0  loss_mask_3: 0.5464  loss_dice_3: 0.8812  loss_ce_4: 0.5397  loss_cate_4: 0  loss_mask_4: 0.5601  loss_dice_4: 0.8771  loss_ce_5: 0.5038  loss_cate_5: 0  loss_mask_5: 0.5749  loss_dice_5: 0.846  loss_ce_6: 0.4908  loss_cate_6: 0  loss_mask_6: 0.5724  loss_dice_6: 0.8733  loss_ce_7: 0.5097  loss_cate_7: 0  loss_mask_7: 0.5799  loss_dice_7: 0.8971  loss_ce_8: 0.5124  loss_cate_8: 0  loss_mask_8: 0.5645  loss_dice_8: 0.8578  time: 1.3904  data_time: 0.0140  lr: 9.2028e-06  max_mem: 23815M
[01/01 11:54:48] d2.utils.events INFO:  eta: 13:39:09  iter: 3459  total_loss: 18.66  loss_ce: 0.3968  loss_cate: 0.1243  loss_mask: 0.6316  loss_dice: 0.6457  loss_ce_0: 0.7488  loss_cate_0: 0  loss_mask_0: 0.6005  loss_dice_0: 0.6853  loss_ce_1: 0.446  loss_cate_1: 0  loss_mask_1: 0.6333  loss_dice_1: 0.6544  loss_ce_2: 0.3874  loss_cate_2: 0  loss_mask_2: 0.6411  loss_dice_2: 0.6471  loss_ce_3: 0.3896  loss_cate_3: 0  loss_mask_3: 0.6488  loss_dice_3: 0.6481  loss_ce_4: 0.3537  loss_cate_4: 0  loss_mask_4: 0.6334  loss_dice_4: 0.6449  loss_ce_5: 0.3416  loss_cate_5: 0  loss_mask_5: 0.6092  loss_dice_5: 0.6507  loss_ce_6: 0.3374  loss_cate_6: 0  loss_mask_6: 0.6288  loss_dice_6: 0.6414  loss_ce_7: 0.3411  loss_cate_7: 0  loss_mask_7: 0.6506  loss_dice_7: 0.6845  loss_ce_8: 0.3812  loss_cate_8: 0  loss_mask_8: 0.6561  loss_dice_8: 0.6604  time: 1.3905  data_time: 0.0144  lr: 9.1981e-06  max_mem: 23815M
[01/01 11:55:16] d2.utils.events INFO:  eta: 13:38:48  iter: 3479  total_loss: 16.25  loss_ce: 0.4307  loss_cate: 0.1197  loss_mask: 0.4668  loss_dice: 0.6953  loss_ce_0: 0.7374  loss_cate_0: 0  loss_mask_0: 0.4729  loss_dice_0: 0.6578  loss_ce_1: 0.5043  loss_cate_1: 0  loss_mask_1: 0.4827  loss_dice_1: 0.7024  loss_ce_2: 0.4223  loss_cate_2: 0  loss_mask_2: 0.4734  loss_dice_2: 0.6605  loss_ce_3: 0.4  loss_cate_3: 0  loss_mask_3: 0.5002  loss_dice_3: 0.6537  loss_ce_4: 0.4145  loss_cate_4: 0  loss_mask_4: 0.4895  loss_dice_4: 0.6538  loss_ce_5: 0.3908  loss_cate_5: 0  loss_mask_5: 0.4844  loss_dice_5: 0.6817  loss_ce_6: 0.4508  loss_cate_6: 0  loss_mask_6: 0.4698  loss_dice_6: 0.6547  loss_ce_7: 0.3858  loss_cate_7: 0  loss_mask_7: 0.4727  loss_dice_7: 0.6743  loss_ce_8: 0.48  loss_cate_8: 0  loss_mask_8: 0.4647  loss_dice_8: 0.689  time: 1.3905  data_time: 0.0141  lr: 9.1935e-06  max_mem: 23815M
[01/01 11:55:44] d2.utils.events INFO:  eta: 13:38:27  iter: 3499  total_loss: 22.45  loss_ce: 0.5645  loss_cate: 0.147  loss_mask: 0.6208  loss_dice: 0.9228  loss_ce_0: 1.019  loss_cate_0: 0  loss_mask_0: 0.5848  loss_dice_0: 0.9518  loss_ce_1: 0.6899  loss_cate_1: 0  loss_mask_1: 0.5627  loss_dice_1: 0.9379  loss_ce_2: 0.5476  loss_cate_2: 0  loss_mask_2: 0.6033  loss_dice_2: 0.9924  loss_ce_3: 0.595  loss_cate_3: 0  loss_mask_3: 0.5949  loss_dice_3: 0.9607  loss_ce_4: 0.5522  loss_cate_4: 0  loss_mask_4: 0.6164  loss_dice_4: 0.9451  loss_ce_5: 0.55  loss_cate_5: 0  loss_mask_5: 0.6357  loss_dice_5: 0.9099  loss_ce_6: 0.5024  loss_cate_6: 0  loss_mask_6: 0.5968  loss_dice_6: 0.9261  loss_ce_7: 0.5401  loss_cate_7: 0  loss_mask_7: 0.6126  loss_dice_7: 0.9285  loss_ce_8: 0.5182  loss_cate_8: 0  loss_mask_8: 0.6572  loss_dice_8: 0.9101  time: 1.3906  data_time: 0.0134  lr: 9.1888e-06  max_mem: 23815M
[01/01 11:56:12] d2.utils.events INFO:  eta: 13:37:59  iter: 3519  total_loss: 19.28  loss_ce: 0.5166  loss_cate: 0.1292  loss_mask: 0.6255  loss_dice: 0.7879  loss_ce_0: 0.8024  loss_cate_0: 0  loss_mask_0: 0.6265  loss_dice_0: 0.8722  loss_ce_1: 0.6329  loss_cate_1: 0  loss_mask_1: 0.6147  loss_dice_1: 0.764  loss_ce_2: 0.5635  loss_cate_2: 0  loss_mask_2: 0.6131  loss_dice_2: 0.8174  loss_ce_3: 0.5535  loss_cate_3: 0  loss_mask_3: 0.5901  loss_dice_3: 0.7427  loss_ce_4: 0.4992  loss_cate_4: 0  loss_mask_4: 0.6539  loss_dice_4: 0.8473  loss_ce_5: 0.5598  loss_cate_5: 0  loss_mask_5: 0.6292  loss_dice_5: 0.6964  loss_ce_6: 0.5175  loss_cate_6: 0  loss_mask_6: 0.6128  loss_dice_6: 0.7424  loss_ce_7: 0.4383  loss_cate_7: 0  loss_mask_7: 0.6092  loss_dice_7: 0.9075  loss_ce_8: 0.4943  loss_cate_8: 0  loss_mask_8: 0.6381  loss_dice_8: 0.777  time: 1.3907  data_time: 0.0139  lr: 9.1841e-06  max_mem: 23815M
[01/01 11:56:40] d2.utils.events INFO:  eta: 13:37:32  iter: 3539  total_loss: 23.42  loss_ce: 0.6425  loss_cate: 0.1684  loss_mask: 0.6272  loss_dice: 0.92  loss_ce_0: 0.9748  loss_cate_0: 0  loss_mask_0: 0.6766  loss_dice_0: 0.9875  loss_ce_1: 0.6134  loss_cate_1: 0  loss_mask_1: 0.6908  loss_dice_1: 1.053  loss_ce_2: 0.5585  loss_cate_2: 0  loss_mask_2: 0.6499  loss_dice_2: 0.9559  loss_ce_3: 0.6008  loss_cate_3: 0  loss_mask_3: 0.7085  loss_dice_3: 0.9693  loss_ce_4: 0.5843  loss_cate_4: 0  loss_mask_4: 0.651  loss_dice_4: 0.951  loss_ce_5: 0.4745  loss_cate_5: 0  loss_mask_5: 0.7319  loss_dice_5: 0.9861  loss_ce_6: 0.5768  loss_cate_6: 0  loss_mask_6: 0.6984  loss_dice_6: 0.979  loss_ce_7: 0.5679  loss_cate_7: 0  loss_mask_7: 0.5776  loss_dice_7: 0.9904  loss_ce_8: 0.5774  loss_cate_8: 0  loss_mask_8: 0.6133  loss_dice_8: 0.9527  time: 1.3907  data_time: 0.0127  lr: 9.1795e-06  max_mem: 23815M
[01/01 11:57:08] d2.utils.events INFO:  eta: 13:37:02  iter: 3559  total_loss: 23.47  loss_ce: 0.566  loss_cate: 0.1663  loss_mask: 0.6628  loss_dice: 1.043  loss_ce_0: 0.9651  loss_cate_0: 0  loss_mask_0: 0.5605  loss_dice_0: 0.9821  loss_ce_1: 0.6565  loss_cate_1: 0  loss_mask_1: 0.5847  loss_dice_1: 0.961  loss_ce_2: 0.6231  loss_cate_2: 0  loss_mask_2: 0.6364  loss_dice_2: 0.9835  loss_ce_3: 0.5251  loss_cate_3: 0  loss_mask_3: 0.6376  loss_dice_3: 0.995  loss_ce_4: 0.5208  loss_cate_4: 0  loss_mask_4: 0.6614  loss_dice_4: 0.9602  loss_ce_5: 0.5955  loss_cate_5: 0  loss_mask_5: 0.636  loss_dice_5: 1.005  loss_ce_6: 0.551  loss_cate_6: 0  loss_mask_6: 0.6623  loss_dice_6: 1.025  loss_ce_7: 0.5512  loss_cate_7: 0  loss_mask_7: 0.6197  loss_dice_7: 1.026  loss_ce_8: 0.6119  loss_cate_8: 0  loss_mask_8: 0.6572  loss_dice_8: 0.9751  time: 1.3907  data_time: 0.0118  lr: 9.1748e-06  max_mem: 23815M
[01/01 11:57:36] d2.utils.events INFO:  eta: 13:36:28  iter: 3579  total_loss: 22.63  loss_ce: 0.48  loss_cate: 0.1168  loss_mask: 0.7317  loss_dice: 0.951  loss_ce_0: 0.7671  loss_cate_0: 0  loss_mask_0: 0.6622  loss_dice_0: 0.9761  loss_ce_1: 0.4674  loss_cate_1: 0  loss_mask_1: 0.7029  loss_dice_1: 0.9151  loss_ce_2: 0.4669  loss_cate_2: 0  loss_mask_2: 0.6646  loss_dice_2: 0.9377  loss_ce_3: 0.5389  loss_cate_3: 0  loss_mask_3: 0.6458  loss_dice_3: 0.9064  loss_ce_4: 0.4891  loss_cate_4: 0  loss_mask_4: 0.7686  loss_dice_4: 0.8736  loss_ce_5: 0.5321  loss_cate_5: 0  loss_mask_5: 0.7869  loss_dice_5: 0.8962  loss_ce_6: 0.4798  loss_cate_6: 0  loss_mask_6: 0.7287  loss_dice_6: 0.9167  loss_ce_7: 0.4651  loss_cate_7: 0  loss_mask_7: 0.8116  loss_dice_7: 0.9699  loss_ce_8: 0.4709  loss_cate_8: 0  loss_mask_8: 0.7614  loss_dice_8: 0.9611  time: 1.3907  data_time: 0.0116  lr: 9.1702e-06  max_mem: 23815M
[01/01 11:58:03] d2.utils.events INFO:  eta: 13:35:56  iter: 3599  total_loss: 22.6  loss_ce: 0.5973  loss_cate: 0.1352  loss_mask: 0.5864  loss_dice: 0.9365  loss_ce_0: 0.8463  loss_cate_0: 0  loss_mask_0: 0.5714  loss_dice_0: 0.9533  loss_ce_1: 0.5826  loss_cate_1: 0  loss_mask_1: 0.6331  loss_dice_1: 0.9227  loss_ce_2: 0.5929  loss_cate_2: 0  loss_mask_2: 0.6039  loss_dice_2: 0.9852  loss_ce_3: 0.5725  loss_cate_3: 0  loss_mask_3: 0.5694  loss_dice_3: 0.9626  loss_ce_4: 0.5292  loss_cate_4: 0  loss_mask_4: 0.6296  loss_dice_4: 0.9549  loss_ce_5: 0.5965  loss_cate_5: 0  loss_mask_5: 0.5734  loss_dice_5: 0.9173  loss_ce_6: 0.6101  loss_cate_6: 0  loss_mask_6: 0.6164  loss_dice_6: 0.9497  loss_ce_7: 0.5393  loss_cate_7: 0  loss_mask_7: 0.6371  loss_dice_7: 0.9348  loss_ce_8: 0.5368  loss_cate_8: 0  loss_mask_8: 0.6041  loss_dice_8: 0.9082  time: 1.3906  data_time: 0.0125  lr: 9.1655e-06  max_mem: 23815M
[01/01 11:58:31] d2.utils.events INFO:  eta: 13:35:28  iter: 3619  total_loss: 20.95  loss_ce: 0.4506  loss_cate: 0.1334  loss_mask: 0.4886  loss_dice: 0.871  loss_ce_0: 0.8712  loss_cate_0: 0  loss_mask_0: 0.4872  loss_dice_0: 0.9186  loss_ce_1: 0.4659  loss_cate_1: 0  loss_mask_1: 0.4963  loss_dice_1: 0.8865  loss_ce_2: 0.5384  loss_cate_2: 0  loss_mask_2: 0.4852  loss_dice_2: 0.8568  loss_ce_3: 0.5052  loss_cate_3: 0  loss_mask_3: 0.5145  loss_dice_3: 0.9126  loss_ce_4: 0.4457  loss_cate_4: 0  loss_mask_4: 0.5382  loss_dice_4: 0.9002  loss_ce_5: 0.5027  loss_cate_5: 0  loss_mask_5: 0.5056  loss_dice_5: 0.8759  loss_ce_6: 0.5031  loss_cate_6: 0  loss_mask_6: 0.4563  loss_dice_6: 0.8125  loss_ce_7: 0.4463  loss_cate_7: 0  loss_mask_7: 0.5022  loss_dice_7: 0.8557  loss_ce_8: 0.4561  loss_cate_8: 0  loss_mask_8: 0.4858  loss_dice_8: 0.868  time: 1.3906  data_time: 0.0142  lr: 9.1608e-06  max_mem: 23815M
[01/01 11:58:59] d2.utils.events INFO:  eta: 13:34:57  iter: 3639  total_loss: 22.34  loss_ce: 0.4047  loss_cate: 0.1738  loss_mask: 0.545  loss_dice: 0.9118  loss_ce_0: 0.8592  loss_cate_0: 0  loss_mask_0: 0.6091  loss_dice_0: 0.8142  loss_ce_1: 0.4622  loss_cate_1: 0  loss_mask_1: 0.6309  loss_dice_1: 0.8873  loss_ce_2: 0.5158  loss_cate_2: 0  loss_mask_2: 0.5984  loss_dice_2: 0.8363  loss_ce_3: 0.5163  loss_cate_3: 0  loss_mask_3: 0.592  loss_dice_3: 0.824  loss_ce_4: 0.4431  loss_cate_4: 0  loss_mask_4: 0.6114  loss_dice_4: 0.8131  loss_ce_5: 0.5232  loss_cate_5: 0  loss_mask_5: 0.5875  loss_dice_5: 0.797  loss_ce_6: 0.513  loss_cate_6: 0  loss_mask_6: 0.5762  loss_dice_6: 0.7826  loss_ce_7: 0.4513  loss_cate_7: 0  loss_mask_7: 0.6339  loss_dice_7: 0.85  loss_ce_8: 0.4415  loss_cate_8: 0  loss_mask_8: 0.5781  loss_dice_8: 0.9765  time: 1.3906  data_time: 0.0113  lr: 9.1562e-06  max_mem: 23815M
[01/01 11:59:27] d2.utils.events INFO:  eta: 13:34:27  iter: 3659  total_loss: 23.46  loss_ce: 0.3352  loss_cate: 0.1468  loss_mask: 0.7742  loss_dice: 0.8479  loss_ce_0: 0.736  loss_cate_0: 0  loss_mask_0: 0.7476  loss_dice_0: 0.8275  loss_ce_1: 0.4113  loss_cate_1: 0  loss_mask_1: 0.7337  loss_dice_1: 0.7761  loss_ce_2: 0.4179  loss_cate_2: 0  loss_mask_2: 0.739  loss_dice_2: 0.8068  loss_ce_3: 0.3492  loss_cate_3: 0  loss_mask_3: 0.7205  loss_dice_3: 0.8081  loss_ce_4: 0.3435  loss_cate_4: 0  loss_mask_4: 0.732  loss_dice_4: 0.8251  loss_ce_5: 0.3671  loss_cate_5: 0  loss_mask_5: 0.7553  loss_dice_5: 0.7673  loss_ce_6: 0.3458  loss_cate_6: 0  loss_mask_6: 0.7464  loss_dice_6: 0.8213  loss_ce_7: 0.3228  loss_cate_7: 0  loss_mask_7: 0.7302  loss_dice_7: 0.8116  loss_ce_8: 0.3404  loss_cate_8: 0  loss_mask_8: 0.7433  loss_dice_8: 0.7896  time: 1.3906  data_time: 0.0125  lr: 9.1515e-06  max_mem: 23815M
[01/01 11:59:54] d2.utils.events INFO:  eta: 13:33:58  iter: 3679  total_loss: 15.44  loss_ce: 0.3479  loss_cate: 0.09777  loss_mask: 0.4669  loss_dice: 0.602  loss_ce_0: 0.6912  loss_cate_0: 0  loss_mask_0: 0.4494  loss_dice_0: 0.6381  loss_ce_1: 0.3756  loss_cate_1: 0  loss_mask_1: 0.4586  loss_dice_1: 0.6556  loss_ce_2: 0.3651  loss_cate_2: 0  loss_mask_2: 0.4481  loss_dice_2: 0.6189  loss_ce_3: 0.3154  loss_cate_3: 0  loss_mask_3: 0.4549  loss_dice_3: 0.581  loss_ce_4: 0.3282  loss_cate_4: 0  loss_mask_4: 0.4604  loss_dice_4: 0.6072  loss_ce_5: 0.4178  loss_cate_5: 0  loss_mask_5: 0.4531  loss_dice_5: 0.6079  loss_ce_6: 0.3118  loss_cate_6: 0  loss_mask_6: 0.4701  loss_dice_6: 0.6605  loss_ce_7: 0.3572  loss_cate_7: 0  loss_mask_7: 0.4495  loss_dice_7: 0.5901  loss_ce_8: 0.3426  loss_cate_8: 0  loss_mask_8: 0.4706  loss_dice_8: 0.6345  time: 1.3905  data_time: 0.0108  lr: 9.1468e-06  max_mem: 23815M
[01/01 12:00:23] d2.utils.events INFO:  eta: 13:33:32  iter: 3699  total_loss: 20.22  loss_ce: 0.5427  loss_cate: 0.133  loss_mask: 0.6358  loss_dice: 0.9189  loss_ce_0: 0.8016  loss_cate_0: 0  loss_mask_0: 0.6351  loss_dice_0: 1.017  loss_ce_1: 0.5689  loss_cate_1: 0  loss_mask_1: 0.6277  loss_dice_1: 0.9984  loss_ce_2: 0.464  loss_cate_2: 0  loss_mask_2: 0.6612  loss_dice_2: 0.9489  loss_ce_3: 0.4933  loss_cate_3: 0  loss_mask_3: 0.616  loss_dice_3: 0.9986  loss_ce_4: 0.4995  loss_cate_4: 0  loss_mask_4: 0.6201  loss_dice_4: 0.9642  loss_ce_5: 0.4582  loss_cate_5: 0  loss_mask_5: 0.6059  loss_dice_5: 0.9475  loss_ce_6: 0.4042  loss_cate_6: 0  loss_mask_6: 0.5983  loss_dice_6: 0.9675  loss_ce_7: 0.4728  loss_cate_7: 0  loss_mask_7: 0.5929  loss_dice_7: 0.9803  loss_ce_8: 0.4992  loss_cate_8: 0  loss_mask_8: 0.6127  loss_dice_8: 0.9467  time: 1.3906  data_time: 0.0139  lr: 9.1422e-06  max_mem: 23815M
[01/01 12:00:50] d2.utils.events INFO:  eta: 13:33:08  iter: 3719  total_loss: 16.66  loss_ce: 0.4126  loss_cate: 0.1251  loss_mask: 0.5144  loss_dice: 0.5359  loss_ce_0: 0.7601  loss_cate_0: 0  loss_mask_0: 0.5574  loss_dice_0: 0.55  loss_ce_1: 0.4503  loss_cate_1: 0  loss_mask_1: 0.5136  loss_dice_1: 0.5744  loss_ce_2: 0.4153  loss_cate_2: 0  loss_mask_2: 0.4852  loss_dice_2: 0.5733  loss_ce_3: 0.3207  loss_cate_3: 0  loss_mask_3: 0.4919  loss_dice_3: 0.5181  loss_ce_4: 0.3595  loss_cate_4: 0  loss_mask_4: 0.5072  loss_dice_4: 0.5164  loss_ce_5: 0.3602  loss_cate_5: 0  loss_mask_5: 0.5367  loss_dice_5: 0.4876  loss_ce_6: 0.4039  loss_cate_6: 0  loss_mask_6: 0.4663  loss_dice_6: 0.4802  loss_ce_7: 0.3888  loss_cate_7: 0  loss_mask_7: 0.448  loss_dice_7: 0.5573  loss_ce_8: 0.4101  loss_cate_8: 0  loss_mask_8: 0.5042  loss_dice_8: 0.5248  time: 1.3905  data_time: 0.0154  lr: 9.1375e-06  max_mem: 23815M
[01/01 12:01:18] d2.utils.events INFO:  eta: 13:32:42  iter: 3739  total_loss: 22.53  loss_ce: 0.4185  loss_cate: 0.1168  loss_mask: 0.6591  loss_dice: 1.096  loss_ce_0: 0.765  loss_cate_0: 0  loss_mask_0: 0.7347  loss_dice_0: 1.047  loss_ce_1: 0.4318  loss_cate_1: 0  loss_mask_1: 0.7072  loss_dice_1: 1.001  loss_ce_2: 0.4762  loss_cate_2: 0  loss_mask_2: 0.6159  loss_dice_2: 1.047  loss_ce_3: 0.4554  loss_cate_3: 0  loss_mask_3: 0.6642  loss_dice_3: 1.042  loss_ce_4: 0.4056  loss_cate_4: 0  loss_mask_4: 0.7161  loss_dice_4: 1.012  loss_ce_5: 0.3748  loss_cate_5: 0  loss_mask_5: 0.6468  loss_dice_5: 1.016  loss_ce_6: 0.3785  loss_cate_6: 0  loss_mask_6: 0.7112  loss_dice_6: 1.043  loss_ce_7: 0.3398  loss_cate_7: 0  loss_mask_7: 0.7221  loss_dice_7: 1.051  loss_ce_8: 0.3733  loss_cate_8: 0  loss_mask_8: 0.7368  loss_dice_8: 1.025  time: 1.3905  data_time: 0.0132  lr: 9.1329e-06  max_mem: 23815M
[01/01 12:01:46] d2.utils.events INFO:  eta: 13:32:12  iter: 3759  total_loss: 17.53  loss_ce: 0.6434  loss_cate: 0.1191  loss_mask: 0.3967  loss_dice: 0.6854  loss_ce_0: 0.8289  loss_cate_0: 0  loss_mask_0: 0.3544  loss_dice_0: 0.6041  loss_ce_1: 0.6816  loss_cate_1: 0  loss_mask_1: 0.3663  loss_dice_1: 0.6876  loss_ce_2: 0.5413  loss_cate_2: 0  loss_mask_2: 0.3752  loss_dice_2: 0.6996  loss_ce_3: 0.5589  loss_cate_3: 0  loss_mask_3: 0.378  loss_dice_3: 0.6942  loss_ce_4: 0.5103  loss_cate_4: 0  loss_mask_4: 0.3685  loss_dice_4: 0.6808  loss_ce_5: 0.5082  loss_cate_5: 0  loss_mask_5: 0.3895  loss_dice_5: 0.6915  loss_ce_6: 0.6572  loss_cate_6: 0  loss_mask_6: 0.4082  loss_dice_6: 0.6921  loss_ce_7: 0.6331  loss_cate_7: 0  loss_mask_7: 0.3967  loss_dice_7: 0.6915  loss_ce_8: 0.5962  loss_cate_8: 0  loss_mask_8: 0.4099  loss_dice_8: 0.7097  time: 1.3905  data_time: 0.0129  lr: 9.1282e-06  max_mem: 23815M
[01/01 12:02:14] d2.utils.events INFO:  eta: 13:31:45  iter: 3779  total_loss: 19.84  loss_ce: 0.4623  loss_cate: 0.1153  loss_mask: 0.6034  loss_dice: 0.8702  loss_ce_0: 0.8544  loss_cate_0: 0  loss_mask_0: 0.6192  loss_dice_0: 0.7748  loss_ce_1: 0.4547  loss_cate_1: 0  loss_mask_1: 0.626  loss_dice_1: 0.8348  loss_ce_2: 0.4135  loss_cate_2: 0  loss_mask_2: 0.5796  loss_dice_2: 0.8  loss_ce_3: 0.4245  loss_cate_3: 0  loss_mask_3: 0.5672  loss_dice_3: 0.8167  loss_ce_4: 0.4326  loss_cate_4: 0  loss_mask_4: 0.5877  loss_dice_4: 0.792  loss_ce_5: 0.4468  loss_cate_5: 0  loss_mask_5: 0.5877  loss_dice_5: 0.8183  loss_ce_6: 0.4746  loss_cate_6: 0  loss_mask_6: 0.5937  loss_dice_6: 0.8299  loss_ce_7: 0.4919  loss_cate_7: 0  loss_mask_7: 0.5616  loss_dice_7: 0.8398  loss_ce_8: 0.4165  loss_cate_8: 0  loss_mask_8: 0.6061  loss_dice_8: 0.878  time: 1.3905  data_time: 0.0109  lr: 9.1235e-06  max_mem: 23815M
[01/01 12:02:42] d2.utils.events INFO:  eta: 13:31:19  iter: 3799  total_loss: 19.87  loss_ce: 0.464  loss_cate: 0.1309  loss_mask: 0.5767  loss_dice: 0.8758  loss_ce_0: 0.7026  loss_cate_0: 0  loss_mask_0: 0.5991  loss_dice_0: 0.8652  loss_ce_1: 0.5525  loss_cate_1: 0  loss_mask_1: 0.6471  loss_dice_1: 0.8625  loss_ce_2: 0.4769  loss_cate_2: 0  loss_mask_2: 0.6261  loss_dice_2: 0.8257  loss_ce_3: 0.4908  loss_cate_3: 0  loss_mask_3: 0.6057  loss_dice_3: 0.8345  loss_ce_4: 0.5087  loss_cate_4: 0  loss_mask_4: 0.6467  loss_dice_4: 0.7878  loss_ce_5: 0.5601  loss_cate_5: 0  loss_mask_5: 0.7301  loss_dice_5: 0.8301  loss_ce_6: 0.499  loss_cate_6: 0  loss_mask_6: 0.6364  loss_dice_6: 0.8089  loss_ce_7: 0.5351  loss_cate_7: 0  loss_mask_7: 0.5733  loss_dice_7: 0.8286  loss_ce_8: 0.5121  loss_cate_8: 0  loss_mask_8: 0.6  loss_dice_8: 0.854  time: 1.3905  data_time: 0.0137  lr: 9.1189e-06  max_mem: 23815M
[01/01 12:03:09] d2.utils.events INFO:  eta: 13:30:50  iter: 3819  total_loss: 20.99  loss_ce: 0.6158  loss_cate: 0.1196  loss_mask: 0.6249  loss_dice: 0.7485  loss_ce_0: 0.8755  loss_cate_0: 0  loss_mask_0: 0.633  loss_dice_0: 0.8072  loss_ce_1: 0.6217  loss_cate_1: 0  loss_mask_1: 0.6702  loss_dice_1: 0.8021  loss_ce_2: 0.5979  loss_cate_2: 0  loss_mask_2: 0.6273  loss_dice_2: 0.8287  loss_ce_3: 0.6478  loss_cate_3: 0  loss_mask_3: 0.6653  loss_dice_3: 0.7641  loss_ce_4: 0.595  loss_cate_4: 0  loss_mask_4: 0.654  loss_dice_4: 0.7614  loss_ce_5: 0.6702  loss_cate_5: 0  loss_mask_5: 0.6079  loss_dice_5: 0.8829  loss_ce_6: 0.6937  loss_cate_6: 0  loss_mask_6: 0.6106  loss_dice_6: 0.7836  loss_ce_7: 0.6128  loss_cate_7: 0  loss_mask_7: 0.6862  loss_dice_7: 0.7773  loss_ce_8: 0.6238  loss_cate_8: 0  loss_mask_8: 0.6748  loss_dice_8: 0.7739  time: 1.3904  data_time: 0.0109  lr: 9.1142e-06  max_mem: 23815M
[01/01 12:03:37] d2.utils.events INFO:  eta: 13:30:24  iter: 3839  total_loss: 18.25  loss_ce: 0.4771  loss_cate: 0.1032  loss_mask: 0.4906  loss_dice: 0.7699  loss_ce_0: 0.778  loss_cate_0: 0  loss_mask_0: 0.4943  loss_dice_0: 0.849  loss_ce_1: 0.452  loss_cate_1: 0  loss_mask_1: 0.5274  loss_dice_1: 0.8584  loss_ce_2: 0.4967  loss_cate_2: 0  loss_mask_2: 0.5424  loss_dice_2: 0.9222  loss_ce_3: 0.4892  loss_cate_3: 0  loss_mask_3: 0.5023  loss_dice_3: 0.7754  loss_ce_4: 0.4836  loss_cate_4: 0  loss_mask_4: 0.5061  loss_dice_4: 0.7996  loss_ce_5: 0.4687  loss_cate_5: 0  loss_mask_5: 0.521  loss_dice_5: 0.7534  loss_ce_6: 0.4726  loss_cate_6: 0  loss_mask_6: 0.4732  loss_dice_6: 0.7858  loss_ce_7: 0.5124  loss_cate_7: 0  loss_mask_7: 0.4885  loss_dice_7: 0.7955  loss_ce_8: 0.5008  loss_cate_8: 0  loss_mask_8: 0.5161  loss_dice_8: 0.7786  time: 1.3904  data_time: 0.0120  lr: 9.1096e-06  max_mem: 23815M
[01/01 12:04:05] d2.utils.events INFO:  eta: 13:29:55  iter: 3859  total_loss: 20.55  loss_ce: 0.4626  loss_cate: 0.1281  loss_mask: 0.5253  loss_dice: 0.8437  loss_ce_0: 0.7563  loss_cate_0: 0  loss_mask_0: 0.55  loss_dice_0: 0.8935  loss_ce_1: 0.6066  loss_cate_1: 0  loss_mask_1: 0.5418  loss_dice_1: 0.9749  loss_ce_2: 0.5496  loss_cate_2: 0  loss_mask_2: 0.4599  loss_dice_2: 0.8756  loss_ce_3: 0.5362  loss_cate_3: 0  loss_mask_3: 0.4935  loss_dice_3: 0.896  loss_ce_4: 0.4647  loss_cate_4: 0  loss_mask_4: 0.4716  loss_dice_4: 0.9174  loss_ce_5: 0.52  loss_cate_5: 0  loss_mask_5: 0.5043  loss_dice_5: 0.8524  loss_ce_6: 0.5673  loss_cate_6: 0  loss_mask_6: 0.5003  loss_dice_6: 0.8015  loss_ce_7: 0.5534  loss_cate_7: 0  loss_mask_7: 0.4961  loss_dice_7: 0.7624  loss_ce_8: 0.5401  loss_cate_8: 0  loss_mask_8: 0.5088  loss_dice_8: 0.8143  time: 1.3905  data_time: 0.0123  lr: 9.1049e-06  max_mem: 23815M
[01/01 12:04:33] d2.utils.events INFO:  eta: 13:29:40  iter: 3879  total_loss: 24.25  loss_ce: 0.5395  loss_cate: 0.1484  loss_mask: 0.7366  loss_dice: 0.9816  loss_ce_0: 0.7002  loss_cate_0: 0  loss_mask_0: 0.788  loss_dice_0: 1.01  loss_ce_1: 0.5372  loss_cate_1: 0  loss_mask_1: 0.7171  loss_dice_1: 1.069  loss_ce_2: 0.5424  loss_cate_2: 0  loss_mask_2: 0.7495  loss_dice_2: 1.027  loss_ce_3: 0.5874  loss_cate_3: 0  loss_mask_3: 0.7198  loss_dice_3: 1.092  loss_ce_4: 0.5662  loss_cate_4: 0  loss_mask_4: 0.6916  loss_dice_4: 1.063  loss_ce_5: 0.4707  loss_cate_5: 0  loss_mask_5: 0.7302  loss_dice_5: 1.019  loss_ce_6: 0.4885  loss_cate_6: 0  loss_mask_6: 0.7444  loss_dice_6: 0.9727  loss_ce_7: 0.4904  loss_cate_7: 0  loss_mask_7: 0.7615  loss_dice_7: 0.9755  loss_ce_8: 0.4956  loss_cate_8: 0  loss_mask_8: 0.7321  loss_dice_8: 1.042  time: 1.3906  data_time: 0.0140  lr: 9.1002e-06  max_mem: 23815M
[01/01 12:05:01] d2.data.datasets.coco INFO: Loaded 653 images with semantic segmentation from /opt/ml/input/data/images/val_0
[01/01 12:05:01] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=1280, sample_style='choice')]
[01/01 12:05:01] d2.data.common INFO: Serializing 653 elements to byte tensors and concatenating them all ...
[01/01 12:05:01] d2.data.common INFO: Serialized dataset takes 0.09 MiB
[01/01 12:05:01] d2.data.datasets.coco INFO: Loaded 653 images with semantic segmentation from /opt/ml/input/data/images/val_0
[01/01 12:05:01] d2.evaluation.evaluator INFO: Start inference on 653 batches
[01/01 12:05:03] d2.evaluation.evaluator INFO: Inference done 11/653. Dataloading: 0.0012 s/iter. Inference: 0.1132 s/iter. Eval: 0.0124 s/iter. Total: 0.1268 s/iter. ETA=0:01:21
[01/01 12:05:08] d2.evaluation.evaluator INFO: Inference done 51/653. Dataloading: 0.0017 s/iter. Inference: 0.1132 s/iter. Eval: 0.0125 s/iter. Total: 0.1275 s/iter. ETA=0:01:16
[01/01 12:05:13] d2.evaluation.evaluator INFO: Inference done 91/653. Dataloading: 0.0017 s/iter. Inference: 0.1131 s/iter. Eval: 0.0124 s/iter. Total: 0.1273 s/iter. ETA=0:01:11
[01/01 12:05:18] d2.evaluation.evaluator INFO: Inference done 131/653. Dataloading: 0.0017 s/iter. Inference: 0.1127 s/iter. Eval: 0.0124 s/iter. Total: 0.1269 s/iter. ETA=0:01:06
[01/01 12:05:23] d2.evaluation.evaluator INFO: Inference done 173/653. Dataloading: 0.0017 s/iter. Inference: 0.1111 s/iter. Eval: 0.0123 s/iter. Total: 0.1252 s/iter. ETA=0:01:00
[01/01 12:05:28] d2.evaluation.evaluator INFO: Inference done 215/653. Dataloading: 0.0017 s/iter. Inference: 0.1102 s/iter. Eval: 0.0124 s/iter. Total: 0.1244 s/iter. ETA=0:00:54
[01/01 12:05:33] d2.evaluation.evaluator INFO: Inference done 257/653. Dataloading: 0.0017 s/iter. Inference: 0.1095 s/iter. Eval: 0.0124 s/iter. Total: 0.1238 s/iter. ETA=0:00:49
[01/01 12:05:38] d2.evaluation.evaluator INFO: Inference done 298/653. Dataloading: 0.0017 s/iter. Inference: 0.1092 s/iter. Eval: 0.0125 s/iter. Total: 0.1236 s/iter. ETA=0:00:43
[01/01 12:05:43] d2.evaluation.evaluator INFO: Inference done 340/653. Dataloading: 0.0017 s/iter. Inference: 0.1088 s/iter. Eval: 0.0126 s/iter. Total: 0.1232 s/iter. ETA=0:00:38
[01/01 12:05:48] d2.evaluation.evaluator INFO: Inference done 382/653. Dataloading: 0.0017 s/iter. Inference: 0.1084 s/iter. Eval: 0.0126 s/iter. Total: 0.1228 s/iter. ETA=0:00:33
[01/01 12:05:53] d2.evaluation.evaluator INFO: Inference done 424/653. Dataloading: 0.0017 s/iter. Inference: 0.1080 s/iter. Eval: 0.0126 s/iter. Total: 0.1224 s/iter. ETA=0:00:28
[01/01 12:05:58] d2.evaluation.evaluator INFO: Inference done 466/653. Dataloading: 0.0017 s/iter. Inference: 0.1077 s/iter. Eval: 0.0127 s/iter. Total: 0.1221 s/iter. ETA=0:00:22
[01/01 12:06:03] d2.evaluation.evaluator INFO: Inference done 508/653. Dataloading: 0.0017 s/iter. Inference: 0.1075 s/iter. Eval: 0.0127 s/iter. Total: 0.1219 s/iter. ETA=0:00:17
[01/01 12:06:08] d2.evaluation.evaluator INFO: Inference done 551/653. Dataloading: 0.0017 s/iter. Inference: 0.1073 s/iter. Eval: 0.0126 s/iter. Total: 0.1216 s/iter. ETA=0:00:12
[01/01 12:06:14] d2.evaluation.evaluator INFO: Inference done 594/653. Dataloading: 0.0017 s/iter. Inference: 0.1071 s/iter. Eval: 0.0125 s/iter. Total: 0.1213 s/iter. ETA=0:00:07
[01/01 12:06:19] d2.evaluation.evaluator INFO: Inference done 637/653. Dataloading: 0.0017 s/iter. Inference: 0.1069 s/iter. Eval: 0.0124 s/iter. Total: 0.1211 s/iter. ETA=0:00:01
[01/01 12:06:21] d2.evaluation.evaluator INFO: Total inference time: 0:01:18.532489 (0.121192 s / iter per device, on 1 devices)
[01/01 12:06:21] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:01:09 (0.106915 s / iter per device, on 1 devices)
[01/01 12:06:21] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 66.42078006799179, 'fwIoU': 89.42489333168598, 'IoU-Backgroud': 96.69313900102395, 'IoU-General trash': 25.989415865696504, 'IoU-Paper': 77.18499889029205, 'IoU-Paper pack': 37.339647787725895, 'IoU-Metal': 54.303632199653265, 'IoU-Glass': 60.92757933369191, 'IoU-Plastic': 53.319391310957, 'IoU-Styrofoam': 79.77039094368335, 'IoU-Plastic bag': 84.84163460899103, 'IoU-Battery': 80.0976356151408, 'IoU-Clothing': 80.16111519105397, 'mACC': 76.66649169134901, 'pACC': 93.7128233507983, 'ACC-Backgroud': 98.12143104602515, 'ACC-General trash': 33.97444880986999, 'ACC-Paper': 92.45808878779064, 'ACC-Paper pack': 63.124859013975076, 'ACC-Metal': 63.87825041075236, 'ACC-Glass': 64.87120337295305, 'ACC-Plastic': 74.11653065745871, 'ACC-Styrofoam': 89.37291401424574, 'ACC-Plastic bag': 90.90082590042536, 'ACC-Battery': 83.23301154646981, 'ACC-Clothing': 89.27984504487327})])
[01/01 12:06:21] d2.engine.defaults INFO: Evaluation results for trash_recycle_sem_seg_val_0 in csv format:
[01/01 12:06:21] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/01 12:06:21] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/01 12:06:21] d2.evaluation.testing INFO: copypaste: 66.4208,89.4249,76.6665,93.7128
[01/01 12:06:21] fvcore.common.checkpoint INFO: Saving checkpoint to ./trash_dataV1_WarmupPolyLR_1e-5/model_best_3899iter.pth
[01/01 12:06:23] d2.engine.hooks INFO: Saved best model as latest eval score for mIoU is66.42078, better than last best score 57.29028 @ iteration 2599.
[01/01 12:06:23] d2.utils.events INFO:  eta: 13:29:13  iter: 3899  total_loss: 25.34  loss_ce: 0.6219  loss_cate: 0.151  loss_mask: 0.6909  loss_dice: 1.053  loss_ce_0: 1.011  loss_cate_0: 0  loss_mask_0: 0.6622  loss_dice_0: 0.9806  loss_ce_1: 0.7576  loss_cate_1: 0  loss_mask_1: 0.706  loss_dice_1: 1.009  loss_ce_2: 0.7387  loss_cate_2: 0  loss_mask_2: 0.652  loss_dice_2: 0.9837  loss_ce_3: 0.6765  loss_cate_3: 0  loss_mask_3: 0.6823  loss_dice_3: 0.9981  loss_ce_4: 0.7184  loss_cate_4: 0  loss_mask_4: 0.7141  loss_dice_4: 0.9976  loss_ce_5: 0.6677  loss_cate_5: 0  loss_mask_5: 0.7393  loss_dice_5: 1.01  loss_ce_6: 0.6412  loss_cate_6: 0  loss_mask_6: 0.6939  loss_dice_6: 1.021  loss_ce_7: 0.6842  loss_cate_7: 0  loss_mask_7: 0.6552  loss_dice_7: 1.051  loss_ce_8: 0.6981  loss_cate_8: 0  loss_mask_8: 0.7361  loss_dice_8: 0.9994  time: 1.3906  data_time: 0.0132  lr: 9.0956e-06  max_mem: 23815M
[01/01 12:06:50] d2.utils.events INFO:  eta: 13:28:47  iter: 3919  total_loss: 17.39  loss_ce: 0.5146  loss_cate: 0.1332  loss_mask: 0.466  loss_dice: 0.7362  loss_ce_0: 0.742  loss_cate_0: 0  loss_mask_0: 0.4577  loss_dice_0: 0.7583  loss_ce_1: 0.4675  loss_cate_1: 0  loss_mask_1: 0.5171  loss_dice_1: 0.7046  loss_ce_2: 0.4898  loss_cate_2: 0  loss_mask_2: 0.4727  loss_dice_2: 0.7105  loss_ce_3: 0.5076  loss_cate_3: 0  loss_mask_3: 0.4898  loss_dice_3: 0.7206  loss_ce_4: 0.4806  loss_cate_4: 0  loss_mask_4: 0.4919  loss_dice_4: 0.7152  loss_ce_5: 0.4841  loss_cate_5: 0  loss_mask_5: 0.4791  loss_dice_5: 0.7075  loss_ce_6: 0.467  loss_cate_6: 0  loss_mask_6: 0.5034  loss_dice_6: 0.7445  loss_ce_7: 0.4781  loss_cate_7: 0  loss_mask_7: 0.6008  loss_dice_7: 0.6787  loss_ce_8: 0.4242  loss_cate_8: 0  loss_mask_8: 0.4665  loss_dice_8: 0.7174  time: 1.3905  data_time: 0.0117  lr: 9.0909e-06  max_mem: 23815M
[01/01 12:07:18] d2.utils.events INFO:  eta: 13:28:19  iter: 3939  total_loss: 15.53  loss_ce: 0.3372  loss_cate: 0.1166  loss_mask: 0.4673  loss_dice: 0.7162  loss_ce_0: 0.6825  loss_cate_0: 0  loss_mask_0: 0.4976  loss_dice_0: 0.8145  loss_ce_1: 0.348  loss_cate_1: 0  loss_mask_1: 0.4523  loss_dice_1: 0.82  loss_ce_2: 0.3648  loss_cate_2: 0  loss_mask_2: 0.4477  loss_dice_2: 0.811  loss_ce_3: 0.3406  loss_cate_3: 0  loss_mask_3: 0.4706  loss_dice_3: 0.8054  loss_ce_4: 0.322  loss_cate_4: 0  loss_mask_4: 0.4831  loss_dice_4: 0.8375  loss_ce_5: 0.3701  loss_cate_5: 0  loss_mask_5: 0.4773  loss_dice_5: 0.7832  loss_ce_6: 0.3417  loss_cate_6: 0  loss_mask_6: 0.4552  loss_dice_6: 0.7461  loss_ce_7: 0.3766  loss_cate_7: 0  loss_mask_7: 0.4373  loss_dice_7: 0.7307  loss_ce_8: 0.3456  loss_cate_8: 0  loss_mask_8: 0.4725  loss_dice_8: 0.7677  time: 1.3905  data_time: 0.0146  lr: 9.0862e-06  max_mem: 23815M
[01/01 12:07:46] d2.utils.events INFO:  eta: 13:27:50  iter: 3959  total_loss: 17.27  loss_ce: 0.4707  loss_cate: 0.1335  loss_mask: 0.3972  loss_dice: 0.6095  loss_ce_0: 0.8272  loss_cate_0: 0  loss_mask_0: 0.3884  loss_dice_0: 0.7016  loss_ce_1: 0.5632  loss_cate_1: 0  loss_mask_1: 0.399  loss_dice_1: 0.7052  loss_ce_2: 0.5327  loss_cate_2: 0  loss_mask_2: 0.3759  loss_dice_2: 0.7104  loss_ce_3: 0.4737  loss_cate_3: 0  loss_mask_3: 0.3655  loss_dice_3: 0.7383  loss_ce_4: 0.4996  loss_cate_4: 0  loss_mask_4: 0.3652  loss_dice_4: 0.6923  loss_ce_5: 0.4768  loss_cate_5: 0  loss_mask_5: 0.3938  loss_dice_5: 0.7594  loss_ce_6: 0.4756  loss_cate_6: 0  loss_mask_6: 0.3878  loss_dice_6: 0.7019  loss_ce_7: 0.4706  loss_cate_7: 0  loss_mask_7: 0.3887  loss_dice_7: 0.6898  loss_ce_8: 0.4503  loss_cate_8: 0  loss_mask_8: 0.3968  loss_dice_8: 0.6491  time: 1.3905  data_time: 0.0126  lr: 9.0816e-06  max_mem: 23815M
[01/01 12:08:14] d2.utils.events INFO:  eta: 13:27:23  iter: 3979  total_loss: 23.85  loss_ce: 0.6952  loss_cate: 0.1552  loss_mask: 0.615  loss_dice: 0.9268  loss_ce_0: 0.8744  loss_cate_0: 0  loss_mask_0: 0.7069  loss_dice_0: 0.9527  loss_ce_1: 0.6387  loss_cate_1: 0  loss_mask_1: 0.6482  loss_dice_1: 1.002  loss_ce_2: 0.6174  loss_cate_2: 0  loss_mask_2: 0.6825  loss_dice_2: 0.9451  loss_ce_3: 0.6633  loss_cate_3: 0  loss_mask_3: 0.6992  loss_dice_3: 0.9193  loss_ce_4: 0.6615  loss_cate_4: 0  loss_mask_4: 0.6784  loss_dice_4: 0.8848  loss_ce_5: 0.5821  loss_cate_5: 0  loss_mask_5: 0.705  loss_dice_5: 0.8771  loss_ce_6: 0.6305  loss_cate_6: 0  loss_mask_6: 0.6184  loss_dice_6: 0.9067  loss_ce_7: 0.7045  loss_cate_7: 0  loss_mask_7: 0.6113  loss_dice_7: 0.947  loss_ce_8: 0.6698  loss_cate_8: 0  loss_mask_8: 0.6185  loss_dice_8: 0.9356  time: 1.3905  data_time: 0.0117  lr: 9.0769e-06  max_mem: 23815M
[01/01 12:08:42] d2.utils.events INFO:  eta: 13:26:56  iter: 3999  total_loss: 17.53  loss_ce: 0.5115  loss_cate: 0.08873  loss_mask: 0.4437  loss_dice: 0.771  loss_ce_0: 0.7863  loss_cate_0: 0  loss_mask_0: 0.4586  loss_dice_0: 0.7821  loss_ce_1: 0.5041  loss_cate_1: 0  loss_mask_1: 0.4367  loss_dice_1: 0.8004  loss_ce_2: 0.5382  loss_cate_2: 0  loss_mask_2: 0.4339  loss_dice_2: 0.7616  loss_ce_3: 0.5214  loss_cate_3: 0  loss_mask_3: 0.4262  loss_dice_3: 0.7881  loss_ce_4: 0.5071  loss_cate_4: 0  loss_mask_4: 0.3946  loss_dice_4: 0.7946  loss_ce_5: 0.4791  loss_cate_5: 0  loss_mask_5: 0.408  loss_dice_5: 0.7803  loss_ce_6: 0.4558  loss_cate_6: 0  loss_mask_6: 0.4592  loss_dice_6: 0.7697  loss_ce_7: 0.5035  loss_cate_7: 0  loss_mask_7: 0.4191  loss_dice_7: 0.8274  loss_ce_8: 0.4648  loss_cate_8: 0  loss_mask_8: 0.4257  loss_dice_8: 0.7666  time: 1.3905  data_time: 0.0147  lr: 9.0722e-06  max_mem: 23815M
[01/01 12:09:10] d2.utils.events INFO:  eta: 13:26:26  iter: 4019  total_loss: 18.78  loss_ce: 0.5732  loss_cate: 0.1318  loss_mask: 0.5934  loss_dice: 0.7904  loss_ce_0: 0.8717  loss_cate_0: 0  loss_mask_0: 0.6268  loss_dice_0: 0.8454  loss_ce_1: 0.5408  loss_cate_1: 0  loss_mask_1: 0.6134  loss_dice_1: 0.8592  loss_ce_2: 0.5249  loss_cate_2: 0  loss_mask_2: 0.6046  loss_dice_2: 0.8285  loss_ce_3: 0.5415  loss_cate_3: 0  loss_mask_3: 0.578  loss_dice_3: 0.7934  loss_ce_4: 0.5913  loss_cate_4: 0  loss_mask_4: 0.5905  loss_dice_4: 0.8159  loss_ce_5: 0.5215  loss_cate_5: 0  loss_mask_5: 0.5691  loss_dice_5: 0.8091  loss_ce_6: 0.571  loss_cate_6: 0  loss_mask_6: 0.6005  loss_dice_6: 0.7582  loss_ce_7: 0.5615  loss_cate_7: 0  loss_mask_7: 0.5961  loss_dice_7: 0.7492  loss_ce_8: 0.6055  loss_cate_8: 0  loss_mask_8: 0.6071  loss_dice_8: 0.7412  time: 1.3905  data_time: 0.0148  lr: 9.0676e-06  max_mem: 23815M
[01/01 12:09:37] d2.utils.events INFO:  eta: 13:25:54  iter: 4039  total_loss: 12.83  loss_ce: 0.46  loss_cate: 0.112  loss_mask: 0.4604  loss_dice: 0.4835  loss_ce_0: 0.6969  loss_cate_0: 0  loss_mask_0: 0.4454  loss_dice_0: 0.5544  loss_ce_1: 0.4228  loss_cate_1: 0  loss_mask_1: 0.4434  loss_dice_1: 0.5161  loss_ce_2: 0.4841  loss_cate_2: 0  loss_mask_2: 0.4162  loss_dice_2: 0.5348  loss_ce_3: 0.4973  loss_cate_3: 0  loss_mask_3: 0.4041  loss_dice_3: 0.5279  loss_ce_4: 0.5198  loss_cate_4: 0  loss_mask_4: 0.3997  loss_dice_4: 0.5199  loss_ce_5: 0.5285  loss_cate_5: 0  loss_mask_5: 0.4445  loss_dice_5: 0.5186  loss_ce_6: 0.5543  loss_cate_6: 0  loss_mask_6: 0.4683  loss_dice_6: 0.5151  loss_ce_7: 0.4658  loss_cate_7: 0  loss_mask_7: 0.4725  loss_dice_7: 0.4977  loss_ce_8: 0.4603  loss_cate_8: 0  loss_mask_8: 0.4448  loss_dice_8: 0.4932  time: 1.3904  data_time: 0.0111  lr: 9.0629e-06  max_mem: 23815M
[01/01 12:10:05] d2.utils.events INFO:  eta: 13:25:22  iter: 4059  total_loss: 23.71  loss_ce: 0.6618  loss_cate: 0.1261  loss_mask: 0.5873  loss_dice: 0.9376  loss_ce_0: 1.012  loss_cate_0: 0  loss_mask_0: 0.591  loss_dice_0: 1.047  loss_ce_1: 0.7346  loss_cate_1: 0  loss_mask_1: 0.6263  loss_dice_1: 1.016  loss_ce_2: 0.6476  loss_cate_2: 0  loss_mask_2: 0.5473  loss_dice_2: 0.9442  loss_ce_3: 0.7007  loss_cate_3: 0  loss_mask_3: 0.6056  loss_dice_3: 0.9594  loss_ce_4: 0.7244  loss_cate_4: 0  loss_mask_4: 0.5848  loss_dice_4: 0.9552  loss_ce_5: 0.6155  loss_cate_5: 0  loss_mask_5: 0.584  loss_dice_5: 0.9586  loss_ce_6: 0.5713  loss_cate_6: 0  loss_mask_6: 0.5935  loss_dice_6: 1  loss_ce_7: 0.738  loss_cate_7: 0  loss_mask_7: 0.6062  loss_dice_7: 0.9755  loss_ce_8: 0.5861  loss_cate_8: 0  loss_mask_8: 0.5654  loss_dice_8: 0.9865  time: 1.3904  data_time: 0.0120  lr: 9.0582e-06  max_mem: 23815M
[01/01 12:10:33] d2.utils.events INFO:  eta: 13:24:51  iter: 4079  total_loss: 21.82  loss_ce: 0.5741  loss_cate: 0.1591  loss_mask: 0.5476  loss_dice: 0.8425  loss_ce_0: 0.9409  loss_cate_0: 0  loss_mask_0: 0.6178  loss_dice_0: 0.8264  loss_ce_1: 0.6737  loss_cate_1: 0  loss_mask_1: 0.5645  loss_dice_1: 0.8521  loss_ce_2: 0.5629  loss_cate_2: 0  loss_mask_2: 0.5114  loss_dice_2: 0.8426  loss_ce_3: 0.6109  loss_cate_3: 0  loss_mask_3: 0.5422  loss_dice_3: 0.7893  loss_ce_4: 0.5335  loss_cate_4: 0  loss_mask_4: 0.5897  loss_dice_4: 0.7477  loss_ce_5: 0.5734  loss_cate_5: 0  loss_mask_5: 0.5492  loss_dice_5: 0.8497  loss_ce_6: 0.5543  loss_cate_6: 0  loss_mask_6: 0.5444  loss_dice_6: 0.8198  loss_ce_7: 0.5782  loss_cate_7: 0  loss_mask_7: 0.5399  loss_dice_7: 0.7554  loss_ce_8: 0.5413  loss_cate_8: 0  loss_mask_8: 0.543  loss_dice_8: 0.8152  time: 1.3904  data_time: 0.0118  lr: 9.0536e-06  max_mem: 23815M
[01/01 12:11:00] d2.utils.events INFO:  eta: 13:24:21  iter: 4099  total_loss: 15.14  loss_ce: 0.3597  loss_cate: 0.1246  loss_mask: 0.4567  loss_dice: 0.5595  loss_ce_0: 0.7693  loss_cate_0: 0  loss_mask_0: 0.4922  loss_dice_0: 0.6056  loss_ce_1: 0.4895  loss_cate_1: 0  loss_mask_1: 0.4863  loss_dice_1: 0.641  loss_ce_2: 0.4174  loss_cate_2: 0  loss_mask_2: 0.4625  loss_dice_2: 0.5823  loss_ce_3: 0.3837  loss_cate_3: 0  loss_mask_3: 0.4653  loss_dice_3: 0.5862  loss_ce_4: 0.3733  loss_cate_4: 0  loss_mask_4: 0.4884  loss_dice_4: 0.5836  loss_ce_5: 0.3849  loss_cate_5: 0  loss_mask_5: 0.4879  loss_dice_5: 0.5744  loss_ce_6: 0.4116  loss_cate_6: 0  loss_mask_6: 0.4891  loss_dice_6: 0.5136  loss_ce_7: 0.4113  loss_cate_7: 0  loss_mask_7: 0.4669  loss_dice_7: 0.5772  loss_ce_8: 0.3515  loss_cate_8: 0  loss_mask_8: 0.4924  loss_dice_8: 0.5665  time: 1.3903  data_time: 0.0122  lr: 9.0489e-06  max_mem: 23815M
[01/01 12:11:28] d2.utils.events INFO:  eta: 13:23:51  iter: 4119  total_loss: 21.43  loss_ce: 0.4393  loss_cate: 0.1187  loss_mask: 0.5592  loss_dice: 0.9342  loss_ce_0: 0.784  loss_cate_0: 0  loss_mask_0: 0.6099  loss_dice_0: 0.904  loss_ce_1: 0.5369  loss_cate_1: 0  loss_mask_1: 0.5677  loss_dice_1: 0.9363  loss_ce_2: 0.5111  loss_cate_2: 0  loss_mask_2: 0.5454  loss_dice_2: 0.951  loss_ce_3: 0.5455  loss_cate_3: 0  loss_mask_3: 0.4953  loss_dice_3: 0.9079  loss_ce_4: 0.5093  loss_cate_4: 0  loss_mask_4: 0.5535  loss_dice_4: 0.9957  loss_ce_5: 0.4135  loss_cate_5: 0  loss_mask_5: 0.5777  loss_dice_5: 0.9446  loss_ce_6: 0.4878  loss_cate_6: 0  loss_mask_6: 0.5543  loss_dice_6: 0.9477  loss_ce_7: 0.4578  loss_cate_7: 0  loss_mask_7: 0.5222  loss_dice_7: 1.001  loss_ce_8: 0.4326  loss_cate_8: 0  loss_mask_8: 0.5187  loss_dice_8: 0.9022  time: 1.3902  data_time: 0.0138  lr: 9.0442e-06  max_mem: 23815M
[01/01 12:11:56] d2.utils.events INFO:  eta: 13:23:14  iter: 4139  total_loss: 18.03  loss_ce: 0.4431  loss_cate: 0.1419  loss_mask: 0.4631  loss_dice: 0.8095  loss_ce_0: 0.8334  loss_cate_0: 0  loss_mask_0: 0.4617  loss_dice_0: 0.8214  loss_ce_1: 0.4732  loss_cate_1: 0  loss_mask_1: 0.4312  loss_dice_1: 0.8813  loss_ce_2: 0.5143  loss_cate_2: 0  loss_mask_2: 0.5406  loss_dice_2: 0.7698  loss_ce_3: 0.446  loss_cate_3: 0  loss_mask_3: 0.5569  loss_dice_3: 0.8263  loss_ce_4: 0.4471  loss_cate_4: 0  loss_mask_4: 0.5994  loss_dice_4: 0.7949  loss_ce_5: 0.5157  loss_cate_5: 0  loss_mask_5: 0.4454  loss_dice_5: 0.7792  loss_ce_6: 0.5085  loss_cate_6: 0  loss_mask_6: 0.4719  loss_dice_6: 0.8049  loss_ce_7: 0.4401  loss_cate_7: 0  loss_mask_7: 0.5306  loss_dice_7: 0.8168  loss_ce_8: 0.5269  loss_cate_8: 0  loss_mask_8: 0.4524  loss_dice_8: 0.8089  time: 1.3902  data_time: 0.0120  lr: 9.0396e-06  max_mem: 23815M
[01/01 12:12:23] d2.utils.events INFO:  eta: 13:22:48  iter: 4159  total_loss: 20.08  loss_ce: 0.5174  loss_cate: 0.1253  loss_mask: 0.5349  loss_dice: 0.8368  loss_ce_0: 0.8946  loss_cate_0: 0  loss_mask_0: 0.5611  loss_dice_0: 0.8661  loss_ce_1: 0.6054  loss_cate_1: 0  loss_mask_1: 0.5928  loss_dice_1: 0.8909  loss_ce_2: 0.5435  loss_cate_2: 0  loss_mask_2: 0.5701  loss_dice_2: 0.873  loss_ce_3: 0.5697  loss_cate_3: 0  loss_mask_3: 0.5283  loss_dice_3: 0.872  loss_ce_4: 0.5226  loss_cate_4: 0  loss_mask_4: 0.5138  loss_dice_4: 0.9305  loss_ce_5: 0.4892  loss_cate_5: 0  loss_mask_5: 0.5197  loss_dice_5: 0.9142  loss_ce_6: 0.4412  loss_cate_6: 0  loss_mask_6: 0.594  loss_dice_6: 0.8962  loss_ce_7: 0.4508  loss_cate_7: 0  loss_mask_7: 0.5339  loss_dice_7: 0.8566  loss_ce_8: 0.4106  loss_cate_8: 0  loss_mask_8: 0.5812  loss_dice_8: 0.8581  time: 1.3902  data_time: 0.0131  lr: 9.0349e-06  max_mem: 23815M
[01/01 12:12:51] d2.utils.events INFO:  eta: 13:22:19  iter: 4179  total_loss: 18.6  loss_ce: 0.466  loss_cate: 0.1497  loss_mask: 0.5003  loss_dice: 0.7214  loss_ce_0: 0.7932  loss_cate_0: 0  loss_mask_0: 0.4715  loss_dice_0: 0.74  loss_ce_1: 0.5544  loss_cate_1: 0  loss_mask_1: 0.5094  loss_dice_1: 0.7397  loss_ce_2: 0.5208  loss_cate_2: 0  loss_mask_2: 0.5129  loss_dice_2: 0.7131  loss_ce_3: 0.5068  loss_cate_3: 0  loss_mask_3: 0.5669  loss_dice_3: 0.7177  loss_ce_4: 0.502  loss_cate_4: 0  loss_mask_4: 0.529  loss_dice_4: 0.7334  loss_ce_5: 0.4953  loss_cate_5: 0  loss_mask_5: 0.4991  loss_dice_5: 0.7373  loss_ce_6: 0.4774  loss_cate_6: 0  loss_mask_6: 0.4639  loss_dice_6: 0.7107  loss_ce_7: 0.5299  loss_cate_7: 0  loss_mask_7: 0.5233  loss_dice_7: 0.745  loss_ce_8: 0.527  loss_cate_8: 0  loss_mask_8: 0.5193  loss_dice_8: 0.7146  time: 1.3902  data_time: 0.0152  lr: 9.0302e-06  max_mem: 23815M
[01/01 12:13:19] d2.utils.events INFO:  eta: 13:21:43  iter: 4199  total_loss: 17.85  loss_ce: 0.4512  loss_cate: 0.1245  loss_mask: 0.5386  loss_dice: 0.6961  loss_ce_0: 0.7696  loss_cate_0: 0  loss_mask_0: 0.5118  loss_dice_0: 0.728  loss_ce_1: 0.4383  loss_cate_1: 0  loss_mask_1: 0.4905  loss_dice_1: 0.7548  loss_ce_2: 0.4815  loss_cate_2: 0  loss_mask_2: 0.5198  loss_dice_2: 0.7636  loss_ce_3: 0.5247  loss_cate_3: 0  loss_mask_3: 0.5469  loss_dice_3: 0.6147  loss_ce_4: 0.5148  loss_cate_4: 0  loss_mask_4: 0.5403  loss_dice_4: 0.6605  loss_ce_5: 0.4909  loss_cate_5: 0  loss_mask_5: 0.4762  loss_dice_5: 0.7096  loss_ce_6: 0.5041  loss_cate_6: 0  loss_mask_6: 0.4757  loss_dice_6: 0.6452  loss_ce_7: 0.4295  loss_cate_7: 0  loss_mask_7: 0.4925  loss_dice_7: 0.7087  loss_ce_8: 0.44  loss_cate_8: 0  loss_mask_8: 0.4989  loss_dice_8: 0.6912  time: 1.3902  data_time: 0.0140  lr: 9.0256e-06  max_mem: 23815M
[01/01 12:13:47] d2.utils.events INFO:  eta: 13:21:08  iter: 4219  total_loss: 19.27  loss_ce: 0.4234  loss_cate: 0.1373  loss_mask: 0.5508  loss_dice: 0.8379  loss_ce_0: 0.7845  loss_cate_0: 0  loss_mask_0: 0.5357  loss_dice_0: 0.918  loss_ce_1: 0.4436  loss_cate_1: 0  loss_mask_1: 0.5574  loss_dice_1: 0.875  loss_ce_2: 0.5252  loss_cate_2: 0  loss_mask_2: 0.5897  loss_dice_2: 0.82  loss_ce_3: 0.4389  loss_cate_3: 0  loss_mask_3: 0.5219  loss_dice_3: 0.8438  loss_ce_4: 0.4251  loss_cate_4: 0  loss_mask_4: 0.5754  loss_dice_4: 0.8689  loss_ce_5: 0.3917  loss_cate_5: 0  loss_mask_5: 0.5561  loss_dice_5: 0.846  loss_ce_6: 0.3679  loss_cate_6: 0  loss_mask_6: 0.6034  loss_dice_6: 0.85  loss_ce_7: 0.3867  loss_cate_7: 0  loss_mask_7: 0.5692  loss_dice_7: 0.817  loss_ce_8: 0.4164  loss_cate_8: 0  loss_mask_8: 0.5483  loss_dice_8: 0.8531  time: 1.3901  data_time: 0.0116  lr: 9.0209e-06  max_mem: 23815M
[01/01 12:14:14] d2.utils.events INFO:  eta: 13:20:39  iter: 4239  total_loss: 20.12  loss_ce: 0.449  loss_cate: 0.1221  loss_mask: 0.5967  loss_dice: 0.8322  loss_ce_0: 0.7527  loss_cate_0: 0  loss_mask_0: 0.6285  loss_dice_0: 0.9303  loss_ce_1: 0.4933  loss_cate_1: 0  loss_mask_1: 0.624  loss_dice_1: 0.8951  loss_ce_2: 0.5082  loss_cate_2: 0  loss_mask_2: 0.6298  loss_dice_2: 0.8344  loss_ce_3: 0.3927  loss_cate_3: 0  loss_mask_3: 0.6289  loss_dice_3: 0.8712  loss_ce_4: 0.3541  loss_cate_4: 0  loss_mask_4: 0.611  loss_dice_4: 0.8843  loss_ce_5: 0.3816  loss_cate_5: 0  loss_mask_5: 0.5995  loss_dice_5: 0.8933  loss_ce_6: 0.4843  loss_cate_6: 0  loss_mask_6: 0.6  loss_dice_6: 0.8078  loss_ce_7: 0.4362  loss_cate_7: 0  loss_mask_7: 0.5922  loss_dice_7: 0.8294  loss_ce_8: 0.4496  loss_cate_8: 0  loss_mask_8: 0.6169  loss_dice_8: 0.8416  time: 1.3900  data_time: 0.0111  lr: 9.0162e-06  max_mem: 23815M
[01/01 12:14:42] d2.utils.events INFO:  eta: 13:20:10  iter: 4259  total_loss: 19.79  loss_ce: 0.4414  loss_cate: 0.1421  loss_mask: 0.7213  loss_dice: 0.8423  loss_ce_0: 0.734  loss_cate_0: 0  loss_mask_0: 0.7363  loss_dice_0: 0.8715  loss_ce_1: 0.494  loss_cate_1: 0  loss_mask_1: 0.7668  loss_dice_1: 0.7951  loss_ce_2: 0.4684  loss_cate_2: 0  loss_mask_2: 0.7343  loss_dice_2: 0.8146  loss_ce_3: 0.4301  loss_cate_3: 0  loss_mask_3: 0.741  loss_dice_3: 0.8608  loss_ce_4: 0.5117  loss_cate_4: 0  loss_mask_4: 0.7496  loss_dice_4: 0.8096  loss_ce_5: 0.4458  loss_cate_5: 0  loss_mask_5: 0.7257  loss_dice_5: 0.849  loss_ce_6: 0.4425  loss_cate_6: 0  loss_mask_6: 0.7223  loss_dice_6: 0.8399  loss_ce_7: 0.4432  loss_cate_7: 0  loss_mask_7: 0.7116  loss_dice_7: 0.8252  loss_ce_8: 0.4539  loss_cate_8: 0  loss_mask_8: 0.7122  loss_dice_8: 0.8274  time: 1.3900  data_time: 0.0161  lr: 9.0116e-06  max_mem: 23815M
[01/01 12:15:10] d2.utils.events INFO:  eta: 13:19:38  iter: 4279  total_loss: 22.25  loss_ce: 0.47  loss_cate: 0.1304  loss_mask: 0.5897  loss_dice: 0.8694  loss_ce_0: 0.8723  loss_cate_0: 0  loss_mask_0: 0.6029  loss_dice_0: 0.8604  loss_ce_1: 0.4961  loss_cate_1: 0  loss_mask_1: 0.5893  loss_dice_1: 0.8897  loss_ce_2: 0.443  loss_cate_2: 0  loss_mask_2: 0.5558  loss_dice_2: 0.9221  loss_ce_3: 0.4455  loss_cate_3: 0  loss_mask_3: 0.5687  loss_dice_3: 0.8866  loss_ce_4: 0.4438  loss_cate_4: 0  loss_mask_4: 0.5501  loss_dice_4: 0.9153  loss_ce_5: 0.4328  loss_cate_5: 0  loss_mask_5: 0.6052  loss_dice_5: 0.9636  loss_ce_6: 0.4749  loss_cate_6: 0  loss_mask_6: 0.6031  loss_dice_6: 0.929  loss_ce_7: 0.5127  loss_cate_7: 0  loss_mask_7: 0.6031  loss_dice_7: 0.8599  loss_ce_8: 0.4748  loss_cate_8: 0  loss_mask_8: 0.5934  loss_dice_8: 0.8853  time: 1.3900  data_time: 0.0152  lr: 9.0069e-06  max_mem: 23815M
[01/01 12:15:37] d2.utils.events INFO:  eta: 13:19:03  iter: 4299  total_loss: 20.87  loss_ce: 0.5926  loss_cate: 0.1302  loss_mask: 0.5726  loss_dice: 0.9662  loss_ce_0: 0.8955  loss_cate_0: 0  loss_mask_0: 0.4805  loss_dice_0: 0.9054  loss_ce_1: 0.5589  loss_cate_1: 0  loss_mask_1: 0.5967  loss_dice_1: 0.989  loss_ce_2: 0.4678  loss_cate_2: 0  loss_mask_2: 0.5486  loss_dice_2: 0.9498  loss_ce_3: 0.5674  loss_cate_3: 0  loss_mask_3: 0.5631  loss_dice_3: 0.9614  loss_ce_4: 0.5181  loss_cate_4: 0  loss_mask_4: 0.5616  loss_dice_4: 0.9536  loss_ce_5: 0.4871  loss_cate_5: 0  loss_mask_5: 0.5961  loss_dice_5: 0.9413  loss_ce_6: 0.5394  loss_cate_6: 0  loss_mask_6: 0.5133  loss_dice_6: 0.9487  loss_ce_7: 0.5406  loss_cate_7: 0  loss_mask_7: 0.5937  loss_dice_7: 0.9819  loss_ce_8: 0.554  loss_cate_8: 0  loss_mask_8: 0.5798  loss_dice_8: 0.9136  time: 1.3899  data_time: 0.0110  lr: 9.0022e-06  max_mem: 23815M
[01/01 12:16:05] d2.utils.events INFO:  eta: 13:18:21  iter: 4319  total_loss: 20.81  loss_ce: 0.515  loss_cate: 0.1838  loss_mask: 0.6135  loss_dice: 0.8474  loss_ce_0: 0.9762  loss_cate_0: 0  loss_mask_0: 0.5365  loss_dice_0: 0.8615  loss_ce_1: 0.632  loss_cate_1: 0  loss_mask_1: 0.5935  loss_dice_1: 0.8866  loss_ce_2: 0.5996  loss_cate_2: 0  loss_mask_2: 0.5811  loss_dice_2: 0.8349  loss_ce_3: 0.5664  loss_cate_3: 0  loss_mask_3: 0.5694  loss_dice_3: 0.8733  loss_ce_4: 0.5445  loss_cate_4: 0  loss_mask_4: 0.6121  loss_dice_4: 0.8736  loss_ce_5: 0.5473  loss_cate_5: 0  loss_mask_5: 0.6204  loss_dice_5: 0.7944  loss_ce_6: 0.5659  loss_cate_6: 0  loss_mask_6: 0.6069  loss_dice_6: 0.8229  loss_ce_7: 0.5509  loss_cate_7: 0  loss_mask_7: 0.63  loss_dice_7: 0.8438  loss_ce_8: 0.55  loss_cate_8: 0  loss_mask_8: 0.6298  loss_dice_8: 0.8267  time: 1.3898  data_time: 0.0111  lr: 8.9976e-06  max_mem: 23815M
[01/01 12:16:32] d2.utils.events INFO:  eta: 13:17:50  iter: 4339  total_loss: 19.07  loss_ce: 0.4853  loss_cate: 0.1166  loss_mask: 0.499  loss_dice: 0.7223  loss_ce_0: 0.7745  loss_cate_0: 0  loss_mask_0: 0.5087  loss_dice_0: 0.6559  loss_ce_1: 0.4902  loss_cate_1: 0  loss_mask_1: 0.5114  loss_dice_1: 0.7689  loss_ce_2: 0.494  loss_cate_2: 0  loss_mask_2: 0.5242  loss_dice_2: 0.7451  loss_ce_3: 0.4994  loss_cate_3: 0  loss_mask_3: 0.471  loss_dice_3: 0.7198  loss_ce_4: 0.5018  loss_cate_4: 0  loss_mask_4: 0.4842  loss_dice_4: 0.6311  loss_ce_5: 0.415  loss_cate_5: 0  loss_mask_5: 0.4693  loss_dice_5: 0.7147  loss_ce_6: 0.5571  loss_cate_6: 0  loss_mask_6: 0.4998  loss_dice_6: 0.6691  loss_ce_7: 0.5025  loss_cate_7: 0  loss_mask_7: 0.4971  loss_dice_7: 0.7838  loss_ce_8: 0.5056  loss_cate_8: 0  loss_mask_8: 0.4862  loss_dice_8: 0.7752  time: 1.3898  data_time: 0.0113  lr: 8.9929e-06  max_mem: 23815M
[01/01 12:17:00] d2.utils.events INFO:  eta: 13:17:10  iter: 4359  total_loss: 19.17  loss_ce: 0.5172  loss_cate: 0.1178  loss_mask: 0.5136  loss_dice: 0.7642  loss_ce_0: 0.7612  loss_cate_0: 0  loss_mask_0: 0.5742  loss_dice_0: 0.7436  loss_ce_1: 0.5717  loss_cate_1: 0  loss_mask_1: 0.5025  loss_dice_1: 0.7244  loss_ce_2: 0.5152  loss_cate_2: 0  loss_mask_2: 0.5292  loss_dice_2: 0.8061  loss_ce_3: 0.5555  loss_cate_3: 0  loss_mask_3: 0.5595  loss_dice_3: 0.7714  loss_ce_4: 0.4989  loss_cate_4: 0  loss_mask_4: 0.5616  loss_dice_4: 0.7733  loss_ce_5: 0.4584  loss_cate_5: 0  loss_mask_5: 0.5676  loss_dice_5: 0.8061  loss_ce_6: 0.4833  loss_cate_6: 0  loss_mask_6: 0.5397  loss_dice_6: 0.8447  loss_ce_7: 0.4832  loss_cate_7: 0  loss_mask_7: 0.5445  loss_dice_7: 0.8202  loss_ce_8: 0.5054  loss_cate_8: 0  loss_mask_8: 0.5496  loss_dice_8: 0.7419  time: 1.3897  data_time: 0.0118  lr: 8.9882e-06  max_mem: 23815M
[01/01 12:17:27] d2.utils.events INFO:  eta: 13:16:40  iter: 4379  total_loss: 18.46  loss_ce: 0.5258  loss_cate: 0.1193  loss_mask: 0.4892  loss_dice: 0.7598  loss_ce_0: 0.8  loss_cate_0: 0  loss_mask_0: 0.5176  loss_dice_0: 0.818  loss_ce_1: 0.4893  loss_cate_1: 0  loss_mask_1: 0.4981  loss_dice_1: 0.7566  loss_ce_2: 0.5091  loss_cate_2: 0  loss_mask_2: 0.5441  loss_dice_2: 0.6801  loss_ce_3: 0.485  loss_cate_3: 0  loss_mask_3: 0.5126  loss_dice_3: 0.736  loss_ce_4: 0.5028  loss_cate_4: 0  loss_mask_4: 0.523  loss_dice_4: 0.761  loss_ce_5: 0.5294  loss_cate_5: 0  loss_mask_5: 0.5558  loss_dice_5: 0.7905  loss_ce_6: 0.4987  loss_cate_6: 0  loss_mask_6: 0.5364  loss_dice_6: 0.7528  loss_ce_7: 0.5206  loss_cate_7: 0  loss_mask_7: 0.5478  loss_dice_7: 0.7406  loss_ce_8: 0.5056  loss_cate_8: 0  loss_mask_8: 0.5867  loss_dice_8: 0.7667  time: 1.3896  data_time: 0.0115  lr: 8.9835e-06  max_mem: 23815M
[01/01 12:17:55] d2.utils.events INFO:  eta: 13:16:11  iter: 4399  total_loss: 21.71  loss_ce: 0.5206  loss_cate: 0.1802  loss_mask: 0.6937  loss_dice: 0.9435  loss_ce_0: 0.8932  loss_cate_0: 0  loss_mask_0: 0.6455  loss_dice_0: 0.9257  loss_ce_1: 0.6253  loss_cate_1: 0  loss_mask_1: 0.6848  loss_dice_1: 0.9165  loss_ce_2: 0.561  loss_cate_2: 0  loss_mask_2: 0.7013  loss_dice_2: 0.9438  loss_ce_3: 0.5284  loss_cate_3: 0  loss_mask_3: 0.657  loss_dice_3: 0.9382  loss_ce_4: 0.4922  loss_cate_4: 0  loss_mask_4: 0.6958  loss_dice_4: 0.9232  loss_ce_5: 0.5169  loss_cate_5: 0  loss_mask_5: 0.6542  loss_dice_5: 0.9308  loss_ce_6: 0.5449  loss_cate_6: 0  loss_mask_6: 0.698  loss_dice_6: 0.873  loss_ce_7: 0.5168  loss_cate_7: 0  loss_mask_7: 0.6737  loss_dice_7: 0.9192  loss_ce_8: 0.4918  loss_cate_8: 0  loss_mask_8: 0.6631  loss_dice_8: 0.9184  time: 1.3895  data_time: 0.0118  lr: 8.9789e-06  max_mem: 23815M
[01/01 12:18:22] d2.utils.events INFO:  eta: 13:15:29  iter: 4419  total_loss: 21.39  loss_ce: 0.3787  loss_cate: 0.1244  loss_mask: 0.5681  loss_dice: 0.9071  loss_ce_0: 0.8767  loss_cate_0: 0  loss_mask_0: 0.6243  loss_dice_0: 0.884  loss_ce_1: 0.5676  loss_cate_1: 0  loss_mask_1: 0.5936  loss_dice_1: 0.806  loss_ce_2: 0.5004  loss_cate_2: 0  loss_mask_2: 0.6146  loss_dice_2: 0.9146  loss_ce_3: 0.4948  loss_cate_3: 0  loss_mask_3: 0.5864  loss_dice_3: 0.9547  loss_ce_4: 0.4405  loss_cate_4: 0  loss_mask_4: 0.5684  loss_dice_4: 0.8287  loss_ce_5: 0.479  loss_cate_5: 0  loss_mask_5: 0.5611  loss_dice_5: 0.8506  loss_ce_6: 0.3709  loss_cate_6: 0  loss_mask_6: 0.5828  loss_dice_6: 0.8571  loss_ce_7: 0.4492  loss_cate_7: 0  loss_mask_7: 0.5507  loss_dice_7: 0.8634  loss_ce_8: 0.4343  loss_cate_8: 0  loss_mask_8: 0.5597  loss_dice_8: 0.8686  time: 1.3894  data_time: 0.0104  lr: 8.9742e-06  max_mem: 23815M
[01/01 12:18:50] d2.utils.events INFO:  eta: 13:14:56  iter: 4439  total_loss: 19.9  loss_ce: 0.4727  loss_cate: 0.1576  loss_mask: 0.5656  loss_dice: 0.7714  loss_ce_0: 0.781  loss_cate_0: 0  loss_mask_0: 0.531  loss_dice_0: 0.8994  loss_ce_1: 0.4581  loss_cate_1: 0  loss_mask_1: 0.555  loss_dice_1: 0.8287  loss_ce_2: 0.4466  loss_cate_2: 0  loss_mask_2: 0.5291  loss_dice_2: 0.7782  loss_ce_3: 0.4307  loss_cate_3: 0  loss_mask_3: 0.5408  loss_dice_3: 0.8242  loss_ce_4: 0.4539  loss_cate_4: 0  loss_mask_4: 0.5735  loss_dice_4: 0.7796  loss_ce_5: 0.4186  loss_cate_5: 0  loss_mask_5: 0.5041  loss_dice_5: 0.8173  loss_ce_6: 0.45  loss_cate_6: 0  loss_mask_6: 0.5327  loss_dice_6: 0.753  loss_ce_7: 0.4458  loss_cate_7: 0  loss_mask_7: 0.5606  loss_dice_7: 0.733  loss_ce_8: 0.4506  loss_cate_8: 0  loss_mask_8: 0.5428  loss_dice_8: 0.7716  time: 1.3893  data_time: 0.0114  lr: 8.9695e-06  max_mem: 23815M
[01/01 12:19:17] d2.utils.events INFO:  eta: 13:14:19  iter: 4459  total_loss: 18.52  loss_ce: 0.4294  loss_cate: 0.1134  loss_mask: 0.5466  loss_dice: 0.7995  loss_ce_0: 0.7865  loss_cate_0: 0  loss_mask_0: 0.4783  loss_dice_0: 0.8088  loss_ce_1: 0.4691  loss_cate_1: 0  loss_mask_1: 0.4971  loss_dice_1: 0.818  loss_ce_2: 0.3809  loss_cate_2: 0  loss_mask_2: 0.5382  loss_dice_2: 0.8196  loss_ce_3: 0.4062  loss_cate_3: 0  loss_mask_3: 0.5569  loss_dice_3: 0.8023  loss_ce_4: 0.4271  loss_cate_4: 0  loss_mask_4: 0.6039  loss_dice_4: 0.835  loss_ce_5: 0.4657  loss_cate_5: 0  loss_mask_5: 0.5631  loss_dice_5: 0.8856  loss_ce_6: 0.4562  loss_cate_6: 0  loss_mask_6: 0.5383  loss_dice_6: 0.8414  loss_ce_7: 0.4082  loss_cate_7: 0  loss_mask_7: 0.4963  loss_dice_7: 0.8107  loss_ce_8: 0.4419  loss_cate_8: 0  loss_mask_8: 0.465  loss_dice_8: 0.7007  time: 1.3893  data_time: 0.0113  lr: 8.9649e-06  max_mem: 23815M
[01/01 12:19:45] d2.utils.events INFO:  eta: 13:13:51  iter: 4479  total_loss: 23.43  loss_ce: 0.5661  loss_cate: 0.179  loss_mask: 0.5217  loss_dice: 1.027  loss_ce_0: 0.906  loss_cate_0: 0  loss_mask_0: 0.5939  loss_dice_0: 0.9627  loss_ce_1: 0.5846  loss_cate_1: 0  loss_mask_1: 0.5201  loss_dice_1: 0.9701  loss_ce_2: 0.5983  loss_cate_2: 0  loss_mask_2: 0.5131  loss_dice_2: 0.9379  loss_ce_3: 0.5546  loss_cate_3: 0  loss_mask_3: 0.5337  loss_dice_3: 0.9527  loss_ce_4: 0.639  loss_cate_4: 0  loss_mask_4: 0.5292  loss_dice_4: 0.9678  loss_ce_5: 0.5648  loss_cate_5: 0  loss_mask_5: 0.5483  loss_dice_5: 1.012  loss_ce_6: 0.5639  loss_cate_6: 0  loss_mask_6: 0.5229  loss_dice_6: 0.9994  loss_ce_7: 0.6176  loss_cate_7: 0  loss_mask_7: 0.5395  loss_dice_7: 0.9905  loss_ce_8: 0.5267  loss_cate_8: 0  loss_mask_8: 0.5508  loss_dice_8: 0.9984  time: 1.3892  data_time: 0.0125  lr: 8.9602e-06  max_mem: 23815M
[01/01 12:20:12] d2.utils.events INFO:  eta: 13:13:16  iter: 4499  total_loss: 15.95  loss_ce: 0.4081  loss_cate: 0.1251  loss_mask: 0.5308  loss_dice: 0.6166  loss_ce_0: 0.7089  loss_cate_0: 0  loss_mask_0: 0.5826  loss_dice_0: 0.6085  loss_ce_1: 0.4709  loss_cate_1: 0  loss_mask_1: 0.5171  loss_dice_1: 0.6  loss_ce_2: 0.432  loss_cate_2: 0  loss_mask_2: 0.5842  loss_dice_2: 0.5817  loss_ce_3: 0.4466  loss_cate_3: 0  loss_mask_3: 0.5582  loss_dice_3: 0.617  loss_ce_4: 0.419  loss_cate_4: 0  loss_mask_4: 0.5023  loss_dice_4: 0.6328  loss_ce_5: 0.3796  loss_cate_5: 0  loss_mask_5: 0.5166  loss_dice_5: 0.6362  loss_ce_6: 0.3521  loss_cate_6: 0  loss_mask_6: 0.5397  loss_dice_6: 0.5967  loss_ce_7: 0.3879  loss_cate_7: 0  loss_mask_7: 0.5359  loss_dice_7: 0.609  loss_ce_8: 0.4268  loss_cate_8: 0  loss_mask_8: 0.5402  loss_dice_8: 0.6073  time: 1.3892  data_time: 0.0150  lr: 8.9555e-06  max_mem: 23815M
[01/01 12:20:40] d2.utils.events INFO:  eta: 13:12:44  iter: 4519  total_loss: 16.8  loss_ce: 0.5301  loss_cate: 0.1019  loss_mask: 0.4082  loss_dice: 0.575  loss_ce_0: 0.7481  loss_cate_0: 0  loss_mask_0: 0.4446  loss_dice_0: 0.5929  loss_ce_1: 0.4504  loss_cate_1: 0  loss_mask_1: 0.4322  loss_dice_1: 0.6281  loss_ce_2: 0.5128  loss_cate_2: 0  loss_mask_2: 0.4517  loss_dice_2: 0.5747  loss_ce_3: 0.4578  loss_cate_3: 0  loss_mask_3: 0.4484  loss_dice_3: 0.6692  loss_ce_4: 0.4095  loss_cate_4: 0  loss_mask_4: 0.4356  loss_dice_4: 0.6112  loss_ce_5: 0.4724  loss_cate_5: 0  loss_mask_5: 0.4437  loss_dice_5: 0.608  loss_ce_6: 0.4917  loss_cate_6: 0  loss_mask_6: 0.4608  loss_dice_6: 0.6436  loss_ce_7: 0.4881  loss_cate_7: 0  loss_mask_7: 0.4549  loss_dice_7: 0.6291  loss_ce_8: 0.4755  loss_cate_8: 0  loss_mask_8: 0.4316  loss_dice_8: 0.6178  time: 1.3891  data_time: 0.0129  lr: 8.9508e-06  max_mem: 23815M
[01/01 12:21:08] d2.utils.events INFO:  eta: 13:12:14  iter: 4539  total_loss: 18.8  loss_ce: 0.4083  loss_cate: 0.1259  loss_mask: 0.6264  loss_dice: 0.776  loss_ce_0: 0.768  loss_cate_0: 0  loss_mask_0: 0.5891  loss_dice_0: 0.8156  loss_ce_1: 0.4858  loss_cate_1: 0  loss_mask_1: 0.673  loss_dice_1: 0.7494  loss_ce_2: 0.4417  loss_cate_2: 0  loss_mask_2: 0.634  loss_dice_2: 0.8907  loss_ce_3: 0.4307  loss_cate_3: 0  loss_mask_3: 0.6081  loss_dice_3: 0.7535  loss_ce_4: 0.4572  loss_cate_4: 0  loss_mask_4: 0.5958  loss_dice_4: 0.7152  loss_ce_5: 0.4303  loss_cate_5: 0  loss_mask_5: 0.6187  loss_dice_5: 0.7875  loss_ce_6: 0.5238  loss_cate_6: 0  loss_mask_6: 0.6539  loss_dice_6: 0.7491  loss_ce_7: 0.5044  loss_cate_7: 0  loss_mask_7: 0.5756  loss_dice_7: 0.7765  loss_ce_8: 0.4635  loss_cate_8: 0  loss_mask_8: 0.609  loss_dice_8: 0.7837  time: 1.3891  data_time: 0.0127  lr: 8.9462e-06  max_mem: 23815M
[01/01 12:21:35] d2.utils.events INFO:  eta: 13:11:41  iter: 4559  total_loss: 21.24  loss_ce: 0.4722  loss_cate: 0.1302  loss_mask: 0.6964  loss_dice: 0.8749  loss_ce_0: 0.6773  loss_cate_0: 0  loss_mask_0: 0.6812  loss_dice_0: 0.8156  loss_ce_1: 0.4883  loss_cate_1: 0  loss_mask_1: 0.7539  loss_dice_1: 0.8714  loss_ce_2: 0.4583  loss_cate_2: 0  loss_mask_2: 0.6676  loss_dice_2: 0.8582  loss_ce_3: 0.446  loss_cate_3: 0  loss_mask_3: 0.648  loss_dice_3: 0.819  loss_ce_4: 0.4617  loss_cate_4: 0  loss_mask_4: 0.6763  loss_dice_4: 0.8395  loss_ce_5: 0.4495  loss_cate_5: 0  loss_mask_5: 0.6981  loss_dice_5: 0.8953  loss_ce_6: 0.4559  loss_cate_6: 0  loss_mask_6: 0.6902  loss_dice_6: 0.842  loss_ce_7: 0.4696  loss_cate_7: 0  loss_mask_7: 0.6967  loss_dice_7: 0.8057  loss_ce_8: 0.5009  loss_cate_8: 0  loss_mask_8: 0.5607  loss_dice_8: 0.8259  time: 1.3890  data_time: 0.0127  lr: 8.9415e-06  max_mem: 23815M
[01/01 12:22:03] d2.utils.events INFO:  eta: 13:11:11  iter: 4579  total_loss: 19.14  loss_ce: 0.3907  loss_cate: 0.1296  loss_mask: 0.5839  loss_dice: 0.9206  loss_ce_0: 0.7876  loss_cate_0: 0  loss_mask_0: 0.6586  loss_dice_0: 0.8777  loss_ce_1: 0.5634  loss_cate_1: 0  loss_mask_1: 0.6122  loss_dice_1: 0.8333  loss_ce_2: 0.3924  loss_cate_2: 0  loss_mask_2: 0.5898  loss_dice_2: 0.881  loss_ce_3: 0.4588  loss_cate_3: 0  loss_mask_3: 0.5576  loss_dice_3: 0.9148  loss_ce_4: 0.39  loss_cate_4: 0  loss_mask_4: 0.5925  loss_dice_4: 0.9047  loss_ce_5: 0.4667  loss_cate_5: 0  loss_mask_5: 0.5842  loss_dice_5: 0.8513  loss_ce_6: 0.5144  loss_cate_6: 0  loss_mask_6: 0.5937  loss_dice_6: 0.8503  loss_ce_7: 0.4629  loss_cate_7: 0  loss_mask_7: 0.5935  loss_dice_7: 0.897  loss_ce_8: 0.4551  loss_cate_8: 0  loss_mask_8: 0.5961  loss_dice_8: 0.8881  time: 1.3891  data_time: 0.0136  lr: 8.9368e-06  max_mem: 23815M
[01/01 12:22:31] d2.utils.events INFO:  eta: 13:10:36  iter: 4599  total_loss: 17.1  loss_ce: 0.4739  loss_cate: 0.1191  loss_mask: 0.4361  loss_dice: 0.761  loss_ce_0: 0.7491  loss_cate_0: 0  loss_mask_0: 0.4177  loss_dice_0: 0.7278  loss_ce_1: 0.47  loss_cate_1: 0  loss_mask_1: 0.4373  loss_dice_1: 0.7675  loss_ce_2: 0.4891  loss_cate_2: 0  loss_mask_2: 0.4087  loss_dice_2: 0.7693  loss_ce_3: 0.4396  loss_cate_3: 0  loss_mask_3: 0.415  loss_dice_3: 0.773  loss_ce_4: 0.4344  loss_cate_4: 0  loss_mask_4: 0.4135  loss_dice_4: 0.7266  loss_ce_5: 0.4514  loss_cate_5: 0  loss_mask_5: 0.4447  loss_dice_5: 0.7828  loss_ce_6: 0.4789  loss_cate_6: 0  loss_mask_6: 0.4135  loss_dice_6: 0.7714  loss_ce_7: 0.5036  loss_cate_7: 0  loss_mask_7: 0.419  loss_dice_7: 0.7717  loss_ce_8: 0.5002  loss_cate_8: 0  loss_mask_8: 0.4128  loss_dice_8: 0.7682  time: 1.3890  data_time: 0.0113  lr: 8.9321e-06  max_mem: 23815M
[01/01 12:22:59] d2.utils.events INFO:  eta: 13:10:06  iter: 4619  total_loss: 19.84  loss_ce: 0.4683  loss_cate: 0.1114  loss_mask: 0.501  loss_dice: 0.9059  loss_ce_0: 0.8854  loss_cate_0: 0  loss_mask_0: 0.4368  loss_dice_0: 0.8755  loss_ce_1: 0.5585  loss_cate_1: 0  loss_mask_1: 0.5228  loss_dice_1: 0.8945  loss_ce_2: 0.5042  loss_cate_2: 0  loss_mask_2: 0.4919  loss_dice_2: 0.8916  loss_ce_3: 0.46  loss_cate_3: 0  loss_mask_3: 0.5  loss_dice_3: 0.852  loss_ce_4: 0.5154  loss_cate_4: 0  loss_mask_4: 0.48  loss_dice_4: 0.9094  loss_ce_5: 0.4856  loss_cate_5: 0  loss_mask_5: 0.4572  loss_dice_5: 0.8652  loss_ce_6: 0.4749  loss_cate_6: 0  loss_mask_6: 0.478  loss_dice_6: 0.8351  loss_ce_7: 0.4514  loss_cate_7: 0  loss_mask_7: 0.4949  loss_dice_7: 0.8891  loss_ce_8: 0.4702  loss_cate_8: 0  loss_mask_8: 0.4923  loss_dice_8: 0.8714  time: 1.3890  data_time: 0.0132  lr: 8.9275e-06  max_mem: 23815M
[01/01 12:23:27] d2.utils.events INFO:  eta: 13:09:36  iter: 4639  total_loss: 21.98  loss_ce: 0.5566  loss_cate: 0.1614  loss_mask: 0.6148  loss_dice: 0.9185  loss_ce_0: 0.8522  loss_cate_0: 0  loss_mask_0: 0.5791  loss_dice_0: 0.8532  loss_ce_1: 0.5712  loss_cate_1: 0  loss_mask_1: 0.5908  loss_dice_1: 0.8271  loss_ce_2: 0.6687  loss_cate_2: 0  loss_mask_2: 0.6097  loss_dice_2: 0.8927  loss_ce_3: 0.5814  loss_cate_3: 0  loss_mask_3: 0.7042  loss_dice_3: 0.9032  loss_ce_4: 0.5423  loss_cate_4: 0  loss_mask_4: 0.623  loss_dice_4: 0.8807  loss_ce_5: 0.5523  loss_cate_5: 0  loss_mask_5: 0.6657  loss_dice_5: 0.9104  loss_ce_6: 0.5749  loss_cate_6: 0  loss_mask_6: 0.6355  loss_dice_6: 0.8943  loss_ce_7: 0.5727  loss_cate_7: 0  loss_mask_7: 0.638  loss_dice_7: 0.8821  loss_ce_8: 0.4875  loss_cate_8: 0  loss_mask_8: 0.6535  loss_dice_8: 0.9074  time: 1.3891  data_time: 0.0128  lr: 8.9228e-06  max_mem: 23815M
[01/01 12:23:55] d2.utils.events INFO:  eta: 13:09:10  iter: 4659  total_loss: 19.87  loss_ce: 0.2879  loss_cate: 0.1288  loss_mask: 0.5952  loss_dice: 0.7801  loss_ce_0: 0.7158  loss_cate_0: 0  loss_mask_0: 0.5886  loss_dice_0: 0.6838  loss_ce_1: 0.452  loss_cate_1: 0  loss_mask_1: 0.5839  loss_dice_1: 0.7057  loss_ce_2: 0.3662  loss_cate_2: 0  loss_mask_2: 0.5301  loss_dice_2: 0.7089  loss_ce_3: 0.3057  loss_cate_3: 0  loss_mask_3: 0.5642  loss_dice_3: 0.6588  loss_ce_4: 0.3221  loss_cate_4: 0  loss_mask_4: 0.5312  loss_dice_4: 0.7855  loss_ce_5: 0.3419  loss_cate_5: 0  loss_mask_5: 0.5557  loss_dice_5: 0.8039  loss_ce_6: 0.3108  loss_cate_6: 0  loss_mask_6: 0.55  loss_dice_6: 0.8114  loss_ce_7: 0.345  loss_cate_7: 0  loss_mask_7: 0.5784  loss_dice_7: 0.801  loss_ce_8: 0.3043  loss_cate_8: 0  loss_mask_8: 0.5794  loss_dice_8: 0.7954  time: 1.3891  data_time: 0.0138  lr: 8.9181e-06  max_mem: 23815M
[01/01 12:24:22] d2.utils.events INFO:  eta: 13:08:38  iter: 4679  total_loss: 20.4  loss_ce: 0.4373  loss_cate: 0.1267  loss_mask: 0.5083  loss_dice: 0.7296  loss_ce_0: 0.9019  loss_cate_0: 0  loss_mask_0: 0.4697  loss_dice_0: 0.7176  loss_ce_1: 0.4957  loss_cate_1: 0  loss_mask_1: 0.4896  loss_dice_1: 0.7082  loss_ce_2: 0.5133  loss_cate_2: 0  loss_mask_2: 0.4247  loss_dice_2: 0.7125  loss_ce_3: 0.4345  loss_cate_3: 0  loss_mask_3: 0.4401  loss_dice_3: 0.7665  loss_ce_4: 0.4296  loss_cate_4: 0  loss_mask_4: 0.4625  loss_dice_4: 0.8042  loss_ce_5: 0.4327  loss_cate_5: 0  loss_mask_5: 0.4541  loss_dice_5: 0.7718  loss_ce_6: 0.4228  loss_cate_6: 0  loss_mask_6: 0.4492  loss_dice_6: 0.7591  loss_ce_7: 0.4478  loss_cate_7: 0  loss_mask_7: 0.4695  loss_dice_7: 0.7826  loss_ce_8: 0.4256  loss_cate_8: 0  loss_mask_8: 0.4725  loss_dice_8: 0.7461  time: 1.3890  data_time: 0.0106  lr: 8.9134e-06  max_mem: 23815M
[01/01 12:24:50] d2.utils.events INFO:  eta: 13:07:58  iter: 4699  total_loss: 16.04  loss_ce: 0.3649  loss_cate: 0.1121  loss_mask: 0.5393  loss_dice: 0.5925  loss_ce_0: 0.6254  loss_cate_0: 0  loss_mask_0: 0.5228  loss_dice_0: 0.6344  loss_ce_1: 0.3597  loss_cate_1: 0  loss_mask_1: 0.6098  loss_dice_1: 0.556  loss_ce_2: 0.3583  loss_cate_2: 0  loss_mask_2: 0.5105  loss_dice_2: 0.5991  loss_ce_3: 0.3188  loss_cate_3: 0  loss_mask_3: 0.531  loss_dice_3: 0.5934  loss_ce_4: 0.3098  loss_cate_4: 0  loss_mask_4: 0.5162  loss_dice_4: 0.5963  loss_ce_5: 0.3288  loss_cate_5: 0  loss_mask_5: 0.504  loss_dice_5: 0.6437  loss_ce_6: 0.3369  loss_cate_6: 0  loss_mask_6: 0.4998  loss_dice_6: 0.6263  loss_ce_7: 0.2964  loss_cate_7: 0  loss_mask_7: 0.5338  loss_dice_7: 0.6257  loss_ce_8: 0.3412  loss_cate_8: 0  loss_mask_8: 0.5598  loss_dice_8: 0.6398  time: 1.3889  data_time: 0.0104  lr: 8.9088e-06  max_mem: 23815M
[01/01 12:25:17] d2.utils.events INFO:  eta: 13:07:21  iter: 4719  total_loss: 18.91  loss_ce: 0.5121  loss_cate: 0.1278  loss_mask: 0.5806  loss_dice: 0.7148  loss_ce_0: 0.8789  loss_cate_0: 0  loss_mask_0: 0.603  loss_dice_0: 0.7371  loss_ce_1: 0.5032  loss_cate_1: 0  loss_mask_1: 0.5977  loss_dice_1: 0.6854  loss_ce_2: 0.4305  loss_cate_2: 0  loss_mask_2: 0.5665  loss_dice_2: 0.6762  loss_ce_3: 0.4171  loss_cate_3: 0  loss_mask_3: 0.5939  loss_dice_3: 0.7499  loss_ce_4: 0.4264  loss_cate_4: 0  loss_mask_4: 0.5632  loss_dice_4: 0.7814  loss_ce_5: 0.3977  loss_cate_5: 0  loss_mask_5: 0.6074  loss_dice_5: 0.7049  loss_ce_6: 0.4021  loss_cate_6: 0  loss_mask_6: 0.5727  loss_dice_6: 0.7158  loss_ce_7: 0.4278  loss_cate_7: 0  loss_mask_7: 0.5914  loss_dice_7: 0.7244  loss_ce_8: 0.4254  loss_cate_8: 0  loss_mask_8: 0.5936  loss_dice_8: 0.7055  time: 1.3889  data_time: 0.0121  lr: 8.9041e-06  max_mem: 23815M
[01/01 12:25:44] d2.utils.events INFO:  eta: 13:06:46  iter: 4739  total_loss: 21.29  loss_ce: 0.4652  loss_cate: 0.1077  loss_mask: 0.6374  loss_dice: 0.789  loss_ce_0: 0.7837  loss_cate_0: 0  loss_mask_0: 0.6344  loss_dice_0: 0.8818  loss_ce_1: 0.6251  loss_cate_1: 0  loss_mask_1: 0.7175  loss_dice_1: 0.8105  loss_ce_2: 0.4782  loss_cate_2: 0  loss_mask_2: 0.6528  loss_dice_2: 0.8103  loss_ce_3: 0.5473  loss_cate_3: 0  loss_mask_3: 0.6166  loss_dice_3: 0.839  loss_ce_4: 0.5239  loss_cate_4: 0  loss_mask_4: 0.6234  loss_dice_4: 0.8437  loss_ce_5: 0.4611  loss_cate_5: 0  loss_mask_5: 0.6032  loss_dice_5: 0.8329  loss_ce_6: 0.431  loss_cate_6: 0  loss_mask_6: 0.6039  loss_dice_6: 0.806  loss_ce_7: 0.511  loss_cate_7: 0  loss_mask_7: 0.5855  loss_dice_7: 0.7615  loss_ce_8: 0.5043  loss_cate_8: 0  loss_mask_8: 0.6118  loss_dice_8: 0.8154  time: 1.3888  data_time: 0.0107  lr: 8.8994e-06  max_mem: 23815M
[01/01 12:26:12] d2.utils.events INFO:  eta: 13:06:01  iter: 4759  total_loss: 21.71  loss_ce: 0.5129  loss_cate: 0.1206  loss_mask: 0.5603  loss_dice: 0.9577  loss_ce_0: 0.8285  loss_cate_0: 0  loss_mask_0: 0.5433  loss_dice_0: 1.02  loss_ce_1: 0.5169  loss_cate_1: 0  loss_mask_1: 0.5433  loss_dice_1: 0.9719  loss_ce_2: 0.4885  loss_cate_2: 0  loss_mask_2: 0.5411  loss_dice_2: 0.8936  loss_ce_3: 0.5361  loss_cate_3: 0  loss_mask_3: 0.5379  loss_dice_3: 0.9531  loss_ce_4: 0.5379  loss_cate_4: 0  loss_mask_4: 0.5568  loss_dice_4: 0.916  loss_ce_5: 0.4863  loss_cate_5: 0  loss_mask_5: 0.5473  loss_dice_5: 0.9075  loss_ce_6: 0.4541  loss_cate_6: 0  loss_mask_6: 0.5144  loss_dice_6: 0.8991  loss_ce_7: 0.4836  loss_cate_7: 0  loss_mask_7: 0.5205  loss_dice_7: 0.9518  loss_ce_8: 0.4986  loss_cate_8: 0  loss_mask_8: 0.5713  loss_dice_8: 0.974  time: 1.3887  data_time: 0.0109  lr: 8.8947e-06  max_mem: 23815M
[01/01 12:26:39] d2.utils.events INFO:  eta: 13:05:22  iter: 4779  total_loss: 19.65  loss_ce: 0.4008  loss_cate: 0.09853  loss_mask: 0.5415  loss_dice: 0.8304  loss_ce_0: 0.8439  loss_cate_0: 0  loss_mask_0: 0.4726  loss_dice_0: 0.7841  loss_ce_1: 0.5321  loss_cate_1: 0  loss_mask_1: 0.511  loss_dice_1: 0.7996  loss_ce_2: 0.4087  loss_cate_2: 0  loss_mask_2: 0.5269  loss_dice_2: 0.8413  loss_ce_3: 0.4667  loss_cate_3: 0  loss_mask_3: 0.5023  loss_dice_3: 0.8151  loss_ce_4: 0.4198  loss_cate_4: 0  loss_mask_4: 0.5219  loss_dice_4: 0.7909  loss_ce_5: 0.4319  loss_cate_5: 0  loss_mask_5: 0.5183  loss_dice_5: 0.8141  loss_ce_6: 0.3838  loss_cate_6: 0  loss_mask_6: 0.5013  loss_dice_6: 0.8563  loss_ce_7: 0.3794  loss_cate_7: 0  loss_mask_7: 0.533  loss_dice_7: 0.8426  loss_ce_8: 0.4124  loss_cate_8: 0  loss_mask_8: 0.5466  loss_dice_8: 0.8008  time: 1.3886  data_time: 0.0117  lr: 8.8901e-06  max_mem: 23815M
[01/01 12:27:07] d2.utils.events INFO:  eta: 13:04:47  iter: 4799  total_loss: 24.36  loss_ce: 0.6985  loss_cate: 0.1321  loss_mask: 0.5998  loss_dice: 0.9356  loss_ce_0: 1.002  loss_cate_0: 0  loss_mask_0: 0.6286  loss_dice_0: 0.9053  loss_ce_1: 0.6939  loss_cate_1: 0  loss_mask_1: 0.5834  loss_dice_1: 0.922  loss_ce_2: 0.6086  loss_cate_2: 0  loss_mask_2: 0.593  loss_dice_2: 0.9154  loss_ce_3: 0.6401  loss_cate_3: 0  loss_mask_3: 0.6362  loss_dice_3: 0.9428  loss_ce_4: 0.6027  loss_cate_4: 0  loss_mask_4: 0.5613  loss_dice_4: 0.9016  loss_ce_5: 0.6329  loss_cate_5: 0  loss_mask_5: 0.676  loss_dice_5: 0.9378  loss_ce_6: 0.6393  loss_cate_6: 0  loss_mask_6: 0.6954  loss_dice_6: 0.8734  loss_ce_7: 0.6362  loss_cate_7: 0  loss_mask_7: 0.6032  loss_dice_7: 0.9013  loss_ce_8: 0.6766  loss_cate_8: 0  loss_mask_8: 0.5987  loss_dice_8: 0.8597  time: 1.3885  data_time: 0.0120  lr: 8.8854e-06  max_mem: 23815M
[01/01 12:27:34] d2.utils.events INFO:  eta: 13:04:11  iter: 4819  total_loss: 16.13  loss_ce: 0.3752  loss_cate: 0.1006  loss_mask: 0.4714  loss_dice: 0.6178  loss_ce_0: 0.804  loss_cate_0: 0  loss_mask_0: 0.5292  loss_dice_0: 0.5925  loss_ce_1: 0.3577  loss_cate_1: 0  loss_mask_1: 0.4784  loss_dice_1: 0.6855  loss_ce_2: 0.332  loss_cate_2: 0  loss_mask_2: 0.4862  loss_dice_2: 0.6634  loss_ce_3: 0.41  loss_cate_3: 0  loss_mask_3: 0.4869  loss_dice_3: 0.6588  loss_ce_4: 0.4005  loss_cate_4: 0  loss_mask_4: 0.466  loss_dice_4: 0.6065  loss_ce_5: 0.37  loss_cate_5: 0  loss_mask_5: 0.4781  loss_dice_5: 0.6697  loss_ce_6: 0.3854  loss_cate_6: 0  loss_mask_6: 0.4615  loss_dice_6: 0.6229  loss_ce_7: 0.3415  loss_cate_7: 0  loss_mask_7: 0.4673  loss_dice_7: 0.618  loss_ce_8: 0.3529  loss_cate_8: 0  loss_mask_8: 0.4668  loss_dice_8: 0.6623  time: 1.3885  data_time: 0.0111  lr: 8.8807e-06  max_mem: 23815M
[01/01 12:28:02] d2.utils.events INFO:  eta: 13:03:36  iter: 4839  total_loss: 17.47  loss_ce: 0.3847  loss_cate: 0.1195  loss_mask: 0.5254  loss_dice: 0.612  loss_ce_0: 0.8417  loss_cate_0: 0  loss_mask_0: 0.4779  loss_dice_0: 0.6642  loss_ce_1: 0.6221  loss_cate_1: 0  loss_mask_1: 0.449  loss_dice_1: 0.6418  loss_ce_2: 0.4856  loss_cate_2: 0  loss_mask_2: 0.4247  loss_dice_2: 0.6524  loss_ce_3: 0.4353  loss_cate_3: 0  loss_mask_3: 0.4452  loss_dice_3: 0.6644  loss_ce_4: 0.4411  loss_cate_4: 0  loss_mask_4: 0.5252  loss_dice_4: 0.6773  loss_ce_5: 0.3878  loss_cate_5: 0  loss_mask_5: 0.5222  loss_dice_5: 0.6537  loss_ce_6: 0.4188  loss_cate_6: 0  loss_mask_6: 0.4877  loss_dice_6: 0.6105  loss_ce_7: 0.36  loss_cate_7: 0  loss_mask_7: 0.5037  loss_dice_7: 0.6102  loss_ce_8: 0.3727  loss_cate_8: 0  loss_mask_8: 0.4882  loss_dice_8: 0.6086  time: 1.3885  data_time: 0.0118  lr: 8.876e-06  max_mem: 23815M
[01/01 12:28:30] d2.utils.events INFO:  eta: 13:03:00  iter: 4859  total_loss: 17.76  loss_ce: 0.4518  loss_cate: 0.1491  loss_mask: 0.4485  loss_dice: 0.7104  loss_ce_0: 0.7243  loss_cate_0: 0  loss_mask_0: 0.4114  loss_dice_0: 0.7111  loss_ce_1: 0.5396  loss_cate_1: 0  loss_mask_1: 0.4374  loss_dice_1: 0.7115  loss_ce_2: 0.4888  loss_cate_2: 0  loss_mask_2: 0.4148  loss_dice_2: 0.6929  loss_ce_3: 0.4489  loss_cate_3: 0  loss_mask_3: 0.4393  loss_dice_3: 0.664  loss_ce_4: 0.4493  loss_cate_4: 0  loss_mask_4: 0.4318  loss_dice_4: 0.7205  loss_ce_5: 0.4564  loss_cate_5: 0  loss_mask_5: 0.4307  loss_dice_5: 0.7487  loss_ce_6: 0.4436  loss_cate_6: 0  loss_mask_6: 0.4278  loss_dice_6: 0.7178  loss_ce_7: 0.472  loss_cate_7: 0  loss_mask_7: 0.4714  loss_dice_7: 0.746  loss_ce_8: 0.4354  loss_cate_8: 0  loss_mask_8: 0.4367  loss_dice_8: 0.7099  time: 1.3885  data_time: 0.0150  lr: 8.8714e-06  max_mem: 23815M
[01/01 12:28:57] d2.utils.events INFO:  eta: 13:02:18  iter: 4879  total_loss: 15.97  loss_ce: 0.4453  loss_cate: 0.1179  loss_mask: 0.506  loss_dice: 0.6291  loss_ce_0: 0.6867  loss_cate_0: 0  loss_mask_0: 0.4949  loss_dice_0: 0.6852  loss_ce_1: 0.4602  loss_cate_1: 0  loss_mask_1: 0.5475  loss_dice_1: 0.6457  loss_ce_2: 0.4574  loss_cate_2: 0  loss_mask_2: 0.5431  loss_dice_2: 0.619  loss_ce_3: 0.4754  loss_cate_3: 0  loss_mask_3: 0.5111  loss_dice_3: 0.6405  loss_ce_4: 0.4162  loss_cate_4: 0  loss_mask_4: 0.5041  loss_dice_4: 0.6627  loss_ce_5: 0.4262  loss_cate_5: 0  loss_mask_5: 0.5052  loss_dice_5: 0.6431  loss_ce_6: 0.4279  loss_cate_6: 0  loss_mask_6: 0.5184  loss_dice_6: 0.6248  loss_ce_7: 0.4295  loss_cate_7: 0  loss_mask_7: 0.5305  loss_dice_7: 0.5985  loss_ce_8: 0.442  loss_cate_8: 0  loss_mask_8: 0.504  loss_dice_8: 0.6202  time: 1.3884  data_time: 0.0123  lr: 8.8667e-06  max_mem: 23815M
[01/01 12:29:25] d2.utils.events INFO:  eta: 13:01:46  iter: 4899  total_loss: 18.32  loss_ce: 0.4625  loss_cate: 0.1349  loss_mask: 0.5691  loss_dice: 0.5564  loss_ce_0: 0.74  loss_cate_0: 0  loss_mask_0: 0.6099  loss_dice_0: 0.5638  loss_ce_1: 0.4531  loss_cate_1: 0  loss_mask_1: 0.6203  loss_dice_1: 0.5785  loss_ce_2: 0.4083  loss_cate_2: 0  loss_mask_2: 0.5753  loss_dice_2: 0.5424  loss_ce_3: 0.4456  loss_cate_3: 0  loss_mask_3: 0.5539  loss_dice_3: 0.5953  loss_ce_4: 0.4023  loss_cate_4: 0  loss_mask_4: 0.5891  loss_dice_4: 0.5475  loss_ce_5: 0.4615  loss_cate_5: 0  loss_mask_5: 0.5607  loss_dice_5: 0.5775  loss_ce_6: 0.3822  loss_cate_6: 0  loss_mask_6: 0.5605  loss_dice_6: 0.5855  loss_ce_7: 0.4687  loss_cate_7: 0  loss_mask_7: 0.5422  loss_dice_7: 0.5591  loss_ce_8: 0.4538  loss_cate_8: 0  loss_mask_8: 0.5523  loss_dice_8: 0.5936  time: 1.3884  data_time: 0.0145  lr: 8.862e-06  max_mem: 23815M
[01/01 12:29:53] d2.utils.events INFO:  eta: 13:01:18  iter: 4919  total_loss: 13.09  loss_ce: 0.3095  loss_cate: 0.08487  loss_mask: 0.3518  loss_dice: 0.6299  loss_ce_0: 0.6175  loss_cate_0: 0  loss_mask_0: 0.3743  loss_dice_0: 0.5943  loss_ce_1: 0.3477  loss_cate_1: 0  loss_mask_1: 0.3628  loss_dice_1: 0.5521  loss_ce_2: 0.3343  loss_cate_2: 0  loss_mask_2: 0.3461  loss_dice_2: 0.5711  loss_ce_3: 0.3907  loss_cate_3: 0  loss_mask_3: 0.3502  loss_dice_3: 0.5337  loss_ce_4: 0.3554  loss_cate_4: 0  loss_mask_4: 0.3498  loss_dice_4: 0.5461  loss_ce_5: 0.2913  loss_cate_5: 0  loss_mask_5: 0.3577  loss_dice_5: 0.5414  loss_ce_6: 0.3054  loss_cate_6: 0  loss_mask_6: 0.3469  loss_dice_6: 0.5761  loss_ce_7: 0.2811  loss_cate_7: 0  loss_mask_7: 0.3548  loss_dice_7: 0.5836  loss_ce_8: 0.2803  loss_cate_8: 0  loss_mask_8: 0.3539  loss_dice_8: 0.6016  time: 1.3884  data_time: 0.0167  lr: 8.8573e-06  max_mem: 23815M
[01/01 12:30:21] d2.utils.events INFO:  eta: 13:00:44  iter: 4939  total_loss: 14.31  loss_ce: 0.4065  loss_cate: 0.1312  loss_mask: 0.4732  loss_dice: 0.5633  loss_ce_0: 0.8071  loss_cate_0: 0  loss_mask_0: 0.4796  loss_dice_0: 0.6098  loss_ce_1: 0.4334  loss_cate_1: 0  loss_mask_1: 0.4708  loss_dice_1: 0.626  loss_ce_2: 0.4459  loss_cate_2: 0  loss_mask_2: 0.4834  loss_dice_2: 0.5586  loss_ce_3: 0.4247  loss_cate_3: 0  loss_mask_3: 0.488  loss_dice_3: 0.5719  loss_ce_4: 0.3681  loss_cate_4: 0  loss_mask_4: 0.4939  loss_dice_4: 0.625  loss_ce_5: 0.3828  loss_cate_5: 0  loss_mask_5: 0.4598  loss_dice_5: 0.5512  loss_ce_6: 0.3768  loss_cate_6: 0  loss_mask_6: 0.4608  loss_dice_6: 0.5523  loss_ce_7: 0.3696  loss_cate_7: 0  loss_mask_7: 0.4717  loss_dice_7: 0.5547  loss_ce_8: 0.3891  loss_cate_8: 0  loss_mask_8: 0.4736  loss_dice_8: 0.5814  time: 1.3884  data_time: 0.0144  lr: 8.8527e-06  max_mem: 23815M
[01/01 12:30:49] d2.utils.events INFO:  eta: 13:00:13  iter: 4959  total_loss: 20.69  loss_ce: 0.5727  loss_cate: 0.1261  loss_mask: 0.5344  loss_dice: 0.8608  loss_ce_0: 0.8131  loss_cate_0: 0  loss_mask_0: 0.5606  loss_dice_0: 0.9463  loss_ce_1: 0.7135  loss_cate_1: 0  loss_mask_1: 0.5392  loss_dice_1: 0.9233  loss_ce_2: 0.5961  loss_cate_2: 0  loss_mask_2: 0.5303  loss_dice_2: 0.8798  loss_ce_3: 0.6155  loss_cate_3: 0  loss_mask_3: 0.537  loss_dice_3: 0.8734  loss_ce_4: 0.6316  loss_cate_4: 0  loss_mask_4: 0.5348  loss_dice_4: 0.8753  loss_ce_5: 0.5356  loss_cate_5: 0  loss_mask_5: 0.5317  loss_dice_5: 0.9252  loss_ce_6: 0.5336  loss_cate_6: 0  loss_mask_6: 0.5333  loss_dice_6: 0.9173  loss_ce_7: 0.5549  loss_cate_7: 0  loss_mask_7: 0.5206  loss_dice_7: 0.8684  loss_ce_8: 0.5536  loss_cate_8: 0  loss_mask_8: 0.5141  loss_dice_8: 0.9297  time: 1.3884  data_time: 0.0128  lr: 8.848e-06  max_mem: 23815M
[01/01 12:31:16] d2.utils.events INFO:  eta: 12:59:42  iter: 4979  total_loss: 20.47  loss_ce: 0.6132  loss_cate: 0.1317  loss_mask: 0.5765  loss_dice: 0.7992  loss_ce_0: 0.8787  loss_cate_0: 0  loss_mask_0: 0.5429  loss_dice_0: 0.7133  loss_ce_1: 0.559  loss_cate_1: 0  loss_mask_1: 0.5789  loss_dice_1: 0.7859  loss_ce_2: 0.5465  loss_cate_2: 0  loss_mask_2: 0.5261  loss_dice_2: 0.7903  loss_ce_3: 0.5936  loss_cate_3: 0  loss_mask_3: 0.5181  loss_dice_3: 0.8233  loss_ce_4: 0.5398  loss_cate_4: 0  loss_mask_4: 0.5301  loss_dice_4: 0.788  loss_ce_5: 0.5189  loss_cate_5: 0  loss_mask_5: 0.5681  loss_dice_5: 0.7956  loss_ce_6: 0.5283  loss_cate_6: 0  loss_mask_6: 0.5893  loss_dice_6: 0.7765  loss_ce_7: 0.5124  loss_cate_7: 0  loss_mask_7: 0.5963  loss_dice_7: 0.7891  loss_ce_8: 0.6027  loss_cate_8: 0  loss_mask_8: 0.581  loss_dice_8: 0.8252  time: 1.3883  data_time: 0.0130  lr: 8.8433e-06  max_mem: 23815M
[01/01 12:31:44] fvcore.common.checkpoint INFO: Saving checkpoint to ./trash_dataV1_WarmupPolyLR_1e-5/model_0004999.pth
[01/01 12:31:46] d2.utils.events INFO:  eta: 12:59:09  iter: 4999  total_loss: 22.09  loss_ce: 0.5663  loss_cate: 0.1043  loss_mask: 0.4902  loss_dice: 0.9158  loss_ce_0: 0.8004  loss_cate_0: 0  loss_mask_0: 0.5608  loss_dice_0: 0.9642  loss_ce_1: 0.5373  loss_cate_1: 0  loss_mask_1: 0.5335  loss_dice_1: 0.9389  loss_ce_2: 0.4974  loss_cate_2: 0  loss_mask_2: 0.4943  loss_dice_2: 0.9674  loss_ce_3: 0.5539  loss_cate_3: 0  loss_mask_3: 0.4997  loss_dice_3: 0.8755  loss_ce_4: 0.551  loss_cate_4: 0  loss_mask_4: 0.5106  loss_dice_4: 0.9767  loss_ce_5: 0.5501  loss_cate_5: 0  loss_mask_5: 0.4883  loss_dice_5: 0.8935  loss_ce_6: 0.4996  loss_cate_6: 0  loss_mask_6: 0.5216  loss_dice_6: 0.902  loss_ce_7: 0.5313  loss_cate_7: 0  loss_mask_7: 0.5006  loss_dice_7: 0.8584  loss_ce_8: 0.4962  loss_cate_8: 0  loss_mask_8: 0.5112  loss_dice_8: 0.8766  time: 1.3883  data_time: 0.0117  lr: 8.8386e-06  max_mem: 23815M
[01/01 12:32:13] d2.utils.events INFO:  eta: 12:58:36  iter: 5019  total_loss: 17.31  loss_ce: 0.5326  loss_cate: 0.1206  loss_mask: 0.4503  loss_dice: 0.7812  loss_ce_0: 0.8348  loss_cate_0: 0  loss_mask_0: 0.4566  loss_dice_0: 0.7354  loss_ce_1: 0.5452  loss_cate_1: 0  loss_mask_1: 0.4248  loss_dice_1: 0.7844  loss_ce_2: 0.5087  loss_cate_2: 0  loss_mask_2: 0.415  loss_dice_2: 0.7398  loss_ce_3: 0.4636  loss_cate_3: 0  loss_mask_3: 0.4354  loss_dice_3: 0.7436  loss_ce_4: 0.5047  loss_cate_4: 0  loss_mask_4: 0.4383  loss_dice_4: 0.7968  loss_ce_5: 0.4699  loss_cate_5: 0  loss_mask_5: 0.4413  loss_dice_5: 0.7881  loss_ce_6: 0.4876  loss_cate_6: 0  loss_mask_6: 0.457  loss_dice_6: 0.8111  loss_ce_7: 0.4993  loss_cate_7: 0  loss_mask_7: 0.4488  loss_dice_7: 0.7982  loss_ce_8: 0.4944  loss_cate_8: 0  loss_mask_8: 0.4518  loss_dice_8: 0.7181  time: 1.3883  data_time: 0.0146  lr: 8.8339e-06  max_mem: 23815M
[01/01 12:32:41] d2.utils.events INFO:  eta: 12:58:09  iter: 5039  total_loss: 17.9  loss_ce: 0.4295  loss_cate: 0.1331  loss_mask: 0.6441  loss_dice: 0.7879  loss_ce_0: 0.5801  loss_cate_0: 0  loss_mask_0: 0.6854  loss_dice_0: 0.7447  loss_ce_1: 0.3385  loss_cate_1: 0  loss_mask_1: 0.6075  loss_dice_1: 0.7575  loss_ce_2: 0.4032  loss_cate_2: 0  loss_mask_2: 0.6617  loss_dice_2: 0.7916  loss_ce_3: 0.3845  loss_cate_3: 0  loss_mask_3: 0.6083  loss_dice_3: 0.7749  loss_ce_4: 0.3889  loss_cate_4: 0  loss_mask_4: 0.6746  loss_dice_4: 0.7345  loss_ce_5: 0.4603  loss_cate_5: 0  loss_mask_5: 0.6361  loss_dice_5: 0.7382  loss_ce_6: 0.3972  loss_cate_6: 0  loss_mask_6: 0.6016  loss_dice_6: 0.7537  loss_ce_7: 0.3899  loss_cate_7: 0  loss_mask_7: 0.6612  loss_dice_7: 0.7732  loss_ce_8: 0.407  loss_cate_8: 0  loss_mask_8: 0.6727  loss_dice_8: 0.7487  time: 1.3883  data_time: 0.0125  lr: 8.8293e-06  max_mem: 23815M
[01/01 12:33:09] d2.utils.events INFO:  eta: 12:57:40  iter: 5059  total_loss: 24.77  loss_ce: 0.6643  loss_cate: 0.1765  loss_mask: 0.5869  loss_dice: 1.007  loss_ce_0: 0.8472  loss_cate_0: 0  loss_mask_0: 0.6746  loss_dice_0: 1.02  loss_ce_1: 0.632  loss_cate_1: 0  loss_mask_1: 0.6352  loss_dice_1: 0.991  loss_ce_2: 0.6159  loss_cate_2: 0  loss_mask_2: 0.5643  loss_dice_2: 0.9958  loss_ce_3: 0.6264  loss_cate_3: 0  loss_mask_3: 0.5644  loss_dice_3: 0.9764  loss_ce_4: 0.6164  loss_cate_4: 0  loss_mask_4: 0.554  loss_dice_4: 0.918  loss_ce_5: 0.5807  loss_cate_5: 0  loss_mask_5: 0.6098  loss_dice_5: 1.004  loss_ce_6: 0.596  loss_cate_6: 0  loss_mask_6: 0.541  loss_dice_6: 0.998  loss_ce_7: 0.6486  loss_cate_7: 0  loss_mask_7: 0.5641  loss_dice_7: 0.9741  loss_ce_8: 0.6187  loss_cate_8: 0  loss_mask_8: 0.5774  loss_dice_8: 0.9756  time: 1.3882  data_time: 0.0104  lr: 8.8246e-06  max_mem: 23815M
[01/01 12:33:36] d2.utils.events INFO:  eta: 12:57:07  iter: 5079  total_loss: 20.18  loss_ce: 0.4151  loss_cate: 0.1252  loss_mask: 0.5496  loss_dice: 0.6755  loss_ce_0: 0.7919  loss_cate_0: 0  loss_mask_0: 0.5798  loss_dice_0: 0.7349  loss_ce_1: 0.4694  loss_cate_1: 0  loss_mask_1: 0.565  loss_dice_1: 0.7893  loss_ce_2: 0.458  loss_cate_2: 0  loss_mask_2: 0.5913  loss_dice_2: 0.7334  loss_ce_3: 0.4322  loss_cate_3: 0  loss_mask_3: 0.5241  loss_dice_3: 0.7192  loss_ce_4: 0.4456  loss_cate_4: 0  loss_mask_4: 0.5573  loss_dice_4: 0.7354  loss_ce_5: 0.4636  loss_cate_5: 0  loss_mask_5: 0.5499  loss_dice_5: 0.7274  loss_ce_6: 0.4407  loss_cate_6: 0  loss_mask_6: 0.5309  loss_dice_6: 0.6823  loss_ce_7: 0.3961  loss_cate_7: 0  loss_mask_7: 0.523  loss_dice_7: 0.6708  loss_ce_8: 0.3739  loss_cate_8: 0  loss_mask_8: 0.5153  loss_dice_8: 0.6705  time: 1.3881  data_time: 0.0116  lr: 8.8199e-06  max_mem: 23815M
[01/01 12:34:04] d2.utils.events INFO:  eta: 12:56:41  iter: 5099  total_loss: 26.61  loss_ce: 0.5587  loss_cate: 0.135  loss_mask: 0.7582  loss_dice: 1.02  loss_ce_0: 0.9108  loss_cate_0: 0  loss_mask_0: 0.8169  loss_dice_0: 1.014  loss_ce_1: 0.6211  loss_cate_1: 0  loss_mask_1: 0.8014  loss_dice_1: 1.031  loss_ce_2: 0.4923  loss_cate_2: 0  loss_mask_2: 0.8017  loss_dice_2: 0.9874  loss_ce_3: 0.5477  loss_cate_3: 0  loss_mask_3: 0.73  loss_dice_3: 0.9714  loss_ce_4: 0.55  loss_cate_4: 0  loss_mask_4: 0.7774  loss_dice_4: 0.9971  loss_ce_5: 0.5192  loss_cate_5: 0  loss_mask_5: 0.7918  loss_dice_5: 1.017  loss_ce_6: 0.5555  loss_cate_6: 0  loss_mask_6: 0.7995  loss_dice_6: 1.048  loss_ce_7: 0.5759  loss_cate_7: 0  loss_mask_7: 0.8057  loss_dice_7: 1  loss_ce_8: 0.5396  loss_cate_8: 0  loss_mask_8: 0.7917  loss_dice_8: 0.992  time: 1.3881  data_time: 0.0112  lr: 8.8152e-06  max_mem: 23815M
[01/01 12:34:31] d2.utils.events INFO:  eta: 12:56:12  iter: 5119  total_loss: 21.57  loss_ce: 0.6401  loss_cate: 0.1213  loss_mask: 0.5562  loss_dice: 0.8578  loss_ce_0: 0.9685  loss_cate_0: 0  loss_mask_0: 0.5314  loss_dice_0: 0.9057  loss_ce_1: 0.6361  loss_cate_1: 0  loss_mask_1: 0.5312  loss_dice_1: 0.9046  loss_ce_2: 0.5796  loss_cate_2: 0  loss_mask_2: 0.4804  loss_dice_2: 0.8501  loss_ce_3: 0.6179  loss_cate_3: 0  loss_mask_3: 0.4799  loss_dice_3: 0.9053  loss_ce_4: 0.5541  loss_cate_4: 0  loss_mask_4: 0.5428  loss_dice_4: 0.9051  loss_ce_5: 0.6397  loss_cate_5: 0  loss_mask_5: 0.5483  loss_dice_5: 0.8677  loss_ce_6: 0.6546  loss_cate_6: 0  loss_mask_6: 0.5501  loss_dice_6: 0.8501  loss_ce_7: 0.5765  loss_cate_7: 0  loss_mask_7: 0.5343  loss_dice_7: 0.8817  loss_ce_8: 0.5697  loss_cate_8: 0  loss_mask_8: 0.5579  loss_dice_8: 0.8639  time: 1.3880  data_time: 0.0158  lr: 8.8105e-06  max_mem: 23815M
[01/01 12:34:59] d2.utils.events INFO:  eta: 12:55:37  iter: 5139  total_loss: 21.13  loss_ce: 0.4348  loss_cate: 0.1055  loss_mask: 0.6709  loss_dice: 0.8426  loss_ce_0: 0.7618  loss_cate_0: 0  loss_mask_0: 0.6389  loss_dice_0: 0.8245  loss_ce_1: 0.4711  loss_cate_1: 0  loss_mask_1: 0.6173  loss_dice_1: 0.9061  loss_ce_2: 0.42  loss_cate_2: 0  loss_mask_2: 0.622  loss_dice_2: 0.7857  loss_ce_3: 0.4293  loss_cate_3: 0  loss_mask_3: 0.6071  loss_dice_3: 0.8595  loss_ce_4: 0.4141  loss_cate_4: 0  loss_mask_4: 0.6261  loss_dice_4: 0.8341  loss_ce_5: 0.4198  loss_cate_5: 0  loss_mask_5: 0.6234  loss_dice_5: 0.7513  loss_ce_6: 0.4472  loss_cate_6: 0  loss_mask_6: 0.6529  loss_dice_6: 0.8089  loss_ce_7: 0.4196  loss_cate_7: 0  loss_mask_7: 0.6661  loss_dice_7: 0.8245  loss_ce_8: 0.4302  loss_cate_8: 0  loss_mask_8: 0.6517  loss_dice_8: 0.8016  time: 1.3880  data_time: 0.0142  lr: 8.8059e-06  max_mem: 23815M
[01/01 12:35:27] d2.utils.events INFO:  eta: 12:55:00  iter: 5159  total_loss: 21.12  loss_ce: 0.5365  loss_cate: 0.1244  loss_mask: 0.4353  loss_dice: 0.7502  loss_ce_0: 0.8683  loss_cate_0: 0  loss_mask_0: 0.4441  loss_dice_0: 0.787  loss_ce_1: 0.532  loss_cate_1: 0  loss_mask_1: 0.5362  loss_dice_1: 0.8182  loss_ce_2: 0.5949  loss_cate_2: 0  loss_mask_2: 0.5147  loss_dice_2: 0.8072  loss_ce_3: 0.6513  loss_cate_3: 0  loss_mask_3: 0.4449  loss_dice_3: 0.808  loss_ce_4: 0.5946  loss_cate_4: 0  loss_mask_4: 0.4841  loss_dice_4: 0.7873  loss_ce_5: 0.6395  loss_cate_5: 0  loss_mask_5: 0.4838  loss_dice_5: 0.7631  loss_ce_6: 0.6157  loss_cate_6: 0  loss_mask_6: 0.4338  loss_dice_6: 0.7425  loss_ce_7: 0.5108  loss_cate_7: 0  loss_mask_7: 0.4861  loss_dice_7: 0.7652  loss_ce_8: 0.5112  loss_cate_8: 0  loss_mask_8: 0.4516  loss_dice_8: 0.741  time: 1.3880  data_time: 0.0112  lr: 8.8012e-06  max_mem: 23815M
[01/01 12:35:54] d2.utils.events INFO:  eta: 12:54:32  iter: 5179  total_loss: 20.36  loss_ce: 0.6256  loss_cate: 0.102  loss_mask: 0.629  loss_dice: 0.7552  loss_ce_0: 0.8155  loss_cate_0: 0  loss_mask_0: 0.5697  loss_dice_0: 0.7247  loss_ce_1: 0.4654  loss_cate_1: 0  loss_mask_1: 0.6152  loss_dice_1: 0.8878  loss_ce_2: 0.5535  loss_cate_2: 0  loss_mask_2: 0.5758  loss_dice_2: 0.7585  loss_ce_3: 0.5653  loss_cate_3: 0  loss_mask_3: 0.5563  loss_dice_3: 0.7177  loss_ce_4: 0.5371  loss_cate_4: 0  loss_mask_4: 0.5695  loss_dice_4: 0.7858  loss_ce_5: 0.5941  loss_cate_5: 0  loss_mask_5: 0.5462  loss_dice_5: 0.731  loss_ce_6: 0.5739  loss_cate_6: 0  loss_mask_6: 0.5244  loss_dice_6: 0.7186  loss_ce_7: 0.5467  loss_cate_7: 0  loss_mask_7: 0.577  loss_dice_7: 0.7075  loss_ce_8: 0.532  loss_cate_8: 0  loss_mask_8: 0.6049  loss_dice_8: 0.7348  time: 1.3879  data_time: 0.0124  lr: 8.7965e-06  max_mem: 23815M
[01/01 12:36:22] d2.data.datasets.coco INFO: Loaded 653 images with semantic segmentation from /opt/ml/input/data/images/val_0
[01/01 12:36:22] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=1280, sample_style='choice')]
[01/01 12:36:22] d2.data.common INFO: Serializing 653 elements to byte tensors and concatenating them all ...
[01/01 12:36:22] d2.data.common INFO: Serialized dataset takes 0.09 MiB
[01/01 12:36:22] d2.data.datasets.coco INFO: Loaded 653 images with semantic segmentation from /opt/ml/input/data/images/val_0
[01/01 12:36:22] d2.evaluation.evaluator INFO: Start inference on 653 batches
[01/01 12:36:23] d2.evaluation.evaluator INFO: Inference done 11/653. Dataloading: 0.0008 s/iter. Inference: 0.1050 s/iter. Eval: 0.0124 s/iter. Total: 0.1182 s/iter. ETA=0:01:15
[01/01 12:36:28] d2.evaluation.evaluator INFO: Inference done 54/653. Dataloading: 0.0016 s/iter. Inference: 0.1049 s/iter. Eval: 0.0122 s/iter. Total: 0.1188 s/iter. ETA=0:01:11
[01/01 12:36:33] d2.evaluation.evaluator INFO: Inference done 96/653. Dataloading: 0.0016 s/iter. Inference: 0.1051 s/iter. Eval: 0.0122 s/iter. Total: 0.1190 s/iter. ETA=0:01:06
[01/01 12:36:39] d2.evaluation.evaluator INFO: Inference done 139/653. Dataloading: 0.0017 s/iter. Inference: 0.1050 s/iter. Eval: 0.0121 s/iter. Total: 0.1188 s/iter. ETA=0:01:01
[01/01 12:36:44] d2.evaluation.evaluator INFO: Inference done 182/653. Dataloading: 0.0017 s/iter. Inference: 0.1049 s/iter. Eval: 0.0120 s/iter. Total: 0.1186 s/iter. ETA=0:00:55
[01/01 12:36:49] d2.evaluation.evaluator INFO: Inference done 225/653. Dataloading: 0.0017 s/iter. Inference: 0.1048 s/iter. Eval: 0.0119 s/iter. Total: 0.1185 s/iter. ETA=0:00:50
[01/01 12:36:54] d2.evaluation.evaluator INFO: Inference done 267/653. Dataloading: 0.0016 s/iter. Inference: 0.1050 s/iter. Eval: 0.0120 s/iter. Total: 0.1187 s/iter. ETA=0:00:45
[01/01 12:36:59] d2.evaluation.evaluator INFO: Inference done 310/653. Dataloading: 0.0016 s/iter. Inference: 0.1049 s/iter. Eval: 0.0120 s/iter. Total: 0.1186 s/iter. ETA=0:00:40
[01/01 12:37:04] d2.evaluation.evaluator INFO: Inference done 353/653. Dataloading: 0.0016 s/iter. Inference: 0.1048 s/iter. Eval: 0.0121 s/iter. Total: 0.1186 s/iter. ETA=0:00:35
[01/01 12:37:09] d2.evaluation.evaluator INFO: Inference done 396/653. Dataloading: 0.0016 s/iter. Inference: 0.1048 s/iter. Eval: 0.0121 s/iter. Total: 0.1185 s/iter. ETA=0:00:30
[01/01 12:37:14] d2.evaluation.evaluator INFO: Inference done 439/653. Dataloading: 0.0016 s/iter. Inference: 0.1047 s/iter. Eval: 0.0121 s/iter. Total: 0.1185 s/iter. ETA=0:00:25
[01/01 12:37:19] d2.evaluation.evaluator INFO: Inference done 482/653. Dataloading: 0.0016 s/iter. Inference: 0.1047 s/iter. Eval: 0.0121 s/iter. Total: 0.1185 s/iter. ETA=0:00:20
[01/01 12:37:24] d2.evaluation.evaluator INFO: Inference done 525/653. Dataloading: 0.0016 s/iter. Inference: 0.1047 s/iter. Eval: 0.0121 s/iter. Total: 0.1185 s/iter. ETA=0:00:15
[01/01 12:37:29] d2.evaluation.evaluator INFO: Inference done 568/653. Dataloading: 0.0016 s/iter. Inference: 0.1047 s/iter. Eval: 0.0121 s/iter. Total: 0.1185 s/iter. ETA=0:00:10
[01/01 12:37:34] d2.evaluation.evaluator INFO: Inference done 611/653. Dataloading: 0.0016 s/iter. Inference: 0.1047 s/iter. Eval: 0.0120 s/iter. Total: 0.1185 s/iter. ETA=0:00:04
[01/01 12:37:39] d2.evaluation.evaluator INFO: Total inference time: 0:01:16.821923 (0.118552 s / iter per device, on 1 devices)
[01/01 12:37:39] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:01:07 (0.104742 s / iter per device, on 1 devices)
[01/01 12:37:39] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 59.999682308626504, 'fwIoU': 89.18380831806162, 'IoU-Backgroud': 96.16821543495404, 'IoU-General trash': 42.0098650854773, 'IoU-Paper': 77.18004231856229, 'IoU-Paper pack': 47.33449674807982, 'IoU-Metal': 46.41824264631958, 'IoU-Glass': 66.15798584639118, 'IoU-Plastic': 46.07534807997651, 'IoU-Styrofoam': 69.6811790622324, 'IoU-Plastic bag': 85.49809947659195, 'IoU-Battery': 0.0, 'IoU-Clothing': 83.4730306963064, 'mACC': 70.0519535090281, 'pACC': 93.4504498749013, 'ACC-Backgroud': 98.23877484633437, 'ACC-General trash': 57.28549669561341, 'ACC-Paper': 83.858432383565, 'ACC-Paper pack': 54.58128493320395, 'ACC-Metal': 62.439897793878565, 'ACC-Glass': 74.54149424838205, 'ACC-Plastic': 83.52758951443496, 'ACC-Styrofoam': 76.29322170435125, 'ACC-Plastic bag': 89.51658717847441, 'ACC-Battery': 0.0, 'ACC-Clothing': 90.2887093010711})])
[01/01 12:37:39] d2.engine.defaults INFO: Evaluation results for trash_recycle_sem_seg_val_0 in csv format:
[01/01 12:37:39] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/01 12:37:39] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/01 12:37:39] d2.evaluation.testing INFO: copypaste: 59.9997,89.1838,70.0520,93.4504
[01/01 12:37:39] d2.engine.hooks INFO: Not saving as latest eval score for mIoU is 59.99968, not better than best score 66.42078 @ iteration 3899.
[01/01 12:37:39] d2.utils.events INFO:  eta: 12:53:59  iter: 5199  total_loss: 22.48  loss_ce: 0.5839  loss_cate: 0.1037  loss_mask: 0.6147  loss_dice: 0.8068  loss_ce_0: 0.9773  loss_cate_0: 0  loss_mask_0: 0.7591  loss_dice_0: 0.7858  loss_ce_1: 0.6162  loss_cate_1: 0  loss_mask_1: 0.6487  loss_dice_1: 0.8176  loss_ce_2: 0.466  loss_cate_2: 0  loss_mask_2: 0.6192  loss_dice_2: 0.7566  loss_ce_3: 0.5949  loss_cate_3: 0  loss_mask_3: 0.5817  loss_dice_3: 0.6617  loss_ce_4: 0.5365  loss_cate_4: 0  loss_mask_4: 0.618  loss_dice_4: 0.7601  loss_ce_5: 0.5122  loss_cate_5: 0  loss_mask_5: 0.6643  loss_dice_5: 0.7394  loss_ce_6: 0.4645  loss_cate_6: 0  loss_mask_6: 0.6367  loss_dice_6: 0.828  loss_ce_7: 0.5208  loss_cate_7: 0  loss_mask_7: 0.6645  loss_dice_7: 0.8178  loss_ce_8: 0.5667  loss_cate_8: 0  loss_mask_8: 0.6404  loss_dice_8: 0.8055  time: 1.3879  data_time: 0.0120  lr: 8.7918e-06  max_mem: 23815M
[01/01 12:38:07] d2.utils.events INFO:  eta: 12:53:27  iter: 5219  total_loss: 22.17  loss_ce: 0.6064  loss_cate: 0.1383  loss_mask: 0.5703  loss_dice: 0.9229  loss_ce_0: 0.9614  loss_cate_0: 0  loss_mask_0: 0.5198  loss_dice_0: 0.9898  loss_ce_1: 0.6397  loss_cate_1: 0  loss_mask_1: 0.5219  loss_dice_1: 0.9124  loss_ce_2: 0.5257  loss_cate_2: 0  loss_mask_2: 0.524  loss_dice_2: 0.9761  loss_ce_3: 0.5927  loss_cate_3: 0  loss_mask_3: 0.5367  loss_dice_3: 0.9611  loss_ce_4: 0.4871  loss_cate_4: 0  loss_mask_4: 0.5615  loss_dice_4: 0.9852  loss_ce_5: 0.5631  loss_cate_5: 0  loss_mask_5: 0.5868  loss_dice_5: 0.942  loss_ce_6: 0.6194  loss_cate_6: 0  loss_mask_6: 0.5734  loss_dice_6: 0.9213  loss_ce_7: 0.6378  loss_cate_7: 0  loss_mask_7: 0.5782  loss_dice_7: 0.9626  loss_ce_8: 0.5532  loss_cate_8: 0  loss_mask_8: 0.5833  loss_dice_8: 0.9338  time: 1.3878  data_time: 0.0127  lr: 8.7871e-06  max_mem: 23815M
[01/01 12:38:35] d2.utils.events INFO:  eta: 12:52:56  iter: 5239  total_loss: 20.69  loss_ce: 0.507  loss_cate: 0.1247  loss_mask: 0.6123  loss_dice: 0.8307  loss_ce_0: 0.8383  loss_cate_0: 0  loss_mask_0: 0.6604  loss_dice_0: 0.8039  loss_ce_1: 0.5282  loss_cate_1: 0  loss_mask_1: 0.6545  loss_dice_1: 0.7757  loss_ce_2: 0.527  loss_cate_2: 0  loss_mask_2: 0.6457  loss_dice_2: 0.8656  loss_ce_3: 0.4824  loss_cate_3: 0  loss_mask_3: 0.5536  loss_dice_3: 0.7475  loss_ce_4: 0.4889  loss_cate_4: 0  loss_mask_4: 0.6015  loss_dice_4: 0.7813  loss_ce_5: 0.4797  loss_cate_5: 0  loss_mask_5: 0.6391  loss_dice_5: 0.8164  loss_ce_6: 0.5253  loss_cate_6: 0  loss_mask_6: 0.6265  loss_dice_6: 0.8199  loss_ce_7: 0.5456  loss_cate_7: 0  loss_mask_7: 0.5652  loss_dice_7: 0.7447  loss_ce_8: 0.5204  loss_cate_8: 0  loss_mask_8: 0.5661  loss_dice_8: 0.8074  time: 1.3878  data_time: 0.0108  lr: 8.7824e-06  max_mem: 23815M
[01/01 12:39:02] d2.utils.events INFO:  eta: 12:52:22  iter: 5259  total_loss: 22.47  loss_ce: 0.5951  loss_cate: 0.1566  loss_mask: 0.7094  loss_dice: 0.8953  loss_ce_0: 0.8432  loss_cate_0: 0  loss_mask_0: 0.7473  loss_dice_0: 0.9686  loss_ce_1: 0.6336  loss_cate_1: 0  loss_mask_1: 0.5579  loss_dice_1: 0.8893  loss_ce_2: 0.6829  loss_cate_2: 0  loss_mask_2: 0.5734  loss_dice_2: 0.8956  loss_ce_3: 0.6783  loss_cate_3: 0  loss_mask_3: 0.6122  loss_dice_3: 0.7902  loss_ce_4: 0.7139  loss_cate_4: 0  loss_mask_4: 0.6216  loss_dice_4: 0.8448  loss_ce_5: 0.6597  loss_cate_5: 0  loss_mask_5: 0.6868  loss_dice_5: 0.8484  loss_ce_6: 0.6037  loss_cate_6: 0  loss_mask_6: 0.7092  loss_dice_6: 0.8683  loss_ce_7: 0.6591  loss_cate_7: 0  loss_mask_7: 0.7488  loss_dice_7: 0.8801  loss_ce_8: 0.662  loss_cate_8: 0  loss_mask_8: 0.7105  loss_dice_8: 0.858  time: 1.3877  data_time: 0.0114  lr: 8.7778e-06  max_mem: 23815M
[01/01 12:39:30] d2.utils.events INFO:  eta: 12:51:45  iter: 5279  total_loss: 16.54  loss_ce: 0.4503  loss_cate: 0.1003  loss_mask: 0.4443  loss_dice: 0.6938  loss_ce_0: 0.7185  loss_cate_0: 0  loss_mask_0: 0.4224  loss_dice_0: 0.6726  loss_ce_1: 0.5031  loss_cate_1: 0  loss_mask_1: 0.4256  loss_dice_1: 0.687  loss_ce_2: 0.4879  loss_cate_2: 0  loss_mask_2: 0.3957  loss_dice_2: 0.6381  loss_ce_3: 0.3997  loss_cate_3: 0  loss_mask_3: 0.4205  loss_dice_3: 0.6746  loss_ce_4: 0.439  loss_cate_4: 0  loss_mask_4: 0.4241  loss_dice_4: 0.6711  loss_ce_5: 0.4184  loss_cate_5: 0  loss_mask_5: 0.39  loss_dice_5: 0.6782  loss_ce_6: 0.4103  loss_cate_6: 0  loss_mask_6: 0.4197  loss_dice_6: 0.6242  loss_ce_7: 0.4589  loss_cate_7: 0  loss_mask_7: 0.4248  loss_dice_7: 0.6299  loss_ce_8: 0.4312  loss_cate_8: 0  loss_mask_8: 0.425  loss_dice_8: 0.7045  time: 1.3877  data_time: 0.0114  lr: 8.7731e-06  max_mem: 23815M
[01/01 12:39:58] d2.utils.events INFO:  eta: 12:51:24  iter: 5299  total_loss: 13.89  loss_ce: 0.4023  loss_cate: 0.09162  loss_mask: 0.4228  loss_dice: 0.5274  loss_ce_0: 0.665  loss_cate_0: 0  loss_mask_0: 0.3637  loss_dice_0: 0.6695  loss_ce_1: 0.4563  loss_cate_1: 0  loss_mask_1: 0.4384  loss_dice_1: 0.6281  loss_ce_2: 0.3643  loss_cate_2: 0  loss_mask_2: 0.4363  loss_dice_2: 0.5715  loss_ce_3: 0.3503  loss_cate_3: 0  loss_mask_3: 0.4245  loss_dice_3: 0.5487  loss_ce_4: 0.3025  loss_cate_4: 0  loss_mask_4: 0.4374  loss_dice_4: 0.5716  loss_ce_5: 0.354  loss_cate_5: 0  loss_mask_5: 0.4495  loss_dice_5: 0.5733  loss_ce_6: 0.3652  loss_cate_6: 0  loss_mask_6: 0.4333  loss_dice_6: 0.5273  loss_ce_7: 0.3963  loss_cate_7: 0  loss_mask_7: 0.4245  loss_dice_7: 0.5231  loss_ce_8: 0.4226  loss_cate_8: 0  loss_mask_8: 0.4257  loss_dice_8: 0.5254  time: 1.3877  data_time: 0.0153  lr: 8.7684e-06  max_mem: 23815M
[01/01 12:40:25] d2.utils.events INFO:  eta: 12:50:57  iter: 5319  total_loss: 19.73  loss_ce: 0.4068  loss_cate: 0.1115  loss_mask: 0.4246  loss_dice: 0.8084  loss_ce_0: 0.8643  loss_cate_0: 0  loss_mask_0: 0.4533  loss_dice_0: 0.8313  loss_ce_1: 0.4617  loss_cate_1: 0  loss_mask_1: 0.4322  loss_dice_1: 0.8662  loss_ce_2: 0.4028  loss_cate_2: 0  loss_mask_2: 0.4285  loss_dice_2: 0.8298  loss_ce_3: 0.4932  loss_cate_3: 0  loss_mask_3: 0.4133  loss_dice_3: 0.7492  loss_ce_4: 0.4126  loss_cate_4: 0  loss_mask_4: 0.4258  loss_dice_4: 0.8362  loss_ce_5: 0.3774  loss_cate_5: 0  loss_mask_5: 0.4172  loss_dice_5: 0.733  loss_ce_6: 0.4115  loss_cate_6: 0  loss_mask_6: 0.4418  loss_dice_6: 0.8069  loss_ce_7: 0.4443  loss_cate_7: 0  loss_mask_7: 0.4358  loss_dice_7: 0.7929  loss_ce_8: 0.4599  loss_cate_8: 0  loss_mask_8: 0.4404  loss_dice_8: 0.8288  time: 1.3877  data_time: 0.0117  lr: 8.7637e-06  max_mem: 23815M
[01/01 12:40:53] d2.utils.events INFO:  eta: 12:50:34  iter: 5339  total_loss: 20.32  loss_ce: 0.3999  loss_cate: 0.1321  loss_mask: 0.6578  loss_dice: 0.849  loss_ce_0: 0.9038  loss_cate_0: 0  loss_mask_0: 0.5903  loss_dice_0: 0.8447  loss_ce_1: 0.5418  loss_cate_1: 0  loss_mask_1: 0.5931  loss_dice_1: 0.9071  loss_ce_2: 0.4583  loss_cate_2: 0  loss_mask_2: 0.6447  loss_dice_2: 0.9113  loss_ce_3: 0.3731  loss_cate_3: 0  loss_mask_3: 0.5974  loss_dice_3: 0.8786  loss_ce_4: 0.442  loss_cate_4: 0  loss_mask_4: 0.6309  loss_dice_4: 0.8995  loss_ce_5: 0.4144  loss_cate_5: 0  loss_mask_5: 0.5534  loss_dice_5: 0.8719  loss_ce_6: 0.424  loss_cate_6: 0  loss_mask_6: 0.5735  loss_dice_6: 0.8974  loss_ce_7: 0.5303  loss_cate_7: 0  loss_mask_7: 0.6304  loss_dice_7: 0.7991  loss_ce_8: 0.436  loss_cate_8: 0  loss_mask_8: 0.6059  loss_dice_8: 0.8755  time: 1.3877  data_time: 0.0147  lr: 8.759e-06  max_mem: 23815M
[01/01 12:41:21] d2.utils.events INFO:  eta: 12:50:05  iter: 5359  total_loss: 16.37  loss_ce: 0.3019  loss_cate: 0.09317  loss_mask: 0.4477  loss_dice: 0.7456  loss_ce_0: 0.8361  loss_cate_0: 0  loss_mask_0: 0.3949  loss_dice_0: 0.7246  loss_ce_1: 0.3726  loss_cate_1: 0  loss_mask_1: 0.3804  loss_dice_1: 0.7248  loss_ce_2: 0.3863  loss_cate_2: 0  loss_mask_2: 0.3763  loss_dice_2: 0.6713  loss_ce_3: 0.3664  loss_cate_3: 0  loss_mask_3: 0.4156  loss_dice_3: 0.7341  loss_ce_4: 0.3638  loss_cate_4: 0  loss_mask_4: 0.3881  loss_dice_4: 0.7745  loss_ce_5: 0.3693  loss_cate_5: 0  loss_mask_5: 0.4182  loss_dice_5: 0.7726  loss_ce_6: 0.3615  loss_cate_6: 0  loss_mask_6: 0.4077  loss_dice_6: 0.716  loss_ce_7: 0.3637  loss_cate_7: 0  loss_mask_7: 0.4244  loss_dice_7: 0.7066  loss_ce_8: 0.3132  loss_cate_8: 0  loss_mask_8: 0.4081  loss_dice_8: 0.7086  time: 1.3877  data_time: 0.0137  lr: 8.7543e-06  max_mem: 23815M
[01/01 12:41:48] d2.utils.events INFO:  eta: 12:49:40  iter: 5379  total_loss: 17.71  loss_ce: 0.4361  loss_cate: 0.1085  loss_mask: 0.5212  loss_dice: 0.6734  loss_ce_0: 0.7659  loss_cate_0: 0  loss_mask_0: 0.4932  loss_dice_0: 0.676  loss_ce_1: 0.4774  loss_cate_1: 0  loss_mask_1: 0.479  loss_dice_1: 0.6887  loss_ce_2: 0.4639  loss_cate_2: 0  loss_mask_2: 0.4983  loss_dice_2: 0.6878  loss_ce_3: 0.3965  loss_cate_3: 0  loss_mask_3: 0.5281  loss_dice_3: 0.6745  loss_ce_4: 0.4336  loss_cate_4: 0  loss_mask_4: 0.5178  loss_dice_4: 0.681  loss_ce_5: 0.443  loss_cate_5: 0  loss_mask_5: 0.5206  loss_dice_5: 0.6742  loss_ce_6: 0.4237  loss_cate_6: 0  loss_mask_6: 0.5255  loss_dice_6: 0.6482  loss_ce_7: 0.4696  loss_cate_7: 0  loss_mask_7: 0.5255  loss_dice_7: 0.6748  loss_ce_8: 0.4727  loss_cate_8: 0  loss_mask_8: 0.5333  loss_dice_8: 0.6831  time: 1.3876  data_time: 0.0120  lr: 8.7497e-06  max_mem: 23815M
[01/01 12:42:16] d2.utils.events INFO:  eta: 12:49:15  iter: 5399  total_loss: 17.14  loss_ce: 0.5354  loss_cate: 0.1245  loss_mask: 0.4586  loss_dice: 0.7491  loss_ce_0: 0.8502  loss_cate_0: 0  loss_mask_0: 0.499  loss_dice_0: 0.7834  loss_ce_1: 0.4559  loss_cate_1: 0  loss_mask_1: 0.4752  loss_dice_1: 0.8425  loss_ce_2: 0.4741  loss_cate_2: 0  loss_mask_2: 0.4384  loss_dice_2: 0.7175  loss_ce_3: 0.4528  loss_cate_3: 0  loss_mask_3: 0.4559  loss_dice_3: 0.7384  loss_ce_4: 0.4495  loss_cate_4: 0  loss_mask_4: 0.4607  loss_dice_4: 0.7427  loss_ce_5: 0.4854  loss_cate_5: 0  loss_mask_5: 0.4465  loss_dice_5: 0.734  loss_ce_6: 0.4684  loss_cate_6: 0  loss_mask_6: 0.4542  loss_dice_6: 0.7532  loss_ce_7: 0.5053  loss_cate_7: 0  loss_mask_7: 0.466  loss_dice_7: 0.6999  loss_ce_8: 0.5384  loss_cate_8: 0  loss_mask_8: 0.4519  loss_dice_8: 0.7333  time: 1.3876  data_time: 0.0139  lr: 8.745e-06  max_mem: 23815M
[01/01 12:42:43] d2.utils.events INFO:  eta: 12:48:52  iter: 5419  total_loss: 15.47  loss_ce: 0.3271  loss_cate: 0.08738  loss_mask: 0.398  loss_dice: 0.6551  loss_ce_0: 0.682  loss_cate_0: 0  loss_mask_0: 0.4459  loss_dice_0: 0.6882  loss_ce_1: 0.4151  loss_cate_1: 0  loss_mask_1: 0.4144  loss_dice_1: 0.6669  loss_ce_2: 0.3753  loss_cate_2: 0  loss_mask_2: 0.4383  loss_dice_2: 0.6651  loss_ce_3: 0.3931  loss_cate_3: 0  loss_mask_3: 0.4258  loss_dice_3: 0.6517  loss_ce_4: 0.3856  loss_cate_4: 0  loss_mask_4: 0.4275  loss_dice_4: 0.6366  loss_ce_5: 0.3854  loss_cate_5: 0  loss_mask_5: 0.4046  loss_dice_5: 0.6697  loss_ce_6: 0.3599  loss_cate_6: 0  loss_mask_6: 0.4203  loss_dice_6: 0.6351  loss_ce_7: 0.3552  loss_cate_7: 0  loss_mask_7: 0.4134  loss_dice_7: 0.6683  loss_ce_8: 0.345  loss_cate_8: 0  loss_mask_8: 0.4232  loss_dice_8: 0.6732  time: 1.3875  data_time: 0.0145  lr: 8.7403e-06  max_mem: 23815M
[01/01 12:43:11] d2.utils.events INFO:  eta: 12:48:29  iter: 5439  total_loss: 20.12  loss_ce: 0.5565  loss_cate: 0.0943  loss_mask: 0.4379  loss_dice: 0.8097  loss_ce_0: 0.7843  loss_cate_0: 0  loss_mask_0: 0.5177  loss_dice_0: 0.8992  loss_ce_1: 0.6009  loss_cate_1: 0  loss_mask_1: 0.5034  loss_dice_1: 0.8792  loss_ce_2: 0.5382  loss_cate_2: 0  loss_mask_2: 0.5191  loss_dice_2: 0.8888  loss_ce_3: 0.5199  loss_cate_3: 0  loss_mask_3: 0.5029  loss_dice_3: 0.8664  loss_ce_4: 0.5031  loss_cate_4: 0  loss_mask_4: 0.499  loss_dice_4: 0.8455  loss_ce_5: 0.4982  loss_cate_5: 0  loss_mask_5: 0.4812  loss_dice_5: 0.853  loss_ce_6: 0.5439  loss_cate_6: 0  loss_mask_6: 0.4805  loss_dice_6: 0.8318  loss_ce_7: 0.5037  loss_cate_7: 0  loss_mask_7: 0.4514  loss_dice_7: 0.8563  loss_ce_8: 0.5044  loss_cate_8: 0  loss_mask_8: 0.433  loss_dice_8: 0.8575  time: 1.3875  data_time: 0.0155  lr: 8.7356e-06  max_mem: 23815M
[01/01 12:43:39] d2.utils.events INFO:  eta: 12:48:03  iter: 5459  total_loss: 15.51  loss_ce: 0.4017  loss_cate: 0.1327  loss_mask: 0.51  loss_dice: 0.6186  loss_ce_0: 0.7025  loss_cate_0: 0  loss_mask_0: 0.545  loss_dice_0: 0.6408  loss_ce_1: 0.3288  loss_cate_1: 0  loss_mask_1: 0.5268  loss_dice_1: 0.6861  loss_ce_2: 0.3344  loss_cate_2: 0  loss_mask_2: 0.5069  loss_dice_2: 0.6177  loss_ce_3: 0.4605  loss_cate_3: 0  loss_mask_3: 0.5002  loss_dice_3: 0.6144  loss_ce_4: 0.4249  loss_cate_4: 0  loss_mask_4: 0.492  loss_dice_4: 0.5933  loss_ce_5: 0.3677  loss_cate_5: 0  loss_mask_5: 0.5215  loss_dice_5: 0.6181  loss_ce_6: 0.3497  loss_cate_6: 0  loss_mask_6: 0.516  loss_dice_6: 0.5632  loss_ce_7: 0.3716  loss_cate_7: 0  loss_mask_7: 0.4978  loss_dice_7: 0.5954  loss_ce_8: 0.361  loss_cate_8: 0  loss_mask_8: 0.4861  loss_dice_8: 0.604  time: 1.3875  data_time: 0.0140  lr: 8.7309e-06  max_mem: 23815M
[01/01 12:44:07] d2.utils.events INFO:  eta: 12:47:37  iter: 5479  total_loss: 20.19  loss_ce: 0.4028  loss_cate: 0.1062  loss_mask: 0.6902  loss_dice: 0.7716  loss_ce_0: 0.8069  loss_cate_0: 0  loss_mask_0: 0.5761  loss_dice_0: 0.8305  loss_ce_1: 0.5762  loss_cate_1: 0  loss_mask_1: 0.6465  loss_dice_1: 0.8373  loss_ce_2: 0.5255  loss_cate_2: 0  loss_mask_2: 0.6169  loss_dice_2: 0.8307  loss_ce_3: 0.4856  loss_cate_3: 0  loss_mask_3: 0.62  loss_dice_3: 0.8131  loss_ce_4: 0.5449  loss_cate_4: 0  loss_mask_4: 0.7077  loss_dice_4: 0.8409  loss_ce_5: 0.4743  loss_cate_5: 0  loss_mask_5: 0.7087  loss_dice_5: 0.8155  loss_ce_6: 0.4377  loss_cate_6: 0  loss_mask_6: 0.6946  loss_dice_6: 0.8274  loss_ce_7: 0.5253  loss_cate_7: 0  loss_mask_7: 0.7079  loss_dice_7: 0.8222  loss_ce_8: 0.4408  loss_cate_8: 0  loss_mask_8: 0.6794  loss_dice_8: 0.8111  time: 1.3875  data_time: 0.0145  lr: 8.7262e-06  max_mem: 23815M
[01/01 12:44:35] d2.utils.events INFO:  eta: 12:47:12  iter: 5499  total_loss: 16.37  loss_ce: 0.4057  loss_cate: 0.1122  loss_mask: 0.4456  loss_dice: 0.538  loss_ce_0: 0.8596  loss_cate_0: 0  loss_mask_0: 0.4352  loss_dice_0: 0.6394  loss_ce_1: 0.5577  loss_cate_1: 0  loss_mask_1: 0.4343  loss_dice_1: 0.6675  loss_ce_2: 0.4158  loss_cate_2: 0  loss_mask_2: 0.419  loss_dice_2: 0.5664  loss_ce_3: 0.4958  loss_cate_3: 0  loss_mask_3: 0.4363  loss_dice_3: 0.5931  loss_ce_4: 0.4406  loss_cate_4: 0  loss_mask_4: 0.4413  loss_dice_4: 0.58  loss_ce_5: 0.4481  loss_cate_5: 0  loss_mask_5: 0.4214  loss_dice_5: 0.5633  loss_ce_6: 0.444  loss_cate_6: 0  loss_mask_6: 0.4532  loss_dice_6: 0.6246  loss_ce_7: 0.44  loss_cate_7: 0  loss_mask_7: 0.394  loss_dice_7: 0.6084  loss_ce_8: 0.4124  loss_cate_8: 0  loss_mask_8: 0.4507  loss_dice_8: 0.5791  time: 1.3875  data_time: 0.0116  lr: 8.7216e-06  max_mem: 23815M
[01/01 12:45:02] d2.utils.events INFO:  eta: 12:46:46  iter: 5519  total_loss: 14.98  loss_ce: 0.4126  loss_cate: 0.0953  loss_mask: 0.4421  loss_dice: 0.6626  loss_ce_0: 0.7824  loss_cate_0: 0  loss_mask_0: 0.4802  loss_dice_0: 0.7204  loss_ce_1: 0.4591  loss_cate_1: 0  loss_mask_1: 0.4424  loss_dice_1: 0.6269  loss_ce_2: 0.3665  loss_cate_2: 0  loss_mask_2: 0.4409  loss_dice_2: 0.6381  loss_ce_3: 0.3726  loss_cate_3: 0  loss_mask_3: 0.4387  loss_dice_3: 0.6476  loss_ce_4: 0.338  loss_cate_4: 0  loss_mask_4: 0.4354  loss_dice_4: 0.646  loss_ce_5: 0.382  loss_cate_5: 0  loss_mask_5: 0.4417  loss_dice_5: 0.6018  loss_ce_6: 0.4031  loss_cate_6: 0  loss_mask_6: 0.4509  loss_dice_6: 0.665  loss_ce_7: 0.383  loss_cate_7: 0  loss_mask_7: 0.4252  loss_dice_7: 0.6121  loss_ce_8: 0.3986  loss_cate_8: 0  loss_mask_8: 0.4456  loss_dice_8: 0.6125  time: 1.3875  data_time: 0.0136  lr: 8.7169e-06  max_mem: 23815M
[01/01 12:45:30] d2.utils.events INFO:  eta: 12:46:25  iter: 5539  total_loss: 17.97  loss_ce: 0.4425  loss_cate: 0.1183  loss_mask: 0.6138  loss_dice: 0.6996  loss_ce_0: 0.8516  loss_cate_0: 0  loss_mask_0: 0.6155  loss_dice_0: 0.7297  loss_ce_1: 0.4735  loss_cate_1: 0  loss_mask_1: 0.6491  loss_dice_1: 0.7737  loss_ce_2: 0.4247  loss_cate_2: 0  loss_mask_2: 0.5985  loss_dice_2: 0.7381  loss_ce_3: 0.4292  loss_cate_3: 0  loss_mask_3: 0.5894  loss_dice_3: 0.7185  loss_ce_4: 0.4147  loss_cate_4: 0  loss_mask_4: 0.5907  loss_dice_4: 0.7289  loss_ce_5: 0.4613  loss_cate_5: 0  loss_mask_5: 0.5706  loss_dice_5: 0.6804  loss_ce_6: 0.424  loss_cate_6: 0  loss_mask_6: 0.6014  loss_dice_6: 0.6939  loss_ce_7: 0.4307  loss_cate_7: 0  loss_mask_7: 0.5992  loss_dice_7: 0.6993  loss_ce_8: 0.4831  loss_cate_8: 0  loss_mask_8: 0.5907  loss_dice_8: 0.7346  time: 1.3876  data_time: 0.0139  lr: 8.7122e-06  max_mem: 23815M
[01/01 12:45:58] d2.utils.events INFO:  eta: 12:46:03  iter: 5559  total_loss: 20.77  loss_ce: 0.4678  loss_cate: 0.1188  loss_mask: 0.5257  loss_dice: 0.9141  loss_ce_0: 0.8534  loss_cate_0: 0  loss_mask_0: 0.5965  loss_dice_0: 0.8712  loss_ce_1: 0.5173  loss_cate_1: 0  loss_mask_1: 0.5206  loss_dice_1: 0.8694  loss_ce_2: 0.4675  loss_cate_2: 0  loss_mask_2: 0.5177  loss_dice_2: 0.8959  loss_ce_3: 0.522  loss_cate_3: 0  loss_mask_3: 0.5475  loss_dice_3: 0.8583  loss_ce_4: 0.5449  loss_cate_4: 0  loss_mask_4: 0.5273  loss_dice_4: 0.8505  loss_ce_5: 0.4959  loss_cate_5: 0  loss_mask_5: 0.5279  loss_dice_5: 0.8663  loss_ce_6: 0.5028  loss_cate_6: 0  loss_mask_6: 0.5406  loss_dice_6: 0.8181  loss_ce_7: 0.4991  loss_cate_7: 0  loss_mask_7: 0.5271  loss_dice_7: 0.9061  loss_ce_8: 0.4925  loss_cate_8: 0  loss_mask_8: 0.5467  loss_dice_8: 0.9131  time: 1.3875  data_time: 0.0131  lr: 8.7075e-06  max_mem: 23815M
[01/01 12:46:26] d2.utils.events INFO:  eta: 12:45:37  iter: 5579  total_loss: 20.22  loss_ce: 0.5544  loss_cate: 0.1236  loss_mask: 0.5925  loss_dice: 0.8247  loss_ce_0: 0.8456  loss_cate_0: 0  loss_mask_0: 0.5465  loss_dice_0: 0.8169  loss_ce_1: 0.5748  loss_cate_1: 0  loss_mask_1: 0.5524  loss_dice_1: 0.8091  loss_ce_2: 0.5405  loss_cate_2: 0  loss_mask_2: 0.5409  loss_dice_2: 0.8025  loss_ce_3: 0.573  loss_cate_3: 0  loss_mask_3: 0.5624  loss_dice_3: 0.818  loss_ce_4: 0.5395  loss_cate_4: 0  loss_mask_4: 0.6532  loss_dice_4: 0.7824  loss_ce_5: 0.5414  loss_cate_5: 0  loss_mask_5: 0.6005  loss_dice_5: 0.8116  loss_ce_6: 0.5529  loss_cate_6: 0  loss_mask_6: 0.6237  loss_dice_6: 0.8294  loss_ce_7: 0.5176  loss_cate_7: 0  loss_mask_7: 0.6431  loss_dice_7: 0.838  loss_ce_8: 0.5364  loss_cate_8: 0  loss_mask_8: 0.6149  loss_dice_8: 0.8442  time: 1.3876  data_time: 0.0158  lr: 8.7028e-06  max_mem: 23815M
[01/01 12:46:54] d2.utils.events INFO:  eta: 12:45:14  iter: 5599  total_loss: 18.72  loss_ce: 0.3515  loss_cate: 0.1168  loss_mask: 0.5488  loss_dice: 0.7911  loss_ce_0: 0.8045  loss_cate_0: 0  loss_mask_0: 0.5709  loss_dice_0: 0.7669  loss_ce_1: 0.3556  loss_cate_1: 0  loss_mask_1: 0.6061  loss_dice_1: 0.7831  loss_ce_2: 0.3595  loss_cate_2: 0  loss_mask_2: 0.5012  loss_dice_2: 0.7511  loss_ce_3: 0.3454  loss_cate_3: 0  loss_mask_3: 0.5435  loss_dice_3: 0.7176  loss_ce_4: 0.3469  loss_cate_4: 0  loss_mask_4: 0.5356  loss_dice_4: 0.7615  loss_ce_5: 0.3666  loss_cate_5: 0  loss_mask_5: 0.5209  loss_dice_5: 0.7615  loss_ce_6: 0.3397  loss_cate_6: 0  loss_mask_6: 0.554  loss_dice_6: 0.7657  loss_ce_7: 0.3623  loss_cate_7: 0  loss_mask_7: 0.5529  loss_dice_7: 0.7319  loss_ce_8: 0.325  loss_cate_8: 0  loss_mask_8: 0.5478  loss_dice_8: 0.8222  time: 1.3875  data_time: 0.0137  lr: 8.6981e-06  max_mem: 23815M
[01/01 12:47:22] d2.utils.events INFO:  eta: 12:44:50  iter: 5619  total_loss: 16.17  loss_ce: 0.3865  loss_cate: 0.1217  loss_mask: 0.4507  loss_dice: 0.7304  loss_ce_0: 0.6731  loss_cate_0: 0  loss_mask_0: 0.4831  loss_dice_0: 0.6958  loss_ce_1: 0.4602  loss_cate_1: 0  loss_mask_1: 0.4782  loss_dice_1: 0.7236  loss_ce_2: 0.4109  loss_cate_2: 0  loss_mask_2: 0.4582  loss_dice_2: 0.6555  loss_ce_3: 0.4193  loss_cate_3: 0  loss_mask_3: 0.4556  loss_dice_3: 0.6751  loss_ce_4: 0.4018  loss_cate_4: 0  loss_mask_4: 0.4683  loss_dice_4: 0.6621  loss_ce_5: 0.408  loss_cate_5: 0  loss_mask_5: 0.4787  loss_dice_5: 0.735  loss_ce_6: 0.3817  loss_cate_6: 0  loss_mask_6: 0.4768  loss_dice_6: 0.6942  loss_ce_7: 0.3886  loss_cate_7: 0  loss_mask_7: 0.4565  loss_dice_7: 0.7302  loss_ce_8: 0.3889  loss_cate_8: 0  loss_mask_8: 0.4377  loss_dice_8: 0.7152  time: 1.3876  data_time: 0.0125  lr: 8.6934e-06  max_mem: 23815M
[01/01 12:47:49] d2.utils.events INFO:  eta: 12:44:20  iter: 5639  total_loss: 18.9  loss_ce: 0.4657  loss_cate: 0.1325  loss_mask: 0.5273  loss_dice: 0.6873  loss_ce_0: 0.8199  loss_cate_0: 0  loss_mask_0: 0.5909  loss_dice_0: 0.7704  loss_ce_1: 0.5625  loss_cate_1: 0  loss_mask_1: 0.5585  loss_dice_1: 0.7557  loss_ce_2: 0.5329  loss_cate_2: 0  loss_mask_2: 0.5197  loss_dice_2: 0.6868  loss_ce_3: 0.5106  loss_cate_3: 0  loss_mask_3: 0.5476  loss_dice_3: 0.7211  loss_ce_4: 0.4486  loss_cate_4: 0  loss_mask_4: 0.5387  loss_dice_4: 0.7164  loss_ce_5: 0.4698  loss_cate_5: 0  loss_mask_5: 0.5449  loss_dice_5: 0.7181  loss_ce_6: 0.4739  loss_cate_6: 0  loss_mask_6: 0.5441  loss_dice_6: 0.6614  loss_ce_7: 0.4766  loss_cate_7: 0  loss_mask_7: 0.5339  loss_dice_7: 0.6933  loss_ce_8: 0.4868  loss_cate_8: 0  loss_mask_8: 0.5335  loss_dice_8: 0.7212  time: 1.3875  data_time: 0.0112  lr: 8.6887e-06  max_mem: 23815M
[01/01 12:48:17] d2.utils.events INFO:  eta: 12:43:54  iter: 5659  total_loss: 16.06  loss_ce: 0.3782  loss_cate: 0.1057  loss_mask: 0.5108  loss_dice: 0.5967  loss_ce_0: 0.6851  loss_cate_0: 0  loss_mask_0: 0.5662  loss_dice_0: 0.6547  loss_ce_1: 0.4054  loss_cate_1: 0  loss_mask_1: 0.5621  loss_dice_1: 0.6188  loss_ce_2: 0.4012  loss_cate_2: 0  loss_mask_2: 0.5315  loss_dice_2: 0.6609  loss_ce_3: 0.3668  loss_cate_3: 0  loss_mask_3: 0.5327  loss_dice_3: 0.6479  loss_ce_4: 0.3803  loss_cate_4: 0  loss_mask_4: 0.5061  loss_dice_4: 0.6801  loss_ce_5: 0.3352  loss_cate_5: 0  loss_mask_5: 0.5128  loss_dice_5: 0.6362  loss_ce_6: 0.3842  loss_cate_6: 0  loss_mask_6: 0.4965  loss_dice_6: 0.5977  loss_ce_7: 0.3915  loss_cate_7: 0  loss_mask_7: 0.539  loss_dice_7: 0.6183  loss_ce_8: 0.3446  loss_cate_8: 0  loss_mask_8: 0.5023  loss_dice_8: 0.606  time: 1.3875  data_time: 0.0120  lr: 8.6841e-06  max_mem: 23815M
[01/01 12:48:45] d2.utils.events INFO:  eta: 12:43:40  iter: 5679  total_loss: 21.1  loss_ce: 0.4957  loss_cate: 0.1605  loss_mask: 0.6592  loss_dice: 0.8652  loss_ce_0: 0.848  loss_cate_0: 0  loss_mask_0: 0.6505  loss_dice_0: 0.8756  loss_ce_1: 0.5306  loss_cate_1: 0  loss_mask_1: 0.6422  loss_dice_1: 0.8467  loss_ce_2: 0.5477  loss_cate_2: 0  loss_mask_2: 0.6154  loss_dice_2: 0.8849  loss_ce_3: 0.5499  loss_cate_3: 0  loss_mask_3: 0.6063  loss_dice_3: 0.9053  loss_ce_4: 0.543  loss_cate_4: 0  loss_mask_4: 0.6016  loss_dice_4: 0.9141  loss_ce_5: 0.5558  loss_cate_5: 0  loss_mask_5: 0.6248  loss_dice_5: 0.9189  loss_ce_6: 0.5689  loss_cate_6: 0  loss_mask_6: 0.6104  loss_dice_6: 0.9018  loss_ce_7: 0.5106  loss_cate_7: 0  loss_mask_7: 0.6305  loss_dice_7: 0.9462  loss_ce_8: 0.573  loss_cate_8: 0  loss_mask_8: 0.6587  loss_dice_8: 0.9273  time: 1.3875  data_time: 0.0114  lr: 8.6794e-06  max_mem: 23815M
[01/01 12:49:12] d2.utils.events INFO:  eta: 12:43:19  iter: 5699  total_loss: 16.43  loss_ce: 0.3638  loss_cate: 0.1631  loss_mask: 0.5812  loss_dice: 0.7933  loss_ce_0: 0.7217  loss_cate_0: 0  loss_mask_0: 0.5365  loss_dice_0: 0.708  loss_ce_1: 0.4395  loss_cate_1: 0  loss_mask_1: 0.5569  loss_dice_1: 0.707  loss_ce_2: 0.3744  loss_cate_2: 0  loss_mask_2: 0.5744  loss_dice_2: 0.6868  loss_ce_3: 0.3775  loss_cate_3: 0  loss_mask_3: 0.5596  loss_dice_3: 0.6795  loss_ce_4: 0.3971  loss_cate_4: 0  loss_mask_4: 0.5347  loss_dice_4: 0.6849  loss_ce_5: 0.3998  loss_cate_5: 0  loss_mask_5: 0.5448  loss_dice_5: 0.6572  loss_ce_6: 0.4072  loss_cate_6: 0  loss_mask_6: 0.5667  loss_dice_6: 0.6254  loss_ce_7: 0.3904  loss_cate_7: 0  loss_mask_7: 0.5502  loss_dice_7: 0.7709  loss_ce_8: 0.3695  loss_cate_8: 0  loss_mask_8: 0.573  loss_dice_8: 0.7216  time: 1.3875  data_time: 0.0128  lr: 8.6747e-06  max_mem: 23815M
[01/01 12:49:41] d2.utils.events INFO:  eta: 12:42:59  iter: 5719  total_loss: 23.5  loss_ce: 0.4745  loss_cate: 0.09636  loss_mask: 0.4654  loss_dice: 1.07  loss_ce_0: 0.8686  loss_cate_0: 0  loss_mask_0: 0.5308  loss_dice_0: 1.056  loss_ce_1: 0.5458  loss_cate_1: 0  loss_mask_1: 0.5038  loss_dice_1: 0.9844  loss_ce_2: 0.5759  loss_cate_2: 0  loss_mask_2: 0.4772  loss_dice_2: 0.9548  loss_ce_3: 0.5197  loss_cate_3: 0  loss_mask_3: 0.4793  loss_dice_3: 1.011  loss_ce_4: 0.638  loss_cate_4: 0  loss_mask_4: 0.4847  loss_dice_4: 1.044  loss_ce_5: 0.5929  loss_cate_5: 0  loss_mask_5: 0.4837  loss_dice_5: 0.9826  loss_ce_6: 0.5237  loss_cate_6: 0  loss_mask_6: 0.4888  loss_dice_6: 1.044  loss_ce_7: 0.5406  loss_cate_7: 0  loss_mask_7: 0.4802  loss_dice_7: 1.068  loss_ce_8: 0.5045  loss_cate_8: 0  loss_mask_8: 0.4675  loss_dice_8: 1.029  time: 1.3875  data_time: 0.0141  lr: 8.67e-06  max_mem: 23815M
[01/01 12:50:09] d2.utils.events INFO:  eta: 12:42:40  iter: 5739  total_loss: 15.75  loss_ce: 0.3932  loss_cate: 0.09528  loss_mask: 0.4902  loss_dice: 0.649  loss_ce_0: 0.5817  loss_cate_0: 0  loss_mask_0: 0.4981  loss_dice_0: 0.7177  loss_ce_1: 0.3985  loss_cate_1: 0  loss_mask_1: 0.5017  loss_dice_1: 0.7065  loss_ce_2: 0.3856  loss_cate_2: 0  loss_mask_2: 0.5141  loss_dice_2: 0.694  loss_ce_3: 0.401  loss_cate_3: 0  loss_mask_3: 0.4638  loss_dice_3: 0.6812  loss_ce_4: 0.411  loss_cate_4: 0  loss_mask_4: 0.4622  loss_dice_4: 0.6533  loss_ce_5: 0.3922  loss_cate_5: 0  loss_mask_5: 0.446  loss_dice_5: 0.6071  loss_ce_6: 0.4314  loss_cate_6: 0  loss_mask_6: 0.4552  loss_dice_6: 0.6456  loss_ce_7: 0.4444  loss_cate_7: 0  loss_mask_7: 0.4876  loss_dice_7: 0.5861  loss_ce_8: 0.4484  loss_cate_8: 0  loss_mask_8: 0.4634  loss_dice_8: 0.6684  time: 1.3876  data_time: 0.0134  lr: 8.6653e-06  max_mem: 23815M
[01/01 12:50:36] d2.utils.events INFO:  eta: 12:42:18  iter: 5759  total_loss: 14.95  loss_ce: 0.3994  loss_cate: 0.1662  loss_mask: 0.484  loss_dice: 0.5915  loss_ce_0: 0.7541  loss_cate_0: 0  loss_mask_0: 0.4939  loss_dice_0: 0.5783  loss_ce_1: 0.5282  loss_cate_1: 0  loss_mask_1: 0.5204  loss_dice_1: 0.5664  loss_ce_2: 0.4169  loss_cate_2: 0  loss_mask_2: 0.5067  loss_dice_2: 0.5713  loss_ce_3: 0.3971  loss_cate_3: 0  loss_mask_3: 0.4899  loss_dice_3: 0.5553  loss_ce_4: 0.4004  loss_cate_4: 0  loss_mask_4: 0.4805  loss_dice_4: 0.5782  loss_ce_5: 0.4145  loss_cate_5: 0  loss_mask_5: 0.4828  loss_dice_5: 0.5691  loss_ce_6: 0.3984  loss_cate_6: 0  loss_mask_6: 0.4709  loss_dice_6: 0.551  loss_ce_7: 0.4133  loss_cate_7: 0  loss_mask_7: 0.4743  loss_dice_7: 0.5474  loss_ce_8: 0.4344  loss_cate_8: 0  loss_mask_8: 0.4867  loss_dice_8: 0.6074  time: 1.3876  data_time: 0.0132  lr: 8.6606e-06  max_mem: 23815M
[01/01 12:51:04] d2.utils.events INFO:  eta: 12:41:57  iter: 5779  total_loss: 15.86  loss_ce: 0.4225  loss_cate: 0.1011  loss_mask: 0.3811  loss_dice: 0.7161  loss_ce_0: 0.7712  loss_cate_0: 0  loss_mask_0: 0.4784  loss_dice_0: 0.7732  loss_ce_1: 0.5271  loss_cate_1: 0  loss_mask_1: 0.3982  loss_dice_1: 0.6577  loss_ce_2: 0.4244  loss_cate_2: 0  loss_mask_2: 0.4099  loss_dice_2: 0.644  loss_ce_3: 0.4304  loss_cate_3: 0  loss_mask_3: 0.3974  loss_dice_3: 0.6903  loss_ce_4: 0.4547  loss_cate_4: 0  loss_mask_4: 0.3826  loss_dice_4: 0.7366  loss_ce_5: 0.4449  loss_cate_5: 0  loss_mask_5: 0.3859  loss_dice_5: 0.7423  loss_ce_6: 0.3943  loss_cate_6: 0  loss_mask_6: 0.377  loss_dice_6: 0.729  loss_ce_7: 0.4177  loss_cate_7: 0  loss_mask_7: 0.3695  loss_dice_7: 0.7075  loss_ce_8: 0.4287  loss_cate_8: 0  loss_mask_8: 0.3838  loss_dice_8: 0.6736  time: 1.3875  data_time: 0.0130  lr: 8.6559e-06  max_mem: 23815M
[01/01 12:51:32] d2.utils.events INFO:  eta: 12:41:32  iter: 5799  total_loss: 18.39  loss_ce: 0.4492  loss_cate: 0.08904  loss_mask: 0.3554  loss_dice: 0.7715  loss_ce_0: 0.8086  loss_cate_0: 0  loss_mask_0: 0.4064  loss_dice_0: 0.7782  loss_ce_1: 0.5143  loss_cate_1: 0  loss_mask_1: 0.3515  loss_dice_1: 0.7607  loss_ce_2: 0.5323  loss_cate_2: 0  loss_mask_2: 0.403  loss_dice_2: 0.8709  loss_ce_3: 0.5218  loss_cate_3: 0  loss_mask_3: 0.3427  loss_dice_3: 0.825  loss_ce_4: 0.5409  loss_cate_4: 0  loss_mask_4: 0.3383  loss_dice_4: 0.7828  loss_ce_5: 0.4548  loss_cate_5: 0  loss_mask_5: 0.3613  loss_dice_5: 0.7488  loss_ce_6: 0.4839  loss_cate_6: 0  loss_mask_6: 0.3516  loss_dice_6: 0.7997  loss_ce_7: 0.5173  loss_cate_7: 0  loss_mask_7: 0.3539  loss_dice_7: 0.7776  loss_ce_8: 0.4837  loss_cate_8: 0  loss_mask_8: 0.3585  loss_dice_8: 0.786  time: 1.3875  data_time: 0.0126  lr: 8.6512e-06  max_mem: 23815M
[01/01 12:51:59] d2.utils.events INFO:  eta: 12:41:10  iter: 5819  total_loss: 14.75  loss_ce: 0.3402  loss_cate: 0.1248  loss_mask: 0.3963  loss_dice: 0.523  loss_ce_0: 0.7673  loss_cate_0: 0  loss_mask_0: 0.3797  loss_dice_0: 0.6119  loss_ce_1: 0.4422  loss_cate_1: 0  loss_mask_1: 0.369  loss_dice_1: 0.5439  loss_ce_2: 0.4503  loss_cate_2: 0  loss_mask_2: 0.3886  loss_dice_2: 0.5822  loss_ce_3: 0.4125  loss_cate_3: 0  loss_mask_3: 0.3704  loss_dice_3: 0.6009  loss_ce_4: 0.3898  loss_cate_4: 0  loss_mask_4: 0.3886  loss_dice_4: 0.5645  loss_ce_5: 0.3741  loss_cate_5: 0  loss_mask_5: 0.4395  loss_dice_5: 0.58  loss_ce_6: 0.3824  loss_cate_6: 0  loss_mask_6: 0.3815  loss_dice_6: 0.5058  loss_ce_7: 0.3719  loss_cate_7: 0  loss_mask_7: 0.3856  loss_dice_7: 0.5465  loss_ce_8: 0.3634  loss_cate_8: 0  loss_mask_8: 0.3663  loss_dice_8: 0.5421  time: 1.3875  data_time: 0.0131  lr: 8.6465e-06  max_mem: 23815M
[01/01 12:52:27] d2.utils.events INFO:  eta: 12:40:44  iter: 5839  total_loss: 23.65  loss_ce: 0.5278  loss_cate: 0.1425  loss_mask: 0.6057  loss_dice: 0.9725  loss_ce_0: 0.7994  loss_cate_0: 0  loss_mask_0: 0.5355  loss_dice_0: 0.9133  loss_ce_1: 0.6242  loss_cate_1: 0  loss_mask_1: 0.6344  loss_dice_1: 0.9309  loss_ce_2: 0.5822  loss_cate_2: 0  loss_mask_2: 0.6231  loss_dice_2: 0.9483  loss_ce_3: 0.5464  loss_cate_3: 0  loss_mask_3: 0.5379  loss_dice_3: 0.9867  loss_ce_4: 0.5491  loss_cate_4: 0  loss_mask_4: 0.5803  loss_dice_4: 0.9691  loss_ce_5: 0.6249  loss_cate_5: 0  loss_mask_5: 0.572  loss_dice_5: 0.9194  loss_ce_6: 0.5789  loss_cate_6: 0  loss_mask_6: 0.5713  loss_dice_6: 0.9971  loss_ce_7: 0.5208  loss_cate_7: 0  loss_mask_7: 0.5769  loss_dice_7: 1.016  loss_ce_8: 0.5679  loss_cate_8: 0  loss_mask_8: 0.5623  loss_dice_8: 0.9766  time: 1.3875  data_time: 0.0125  lr: 8.6418e-06  max_mem: 23815M
[01/01 12:52:55] d2.utils.events INFO:  eta: 12:40:16  iter: 5859  total_loss: 18.02  loss_ce: 0.4273  loss_cate: 0.1121  loss_mask: 0.4242  loss_dice: 0.7258  loss_ce_0: 0.6709  loss_cate_0: 0  loss_mask_0: 0.4731  loss_dice_0: 0.7291  loss_ce_1: 0.5417  loss_cate_1: 0  loss_mask_1: 0.4309  loss_dice_1: 0.7361  loss_ce_2: 0.4602  loss_cate_2: 0  loss_mask_2: 0.4262  loss_dice_2: 0.6676  loss_ce_3: 0.4608  loss_cate_3: 0  loss_mask_3: 0.4186  loss_dice_3: 0.7191  loss_ce_4: 0.4572  loss_cate_4: 0  loss_mask_4: 0.4109  loss_dice_4: 0.6877  loss_ce_5: 0.4571  loss_cate_5: 0  loss_mask_5: 0.461  loss_dice_5: 0.7227  loss_ce_6: 0.4149  loss_cate_6: 0  loss_mask_6: 0.4777  loss_dice_6: 0.747  loss_ce_7: 0.4543  loss_cate_7: 0  loss_mask_7: 0.425  loss_dice_7: 0.7603  loss_ce_8: 0.3999  loss_cate_8: 0  loss_mask_8: 0.4211  loss_dice_8: 0.7751  time: 1.3874  data_time: 0.0133  lr: 8.6372e-06  max_mem: 23815M
[01/01 12:53:22] d2.utils.events INFO:  eta: 12:39:51  iter: 5879  total_loss: 17.76  loss_ce: 0.4475  loss_cate: 0.09342  loss_mask: 0.4059  loss_dice: 0.8092  loss_ce_0: 0.8483  loss_cate_0: 0  loss_mask_0: 0.4263  loss_dice_0: 0.7716  loss_ce_1: 0.4775  loss_cate_1: 0  loss_mask_1: 0.4388  loss_dice_1: 0.8728  loss_ce_2: 0.4535  loss_cate_2: 0  loss_mask_2: 0.4416  loss_dice_2: 0.8017  loss_ce_3: 0.4618  loss_cate_3: 0  loss_mask_3: 0.4174  loss_dice_3: 0.8139  loss_ce_4: 0.4211  loss_cate_4: 0  loss_mask_4: 0.4287  loss_dice_4: 0.8521  loss_ce_5: 0.472  loss_cate_5: 0  loss_mask_5: 0.4218  loss_dice_5: 0.815  loss_ce_6: 0.4753  loss_cate_6: 0  loss_mask_6: 0.4382  loss_dice_6: 0.7692  loss_ce_7: 0.4798  loss_cate_7: 0  loss_mask_7: 0.4227  loss_dice_7: 0.7957  loss_ce_8: 0.4856  loss_cate_8: 0  loss_mask_8: 0.4001  loss_dice_8: 0.8139  time: 1.3874  data_time: 0.0121  lr: 8.6325e-06  max_mem: 23815M
[01/01 12:53:50] d2.utils.events INFO:  eta: 12:39:24  iter: 5899  total_loss: 20.02  loss_ce: 0.5068  loss_cate: 0.1191  loss_mask: 0.5188  loss_dice: 0.8441  loss_ce_0: 0.9164  loss_cate_0: 0  loss_mask_0: 0.4783  loss_dice_0: 0.7744  loss_ce_1: 0.5252  loss_cate_1: 0  loss_mask_1: 0.4785  loss_dice_1: 0.7999  loss_ce_2: 0.5577  loss_cate_2: 0  loss_mask_2: 0.4885  loss_dice_2: 0.7884  loss_ce_3: 0.5629  loss_cate_3: 0  loss_mask_3: 0.5414  loss_dice_3: 0.8615  loss_ce_4: 0.4776  loss_cate_4: 0  loss_mask_4: 0.467  loss_dice_4: 0.8838  loss_ce_5: 0.5126  loss_cate_5: 0  loss_mask_5: 0.4791  loss_dice_5: 0.7948  loss_ce_6: 0.4943  loss_cate_6: 0  loss_mask_6: 0.4966  loss_dice_6: 0.8002  loss_ce_7: 0.5066  loss_cate_7: 0  loss_mask_7: 0.5034  loss_dice_7: 0.8981  loss_ce_8: 0.4996  loss_cate_8: 0  loss_mask_8: 0.4883  loss_dice_8: 0.8587  time: 1.3874  data_time: 0.0117  lr: 8.6278e-06  max_mem: 23815M
[01/01 12:54:17] d2.utils.events INFO:  eta: 12:38:52  iter: 5919  total_loss: 15.6  loss_ce: 0.2953  loss_cate: 0.1139  loss_mask: 0.4412  loss_dice: 0.6206  loss_ce_0: 0.684  loss_cate_0: 0  loss_mask_0: 0.4258  loss_dice_0: 0.7244  loss_ce_1: 0.4453  loss_cate_1: 0  loss_mask_1: 0.4261  loss_dice_1: 0.679  loss_ce_2: 0.4117  loss_cate_2: 0  loss_mask_2: 0.3949  loss_dice_2: 0.605  loss_ce_3: 0.3902  loss_cate_3: 0  loss_mask_3: 0.3952  loss_dice_3: 0.6345  loss_ce_4: 0.3652  loss_cate_4: 0  loss_mask_4: 0.4168  loss_dice_4: 0.6112  loss_ce_5: 0.3  loss_cate_5: 0  loss_mask_5: 0.4239  loss_dice_5: 0.6611  loss_ce_6: 0.2699  loss_cate_6: 0  loss_mask_6: 0.4355  loss_dice_6: 0.6343  loss_ce_7: 0.2821  loss_cate_7: 0  loss_mask_7: 0.4383  loss_dice_7: 0.6053  loss_ce_8: 0.3807  loss_cate_8: 0  loss_mask_8: 0.424  loss_dice_8: 0.6024  time: 1.3873  data_time: 0.0107  lr: 8.6231e-06  max_mem: 23815M
[01/01 12:54:45] d2.utils.events INFO:  eta: 12:38:25  iter: 5939  total_loss: 20.5  loss_ce: 0.3595  loss_cate: 0.1092  loss_mask: 0.4874  loss_dice: 0.904  loss_ce_0: 0.7855  loss_cate_0: 0  loss_mask_0: 0.5373  loss_dice_0: 0.8562  loss_ce_1: 0.4228  loss_cate_1: 0  loss_mask_1: 0.4601  loss_dice_1: 0.9243  loss_ce_2: 0.3427  loss_cate_2: 0  loss_mask_2: 0.455  loss_dice_2: 0.8673  loss_ce_3: 0.4021  loss_cate_3: 0  loss_mask_3: 0.479  loss_dice_3: 0.8858  loss_ce_4: 0.3863  loss_cate_4: 0  loss_mask_4: 0.4589  loss_dice_4: 0.8778  loss_ce_5: 0.3527  loss_cate_5: 0  loss_mask_5: 0.459  loss_dice_5: 0.866  loss_ce_6: 0.3799  loss_cate_6: 0  loss_mask_6: 0.4568  loss_dice_6: 0.8774  loss_ce_7: 0.3491  loss_cate_7: 0  loss_mask_7: 0.4505  loss_dice_7: 0.8922  loss_ce_8: 0.3637  loss_cate_8: 0  loss_mask_8: 0.4865  loss_dice_8: 0.8908  time: 1.3873  data_time: 0.0158  lr: 8.6184e-06  max_mem: 23815M
[01/01 12:55:13] d2.utils.events INFO:  eta: 12:37:57  iter: 5959  total_loss: 19.19  loss_ce: 0.4686  loss_cate: 0.1152  loss_mask: 0.4988  loss_dice: 0.858  loss_ce_0: 0.8165  loss_cate_0: 0  loss_mask_0: 0.4779  loss_dice_0: 0.87  loss_ce_1: 0.4971  loss_cate_1: 0  loss_mask_1: 0.6043  loss_dice_1: 0.8619  loss_ce_2: 0.4573  loss_cate_2: 0  loss_mask_2: 0.5491  loss_dice_2: 0.8396  loss_ce_3: 0.3649  loss_cate_3: 0  loss_mask_3: 0.5902  loss_dice_3: 0.7797  loss_ce_4: 0.3499  loss_cate_4: 0  loss_mask_4: 0.5501  loss_dice_4: 0.8514  loss_ce_5: 0.4058  loss_cate_5: 0  loss_mask_5: 0.5836  loss_dice_5: 0.8622  loss_ce_6: 0.334  loss_cate_6: 0  loss_mask_6: 0.5863  loss_dice_6: 0.8751  loss_ce_7: 0.3242  loss_cate_7: 0  loss_mask_7: 0.5527  loss_dice_7: 0.9069  loss_ce_8: 0.3095  loss_cate_8: 0  loss_mask_8: 0.5524  loss_dice_8: 0.8449  time: 1.3873  data_time: 0.0127  lr: 8.6137e-06  max_mem: 23815M
[01/01 12:55:40] d2.utils.events INFO:  eta: 12:37:29  iter: 5979  total_loss: 15.9  loss_ce: 0.2846  loss_cate: 0.08835  loss_mask: 0.439  loss_dice: 0.7202  loss_ce_0: 0.7616  loss_cate_0: 0  loss_mask_0: 0.3877  loss_dice_0: 0.6988  loss_ce_1: 0.3345  loss_cate_1: 0  loss_mask_1: 0.417  loss_dice_1: 0.7735  loss_ce_2: 0.348  loss_cate_2: 0  loss_mask_2: 0.4515  loss_dice_2: 0.715  loss_ce_3: 0.3215  loss_cate_3: 0  loss_mask_3: 0.4506  loss_dice_3: 0.7308  loss_ce_4: 0.2988  loss_cate_4: 0  loss_mask_4: 0.4588  loss_dice_4: 0.7463  loss_ce_5: 0.2884  loss_cate_5: 0  loss_mask_5: 0.4391  loss_dice_5: 0.734  loss_ce_6: 0.2946  loss_cate_6: 0  loss_mask_6: 0.4597  loss_dice_6: 0.725  loss_ce_7: 0.2585  loss_cate_7: 0  loss_mask_7: 0.4369  loss_dice_7: 0.7501  loss_ce_8: 0.2532  loss_cate_8: 0  loss_mask_8: 0.446  loss_dice_8: 0.7856  time: 1.3873  data_time: 0.0120  lr: 8.609e-06  max_mem: 23815M
[01/01 12:56:08] d2.utils.events INFO:  eta: 12:37:02  iter: 5999  total_loss: 18.79  loss_ce: 0.3874  loss_cate: 0.08737  loss_mask: 0.3672  loss_dice: 0.8047  loss_ce_0: 0.7013  loss_cate_0: 0  loss_mask_0: 0.3927  loss_dice_0: 0.8039  loss_ce_1: 0.3366  loss_cate_1: 0  loss_mask_1: 0.4106  loss_dice_1: 0.8448  loss_ce_2: 0.4051  loss_cate_2: 0  loss_mask_2: 0.4116  loss_dice_2: 0.7756  loss_ce_3: 0.392  loss_cate_3: 0  loss_mask_3: 0.364  loss_dice_3: 0.7953  loss_ce_4: 0.344  loss_cate_4: 0  loss_mask_4: 0.3944  loss_dice_4: 0.7708  loss_ce_5: 0.3396  loss_cate_5: 0  loss_mask_5: 0.3924  loss_dice_5: 0.8083  loss_ce_6: 0.3576  loss_cate_6: 0  loss_mask_6: 0.3903  loss_dice_6: 0.767  loss_ce_7: 0.3413  loss_cate_7: 0  loss_mask_7: 0.3915  loss_dice_7: 0.7783  loss_ce_8: 0.3384  loss_cate_8: 0  loss_mask_8: 0.3762  loss_dice_8: 0.8131  time: 1.3872  data_time: 0.0127  lr: 8.6043e-06  max_mem: 23815M
[01/01 12:56:36] d2.utils.events INFO:  eta: 12:36:27  iter: 6019  total_loss: 14.36  loss_ce: 0.3788  loss_cate: 0.1018  loss_mask: 0.4092  loss_dice: 0.4789  loss_ce_0: 0.7114  loss_cate_0: 0  loss_mask_0: 0.4424  loss_dice_0: 0.5482  loss_ce_1: 0.3649  loss_cate_1: 0  loss_mask_1: 0.4623  loss_dice_1: 0.5489  loss_ce_2: 0.391  loss_cate_2: 0  loss_mask_2: 0.4359  loss_dice_2: 0.5072  loss_ce_3: 0.3507  loss_cate_3: 0  loss_mask_3: 0.4271  loss_dice_3: 0.554  loss_ce_4: 0.4299  loss_cate_4: 0  loss_mask_4: 0.4307  loss_dice_4: 0.5033  loss_ce_5: 0.3634  loss_cate_5: 0  loss_mask_5: 0.4444  loss_dice_5: 0.5201  loss_ce_6: 0.3466  loss_cate_6: 0  loss_mask_6: 0.413  loss_dice_6: 0.5451  loss_ce_7: 0.3153  loss_cate_7: 0  loss_mask_7: 0.4352  loss_dice_7: 0.504  loss_ce_8: 0.3701  loss_cate_8: 0  loss_mask_8: 0.4279  loss_dice_8: 0.5239  time: 1.3872  data_time: 0.0124  lr: 8.5996e-06  max_mem: 23815M
[01/01 12:57:03] d2.utils.events INFO:  eta: 12:35:54  iter: 6039  total_loss: 17.16  loss_ce: 0.5237  loss_cate: 0.09514  loss_mask: 0.4468  loss_dice: 0.7323  loss_ce_0: 0.6893  loss_cate_0: 0  loss_mask_0: 0.5264  loss_dice_0: 0.7439  loss_ce_1: 0.5506  loss_cate_1: 0  loss_mask_1: 0.4317  loss_dice_1: 0.6811  loss_ce_2: 0.4496  loss_cate_2: 0  loss_mask_2: 0.4065  loss_dice_2: 0.8255  loss_ce_3: 0.4376  loss_cate_3: 0  loss_mask_3: 0.4412  loss_dice_3: 0.7372  loss_ce_4: 0.4371  loss_cate_4: 0  loss_mask_4: 0.4308  loss_dice_4: 0.7442  loss_ce_5: 0.44  loss_cate_5: 0  loss_mask_5: 0.4277  loss_dice_5: 0.7008  loss_ce_6: 0.4978  loss_cate_6: 0  loss_mask_6: 0.4316  loss_dice_6: 0.7143  loss_ce_7: 0.4881  loss_cate_7: 0  loss_mask_7: 0.4299  loss_dice_7: 0.7294  loss_ce_8: 0.4872  loss_cate_8: 0  loss_mask_8: 0.4442  loss_dice_8: 0.7181  time: 1.3872  data_time: 0.0111  lr: 8.5949e-06  max_mem: 23815M
[01/01 12:57:31] d2.utils.events INFO:  eta: 12:35:29  iter: 6059  total_loss: 18.44  loss_ce: 0.3441  loss_cate: 0.1074  loss_mask: 0.4712  loss_dice: 0.7497  loss_ce_0: 0.7637  loss_cate_0: 0  loss_mask_0: 0.4956  loss_dice_0: 0.7003  loss_ce_1: 0.4277  loss_cate_1: 0  loss_mask_1: 0.5369  loss_dice_1: 0.7214  loss_ce_2: 0.3188  loss_cate_2: 0  loss_mask_2: 0.4544  loss_dice_2: 0.7019  loss_ce_3: 0.294  loss_cate_3: 0  loss_mask_3: 0.4764  loss_dice_3: 0.6865  loss_ce_4: 0.3571  loss_cate_4: 0  loss_mask_4: 0.4781  loss_dice_4: 0.725  loss_ce_5: 0.3115  loss_cate_5: 0  loss_mask_5: 0.4681  loss_dice_5: 0.6987  loss_ce_6: 0.3195  loss_cate_6: 0  loss_mask_6: 0.4795  loss_dice_6: 0.7097  loss_ce_7: 0.2783  loss_cate_7: 0  loss_mask_7: 0.4741  loss_dice_7: 0.7033  loss_ce_8: 0.3496  loss_cate_8: 0  loss_mask_8: 0.4769  loss_dice_8: 0.7332  time: 1.3871  data_time: 0.0136  lr: 8.5902e-06  max_mem: 23815M
[01/01 12:57:58] d2.utils.events INFO:  eta: 12:35:12  iter: 6079  total_loss: 16.32  loss_ce: 0.4113  loss_cate: 0.124  loss_mask: 0.3568  loss_dice: 0.6809  loss_ce_0: 0.807  loss_cate_0: 0  loss_mask_0: 0.3494  loss_dice_0: 0.7046  loss_ce_1: 0.4347  loss_cate_1: 0  loss_mask_1: 0.3905  loss_dice_1: 0.7319  loss_ce_2: 0.4382  loss_cate_2: 0  loss_mask_2: 0.3727  loss_dice_2: 0.6143  loss_ce_3: 0.4351  loss_cate_3: 0  loss_mask_3: 0.3549  loss_dice_3: 0.5864  loss_ce_4: 0.4374  loss_cate_4: 0  loss_mask_4: 0.3517  loss_dice_4: 0.6328  loss_ce_5: 0.3902  loss_cate_5: 0  loss_mask_5: 0.3498  loss_dice_5: 0.6068  loss_ce_6: 0.4557  loss_cate_6: 0  loss_mask_6: 0.3663  loss_dice_6: 0.6285  loss_ce_7: 0.3877  loss_cate_7: 0  loss_mask_7: 0.3655  loss_dice_7: 0.6735  loss_ce_8: 0.4189  loss_cate_8: 0  loss_mask_8: 0.3497  loss_dice_8: 0.6018  time: 1.3871  data_time: 0.0122  lr: 8.5855e-06  max_mem: 23815M
[01/01 12:58:26] d2.utils.events INFO:  eta: 12:34:47  iter: 6099  total_loss: 18.34  loss_ce: 0.4417  loss_cate: 0.1198  loss_mask: 0.5904  loss_dice: 0.871  loss_ce_0: 0.7143  loss_cate_0: 0  loss_mask_0: 0.5882  loss_dice_0: 0.7614  loss_ce_1: 0.4975  loss_cate_1: 0  loss_mask_1: 0.5353  loss_dice_1: 0.7584  loss_ce_2: 0.4715  loss_cate_2: 0  loss_mask_2: 0.5933  loss_dice_2: 0.7012  loss_ce_3: 0.4424  loss_cate_3: 0  loss_mask_3: 0.5838  loss_dice_3: 0.7952  loss_ce_4: 0.4774  loss_cate_4: 0  loss_mask_4: 0.5328  loss_dice_4: 0.7338  loss_ce_5: 0.4496  loss_cate_5: 0  loss_mask_5: 0.5603  loss_dice_5: 0.7126  loss_ce_6: 0.4882  loss_cate_6: 0  loss_mask_6: 0.6428  loss_dice_6: 0.7718  loss_ce_7: 0.4351  loss_cate_7: 0  loss_mask_7: 0.5615  loss_dice_7: 0.6901  loss_ce_8: 0.4392  loss_cate_8: 0  loss_mask_8: 0.6197  loss_dice_8: 0.7408  time: 1.3871  data_time: 0.0128  lr: 8.5808e-06  max_mem: 23815M
[01/01 12:58:54] d2.utils.events INFO:  eta: 12:34:17  iter: 6119  total_loss: 15.7  loss_ce: 0.3541  loss_cate: 0.1015  loss_mask: 0.4636  loss_dice: 0.623  loss_ce_0: 0.7496  loss_cate_0: 0  loss_mask_0: 0.4498  loss_dice_0: 0.6366  loss_ce_1: 0.4426  loss_cate_1: 0  loss_mask_1: 0.4636  loss_dice_1: 0.7094  loss_ce_2: 0.4064  loss_cate_2: 0  loss_mask_2: 0.488  loss_dice_2: 0.6667  loss_ce_3: 0.4009  loss_cate_3: 0  loss_mask_3: 0.5223  loss_dice_3: 0.6652  loss_ce_4: 0.3529  loss_cate_4: 0  loss_mask_4: 0.5164  loss_dice_4: 0.6382  loss_ce_5: 0.414  loss_cate_5: 0  loss_mask_5: 0.5308  loss_dice_5: 0.6728  loss_ce_6: 0.373  loss_cate_6: 0  loss_mask_6: 0.4901  loss_dice_6: 0.6505  loss_ce_7: 0.3449  loss_cate_7: 0  loss_mask_7: 0.5145  loss_dice_7: 0.6731  loss_ce_8: 0.3291  loss_cate_8: 0  loss_mask_8: 0.4726  loss_dice_8: 0.6295  time: 1.3871  data_time: 0.0139  lr: 8.5762e-06  max_mem: 23815M
[01/01 12:59:21] d2.utils.events INFO:  eta: 12:33:51  iter: 6139  total_loss: 20.09  loss_ce: 0.3621  loss_cate: 0.1149  loss_mask: 0.5602  loss_dice: 0.8148  loss_ce_0: 0.7927  loss_cate_0: 0  loss_mask_0: 0.5644  loss_dice_0: 0.8261  loss_ce_1: 0.3485  loss_cate_1: 0  loss_mask_1: 0.6168  loss_dice_1: 0.8833  loss_ce_2: 0.3658  loss_cate_2: 0  loss_mask_2: 0.5856  loss_dice_2: 0.8579  loss_ce_3: 0.4165  loss_cate_3: 0  loss_mask_3: 0.5683  loss_dice_3: 0.8664  loss_ce_4: 0.3465  loss_cate_4: 0  loss_mask_4: 0.5604  loss_dice_4: 0.8255  loss_ce_5: 0.4262  loss_cate_5: 0  loss_mask_5: 0.5881  loss_dice_5: 0.7968  loss_ce_6: 0.3981  loss_cate_6: 0  loss_mask_6: 0.5618  loss_dice_6: 0.7933  loss_ce_7: 0.431  loss_cate_7: 0  loss_mask_7: 0.5619  loss_dice_7: 0.8432  loss_ce_8: 0.4012  loss_cate_8: 0  loss_mask_8: 0.5643  loss_dice_8: 0.8131  time: 1.3870  data_time: 0.0136  lr: 8.5715e-06  max_mem: 23815M
[01/01 12:59:49] d2.utils.events INFO:  eta: 12:33:26  iter: 6159  total_loss: 16.62  loss_ce: 0.5016  loss_cate: 0.1347  loss_mask: 0.5271  loss_dice: 0.6176  loss_ce_0: 0.9003  loss_cate_0: 0  loss_mask_0: 0.5582  loss_dice_0: 0.6823  loss_ce_1: 0.5261  loss_cate_1: 0  loss_mask_1: 0.5049  loss_dice_1: 0.7517  loss_ce_2: 0.4513  loss_cate_2: 0  loss_mask_2: 0.4596  loss_dice_2: 0.6416  loss_ce_3: 0.4917  loss_cate_3: 0  loss_mask_3: 0.521  loss_dice_3: 0.6614  loss_ce_4: 0.4706  loss_cate_4: 0  loss_mask_4: 0.5492  loss_dice_4: 0.7102  loss_ce_5: 0.486  loss_cate_5: 0  loss_mask_5: 0.5049  loss_dice_5: 0.6989  loss_ce_6: 0.4734  loss_cate_6: 0  loss_mask_6: 0.5453  loss_dice_6: 0.6688  loss_ce_7: 0.5216  loss_cate_7: 0  loss_mask_7: 0.5356  loss_dice_7: 0.6559  loss_ce_8: 0.4549  loss_cate_8: 0  loss_mask_8: 0.4926  loss_dice_8: 0.6211  time: 1.3870  data_time: 0.0130  lr: 8.5668e-06  max_mem: 23815M
[01/01 13:00:16] d2.utils.events INFO:  eta: 12:32:58  iter: 6179  total_loss: 19.48  loss_ce: 0.4737  loss_cate: 0.1225  loss_mask: 0.4856  loss_dice: 0.823  loss_ce_0: 0.8501  loss_cate_0: 0  loss_mask_0: 0.4576  loss_dice_0: 0.7908  loss_ce_1: 0.3704  loss_cate_1: 0  loss_mask_1: 0.5003  loss_dice_1: 0.8881  loss_ce_2: 0.3699  loss_cate_2: 0  loss_mask_2: 0.4538  loss_dice_2: 0.8588  loss_ce_3: 0.3508  loss_cate_3: 0  loss_mask_3: 0.4746  loss_dice_3: 0.755  loss_ce_4: 0.3594  loss_cate_4: 0  loss_mask_4: 0.4989  loss_dice_4: 0.8751  loss_ce_5: 0.3641  loss_cate_5: 0  loss_mask_5: 0.4848  loss_dice_5: 0.8808  loss_ce_6: 0.4073  loss_cate_6: 0  loss_mask_6: 0.4845  loss_dice_6: 0.8281  loss_ce_7: 0.4677  loss_cate_7: 0  loss_mask_7: 0.4673  loss_dice_7: 0.7762  loss_ce_8: 0.453  loss_cate_8: 0  loss_mask_8: 0.4715  loss_dice_8: 0.7799  time: 1.3870  data_time: 0.0123  lr: 8.5621e-06  max_mem: 23815M
[01/01 13:00:44] d2.utils.events INFO:  eta: 12:32:33  iter: 6199  total_loss: 14.53  loss_ce: 0.3394  loss_cate: 0.1074  loss_mask: 0.5321  loss_dice: 0.5984  loss_ce_0: 0.6293  loss_cate_0: 0  loss_mask_0: 0.4883  loss_dice_0: 0.6544  loss_ce_1: 0.3619  loss_cate_1: 0  loss_mask_1: 0.5174  loss_dice_1: 0.6521  loss_ce_2: 0.3279  loss_cate_2: 0  loss_mask_2: 0.5128  loss_dice_2: 0.6433  loss_ce_3: 0.3477  loss_cate_3: 0  loss_mask_3: 0.5383  loss_dice_3: 0.6101  loss_ce_4: 0.3231  loss_cate_4: 0  loss_mask_4: 0.5391  loss_dice_4: 0.6082  loss_ce_5: 0.3554  loss_cate_5: 0  loss_mask_5: 0.5123  loss_dice_5: 0.5935  loss_ce_6: 0.3471  loss_cate_6: 0  loss_mask_6: 0.5458  loss_dice_6: 0.6459  loss_ce_7: 0.3372  loss_cate_7: 0  loss_mask_7: 0.4989  loss_dice_7: 0.5937  loss_ce_8: 0.3702  loss_cate_8: 0  loss_mask_8: 0.521  loss_dice_8: 0.6139  time: 1.3869  data_time: 0.0118  lr: 8.5574e-06  max_mem: 23815M
[01/01 13:01:12] d2.utils.events INFO:  eta: 12:32:08  iter: 6219  total_loss: 21.64  loss_ce: 0.5548  loss_cate: 0.1044  loss_mask: 0.5228  loss_dice: 0.854  loss_ce_0: 0.9771  loss_cate_0: 0  loss_mask_0: 0.5035  loss_dice_0: 0.8422  loss_ce_1: 0.6195  loss_cate_1: 0  loss_mask_1: 0.5338  loss_dice_1: 0.8684  loss_ce_2: 0.5491  loss_cate_2: 0  loss_mask_2: 0.5052  loss_dice_2: 0.8778  loss_ce_3: 0.6271  loss_cate_3: 0  loss_mask_3: 0.4897  loss_dice_3: 0.8529  loss_ce_4: 0.5922  loss_cate_4: 0  loss_mask_4: 0.5082  loss_dice_4: 0.8095  loss_ce_5: 0.5401  loss_cate_5: 0  loss_mask_5: 0.5004  loss_dice_5: 0.8476  loss_ce_6: 0.5262  loss_cate_6: 0  loss_mask_6: 0.494  loss_dice_6: 0.8525  loss_ce_7: 0.6069  loss_cate_7: 0  loss_mask_7: 0.5055  loss_dice_7: 0.8447  loss_ce_8: 0.5614  loss_cate_8: 0  loss_mask_8: 0.5346  loss_dice_8: 0.8118  time: 1.3870  data_time: 0.0126  lr: 8.5527e-06  max_mem: 23815M
[01/01 13:01:40] d2.utils.events INFO:  eta: 12:31:49  iter: 6239  total_loss: 17.42  loss_ce: 0.408  loss_cate: 0.1048  loss_mask: 0.5114  loss_dice: 0.6271  loss_ce_0: 0.5941  loss_cate_0: 0  loss_mask_0: 0.5353  loss_dice_0: 0.6807  loss_ce_1: 0.3543  loss_cate_1: 0  loss_mask_1: 0.5106  loss_dice_1: 0.7724  loss_ce_2: 0.3868  loss_cate_2: 0  loss_mask_2: 0.5348  loss_dice_2: 0.6743  loss_ce_3: 0.4  loss_cate_3: 0  loss_mask_3: 0.4812  loss_dice_3: 0.6773  loss_ce_4: 0.3888  loss_cate_4: 0  loss_mask_4: 0.494  loss_dice_4: 0.6691  loss_ce_5: 0.3812  loss_cate_5: 0  loss_mask_5: 0.4718  loss_dice_5: 0.6066  loss_ce_6: 0.4307  loss_cate_6: 0  loss_mask_6: 0.4821  loss_dice_6: 0.6375  loss_ce_7: 0.3931  loss_cate_7: 0  loss_mask_7: 0.5131  loss_dice_7: 0.6138  loss_ce_8: 0.4006  loss_cate_8: 0  loss_mask_8: 0.5181  loss_dice_8: 0.6237  time: 1.3869  data_time: 0.0145  lr: 8.548e-06  max_mem: 23815M
[01/01 13:02:07] d2.utils.events INFO:  eta: 12:31:32  iter: 6259  total_loss: 13.81  loss_ce: 0.4075  loss_cate: 0.09721  loss_mask: 0.3584  loss_dice: 0.6234  loss_ce_0: 0.6388  loss_cate_0: 0  loss_mask_0: 0.4129  loss_dice_0: 0.6468  loss_ce_1: 0.447  loss_cate_1: 0  loss_mask_1: 0.3449  loss_dice_1: 0.6564  loss_ce_2: 0.4115  loss_cate_2: 0  loss_mask_2: 0.338  loss_dice_2: 0.6909  loss_ce_3: 0.4377  loss_cate_3: 0  loss_mask_3: 0.3522  loss_dice_3: 0.631  loss_ce_4: 0.4364  loss_cate_4: 0  loss_mask_4: 0.3455  loss_dice_4: 0.619  loss_ce_5: 0.4161  loss_cate_5: 0  loss_mask_5: 0.3604  loss_dice_5: 0.6242  loss_ce_6: 0.3897  loss_cate_6: 0  loss_mask_6: 0.3505  loss_dice_6: 0.6198  loss_ce_7: 0.39  loss_cate_7: 0  loss_mask_7: 0.3912  loss_dice_7: 0.6363  loss_ce_8: 0.4215  loss_cate_8: 0  loss_mask_8: 0.3991  loss_dice_8: 0.6014  time: 1.3869  data_time: 0.0134  lr: 8.5433e-06  max_mem: 23815M
[01/01 13:02:35] d2.utils.events INFO:  eta: 12:31:05  iter: 6279  total_loss: 18.35  loss_ce: 0.3721  loss_cate: 0.1099  loss_mask: 0.5058  loss_dice: 0.7881  loss_ce_0: 0.9021  loss_cate_0: 0  loss_mask_0: 0.5154  loss_dice_0: 0.7686  loss_ce_1: 0.4466  loss_cate_1: 0  loss_mask_1: 0.5706  loss_dice_1: 0.7701  loss_ce_2: 0.4106  loss_cate_2: 0  loss_mask_2: 0.5803  loss_dice_2: 0.7317  loss_ce_3: 0.4076  loss_cate_3: 0  loss_mask_3: 0.572  loss_dice_3: 0.788  loss_ce_4: 0.4513  loss_cate_4: 0  loss_mask_4: 0.5333  loss_dice_4: 0.754  loss_ce_5: 0.3988  loss_cate_5: 0  loss_mask_5: 0.5418  loss_dice_5: 0.7614  loss_ce_6: 0.4332  loss_cate_6: 0  loss_mask_6: 0.5373  loss_dice_6: 0.7336  loss_ce_7: 0.4085  loss_cate_7: 0  loss_mask_7: 0.5479  loss_dice_7: 0.7577  loss_ce_8: 0.3709  loss_cate_8: 0  loss_mask_8: 0.5418  loss_dice_8: 0.7608  time: 1.3869  data_time: 0.0153  lr: 8.5386e-06  max_mem: 23815M
[01/01 13:03:03] d2.utils.events INFO:  eta: 12:30:26  iter: 6299  total_loss: 17.58  loss_ce: 0.4081  loss_cate: 0.1022  loss_mask: 0.4508  loss_dice: 0.7052  loss_ce_0: 0.8594  loss_cate_0: 0  loss_mask_0: 0.5274  loss_dice_0: 0.7273  loss_ce_1: 0.3622  loss_cate_1: 0  loss_mask_1: 0.4344  loss_dice_1: 0.7391  loss_ce_2: 0.385  loss_cate_2: 0  loss_mask_2: 0.4491  loss_dice_2: 0.6692  loss_ce_3: 0.4482  loss_cate_3: 0  loss_mask_3: 0.4532  loss_dice_3: 0.6955  loss_ce_4: 0.3516  loss_cate_4: 0  loss_mask_4: 0.4594  loss_dice_4: 0.6926  loss_ce_5: 0.4304  loss_cate_5: 0  loss_mask_5: 0.453  loss_dice_5: 0.6658  loss_ce_6: 0.4513  loss_cate_6: 0  loss_mask_6: 0.457  loss_dice_6: 0.6294  loss_ce_7: 0.4502  loss_cate_7: 0  loss_mask_7: 0.4583  loss_dice_7: 0.6835  loss_ce_8: 0.432  loss_cate_8: 0  loss_mask_8: 0.4358  loss_dice_8: 0.6926  time: 1.3869  data_time: 0.0155  lr: 8.5339e-06  max_mem: 23815M
[01/01 13:03:30] d2.utils.events INFO:  eta: 12:29:57  iter: 6319  total_loss: 13.95  loss_ce: 0.2995  loss_cate: 0.08369  loss_mask: 0.378  loss_dice: 0.4574  loss_ce_0: 0.683  loss_cate_0: 0  loss_mask_0: 0.3718  loss_dice_0: 0.4755  loss_ce_1: 0.3612  loss_cate_1: 0  loss_mask_1: 0.3687  loss_dice_1: 0.4665  loss_ce_2: 0.2618  loss_cate_2: 0  loss_mask_2: 0.3737  loss_dice_2: 0.4112  loss_ce_3: 0.3039  loss_cate_3: 0  loss_mask_3: 0.3941  loss_dice_3: 0.4438  loss_ce_4: 0.3516  loss_cate_4: 0  loss_mask_4: 0.3777  loss_dice_4: 0.4144  loss_ce_5: 0.3108  loss_cate_5: 0  loss_mask_5: 0.3891  loss_dice_5: 0.4271  loss_ce_6: 0.3437  loss_cate_6: 0  loss_mask_6: 0.3659  loss_dice_6: 0.4348  loss_ce_7: 0.3064  loss_cate_7: 0  loss_mask_7: 0.3695  loss_dice_7: 0.4711  loss_ce_8: 0.2998  loss_cate_8: 0  loss_mask_8: 0.3804  loss_dice_8: 0.4749  time: 1.3869  data_time: 0.0112  lr: 8.5292e-06  max_mem: 23815M
[01/01 13:03:58] d2.utils.events INFO:  eta: 12:29:25  iter: 6339  total_loss: 18.46  loss_ce: 0.508  loss_cate: 0.1142  loss_mask: 0.5929  loss_dice: 0.8399  loss_ce_0: 0.9321  loss_cate_0: 0  loss_mask_0: 0.5603  loss_dice_0: 0.8194  loss_ce_1: 0.6427  loss_cate_1: 0  loss_mask_1: 0.5576  loss_dice_1: 0.7817  loss_ce_2: 0.6085  loss_cate_2: 0  loss_mask_2: 0.6143  loss_dice_2: 0.8176  loss_ce_3: 0.5449  loss_cate_3: 0  loss_mask_3: 0.6029  loss_dice_3: 0.7582  loss_ce_4: 0.5643  loss_cate_4: 0  loss_mask_4: 0.5845  loss_dice_4: 0.8059  loss_ce_5: 0.5667  loss_cate_5: 0  loss_mask_5: 0.615  loss_dice_5: 0.784  loss_ce_6: 0.5878  loss_cate_6: 0  loss_mask_6: 0.6251  loss_dice_6: 0.868  loss_ce_7: 0.512  loss_cate_7: 0  loss_mask_7: 0.6096  loss_dice_7: 0.8854  loss_ce_8: 0.5023  loss_cate_8: 0  loss_mask_8: 0.6131  loss_dice_8: 0.8217  time: 1.3869  data_time: 0.0145  lr: 8.5245e-06  max_mem: 23815M
[01/01 13:04:26] d2.utils.events INFO:  eta: 12:28:57  iter: 6359  total_loss: 16.76  loss_ce: 0.4801  loss_cate: 0.1009  loss_mask: 0.4779  loss_dice: 0.6951  loss_ce_0: 0.849  loss_cate_0: 0  loss_mask_0: 0.5891  loss_dice_0: 0.575  loss_ce_1: 0.499  loss_cate_1: 0  loss_mask_1: 0.5343  loss_dice_1: 0.5868  loss_ce_2: 0.496  loss_cate_2: 0  loss_mask_2: 0.516  loss_dice_2: 0.552  loss_ce_3: 0.4915  loss_cate_3: 0  loss_mask_3: 0.5657  loss_dice_3: 0.6573  loss_ce_4: 0.4673  loss_cate_4: 0  loss_mask_4: 0.5222  loss_dice_4: 0.6336  loss_ce_5: 0.4578  loss_cate_5: 0  loss_mask_5: 0.4765  loss_dice_5: 0.6434  loss_ce_6: 0.4547  loss_cate_6: 0  loss_mask_6: 0.4761  loss_dice_6: 0.6271  loss_ce_7: 0.4438  loss_cate_7: 0  loss_mask_7: 0.4606  loss_dice_7: 0.7207  loss_ce_8: 0.4508  loss_cate_8: 0  loss_mask_8: 0.4668  loss_dice_8: 0.6261  time: 1.3868  data_time: 0.0121  lr: 8.5198e-06  max_mem: 23815M
[01/01 13:04:53] d2.utils.events INFO:  eta: 12:28:34  iter: 6379  total_loss: 17.88  loss_ce: 0.4859  loss_cate: 0.1072  loss_mask: 0.5285  loss_dice: 0.7546  loss_ce_0: 0.6889  loss_cate_0: 0  loss_mask_0: 0.5222  loss_dice_0: 0.7899  loss_ce_1: 0.4659  loss_cate_1: 0  loss_mask_1: 0.5219  loss_dice_1: 0.7318  loss_ce_2: 0.4673  loss_cate_2: 0  loss_mask_2: 0.5156  loss_dice_2: 0.7153  loss_ce_3: 0.4567  loss_cate_3: 0  loss_mask_3: 0.5341  loss_dice_3: 0.7416  loss_ce_4: 0.4651  loss_cate_4: 0  loss_mask_4: 0.5432  loss_dice_4: 0.7961  loss_ce_5: 0.4395  loss_cate_5: 0  loss_mask_5: 0.5468  loss_dice_5: 0.7499  loss_ce_6: 0.4429  loss_cate_6: 0  loss_mask_6: 0.5151  loss_dice_6: 0.7423  loss_ce_7: 0.4367  loss_cate_7: 0  loss_mask_7: 0.5344  loss_dice_7: 0.733  loss_ce_8: 0.4336  loss_cate_8: 0  loss_mask_8: 0.5035  loss_dice_8: 0.7547  time: 1.3868  data_time: 0.0136  lr: 8.5151e-06  max_mem: 23815M
[01/01 13:05:21] d2.utils.events INFO:  eta: 12:28:10  iter: 6399  total_loss: 17.76  loss_ce: 0.5147  loss_cate: 0.1096  loss_mask: 0.4704  loss_dice: 0.768  loss_ce_0: 0.7713  loss_cate_0: 0  loss_mask_0: 0.4339  loss_dice_0: 0.8137  loss_ce_1: 0.5562  loss_cate_1: 0  loss_mask_1: 0.3992  loss_dice_1: 0.8059  loss_ce_2: 0.5023  loss_cate_2: 0  loss_mask_2: 0.4576  loss_dice_2: 0.7701  loss_ce_3: 0.4564  loss_cate_3: 0  loss_mask_3: 0.442  loss_dice_3: 0.8291  loss_ce_4: 0.4849  loss_cate_4: 0  loss_mask_4: 0.4638  loss_dice_4: 0.7931  loss_ce_5: 0.4764  loss_cate_5: 0  loss_mask_5: 0.4579  loss_dice_5: 0.7317  loss_ce_6: 0.4332  loss_cate_6: 0  loss_mask_6: 0.457  loss_dice_6: 0.8116  loss_ce_7: 0.5002  loss_cate_7: 0  loss_mask_7: 0.4516  loss_dice_7: 0.7999  loss_ce_8: 0.5572  loss_cate_8: 0  loss_mask_8: 0.4613  loss_dice_8: 0.7909  time: 1.3868  data_time: 0.0123  lr: 8.5104e-06  max_mem: 23815M
[01/01 13:05:49] d2.utils.events INFO:  eta: 12:27:43  iter: 6419  total_loss: 21.31  loss_ce: 0.44  loss_cate: 0.09472  loss_mask: 0.6145  loss_dice: 0.904  loss_ce_0: 0.8189  loss_cate_0: 0  loss_mask_0: 0.5661  loss_dice_0: 0.8824  loss_ce_1: 0.5568  loss_cate_1: 0  loss_mask_1: 0.5928  loss_dice_1: 0.8508  loss_ce_2: 0.5136  loss_cate_2: 0  loss_mask_2: 0.5766  loss_dice_2: 0.8533  loss_ce_3: 0.5022  loss_cate_3: 0  loss_mask_3: 0.6909  loss_dice_3: 0.8488  loss_ce_4: 0.5037  loss_cate_4: 0  loss_mask_4: 0.5858  loss_dice_4: 0.9137  loss_ce_5: 0.487  loss_cate_5: 0  loss_mask_5: 0.5984  loss_dice_5: 0.8851  loss_ce_6: 0.4609  loss_cate_6: 0  loss_mask_6: 0.6175  loss_dice_6: 0.8904  loss_ce_7: 0.4736  loss_cate_7: 0  loss_mask_7: 0.5987  loss_dice_7: 0.8828  loss_ce_8: 0.4569  loss_cate_8: 0  loss_mask_8: 0.6247  loss_dice_8: 0.9299  time: 1.3868  data_time: 0.0118  lr: 8.5057e-06  max_mem: 23815M
[01/01 13:06:17] d2.utils.events INFO:  eta: 12:27:13  iter: 6439  total_loss: 20.51  loss_ce: 0.3649  loss_cate: 0.1264  loss_mask: 0.6479  loss_dice: 1.027  loss_ce_0: 0.8266  loss_cate_0: 0  loss_mask_0: 0.6952  loss_dice_0: 0.9225  loss_ce_1: 0.4624  loss_cate_1: 0  loss_mask_1: 0.7561  loss_dice_1: 1.055  loss_ce_2: 0.4679  loss_cate_2: 0  loss_mask_2: 0.6352  loss_dice_2: 0.958  loss_ce_3: 0.4007  loss_cate_3: 0  loss_mask_3: 0.6604  loss_dice_3: 0.9932  loss_ce_4: 0.4605  loss_cate_4: 0  loss_mask_4: 0.6826  loss_dice_4: 1.044  loss_ce_5: 0.4265  loss_cate_5: 0  loss_mask_5: 0.6192  loss_dice_5: 0.9426  loss_ce_6: 0.397  loss_cate_6: 0  loss_mask_6: 0.6222  loss_dice_6: 0.8943  loss_ce_7: 0.4238  loss_cate_7: 0  loss_mask_7: 0.6133  loss_dice_7: 0.9098  loss_ce_8: 0.4287  loss_cate_8: 0  loss_mask_8: 0.6879  loss_dice_8: 0.976  time: 1.3868  data_time: 0.0141  lr: 8.501e-06  max_mem: 23815M
[01/01 13:06:44] d2.utils.events INFO:  eta: 12:26:43  iter: 6459  total_loss: 17.83  loss_ce: 0.3822  loss_cate: 0.09639  loss_mask: 0.4413  loss_dice: 0.6237  loss_ce_0: 0.6832  loss_cate_0: 0  loss_mask_0: 0.439  loss_dice_0: 0.7247  loss_ce_1: 0.4853  loss_cate_1: 0  loss_mask_1: 0.4149  loss_dice_1: 0.6992  loss_ce_2: 0.4775  loss_cate_2: 0  loss_mask_2: 0.4206  loss_dice_2: 0.7116  loss_ce_3: 0.4763  loss_cate_3: 0  loss_mask_3: 0.4474  loss_dice_3: 0.7223  loss_ce_4: 0.4093  loss_cate_4: 0  loss_mask_4: 0.4183  loss_dice_4: 0.6251  loss_ce_5: 0.4225  loss_cate_5: 0  loss_mask_5: 0.4275  loss_dice_5: 0.6566  loss_ce_6: 0.4058  loss_cate_6: 0  loss_mask_6: 0.4387  loss_dice_6: 0.6394  loss_ce_7: 0.3438  loss_cate_7: 0  loss_mask_7: 0.4358  loss_dice_7: 0.6988  loss_ce_8: 0.4651  loss_cate_8: 0  loss_mask_8: 0.4322  loss_dice_8: 0.5946  time: 1.3868  data_time: 0.0127  lr: 8.4963e-06  max_mem: 23815M
[01/01 13:07:12] d2.utils.events INFO:  eta: 12:26:05  iter: 6479  total_loss: 20.27  loss_ce: 0.4561  loss_cate: 0.1099  loss_mask: 0.5666  loss_dice: 0.7783  loss_ce_0: 0.6896  loss_cate_0: 0  loss_mask_0: 0.6145  loss_dice_0: 0.8352  loss_ce_1: 0.4656  loss_cate_1: 0  loss_mask_1: 0.5514  loss_dice_1: 0.8099  loss_ce_2: 0.4111  loss_cate_2: 0  loss_mask_2: 0.5172  loss_dice_2: 0.771  loss_ce_3: 0.3406  loss_cate_3: 0  loss_mask_3: 0.5664  loss_dice_3: 0.8219  loss_ce_4: 0.3166  loss_cate_4: 0  loss_mask_4: 0.6516  loss_dice_4: 0.7575  loss_ce_5: 0.3348  loss_cate_5: 0  loss_mask_5: 0.6149  loss_dice_5: 0.8887  loss_ce_6: 0.3639  loss_cate_6: 0  loss_mask_6: 0.577  loss_dice_6: 0.8313  loss_ce_7: 0.3768  loss_cate_7: 0  loss_mask_7: 0.5803  loss_dice_7: 0.803  loss_ce_8: 0.3939  loss_cate_8: 0  loss_mask_8: 0.5846  loss_dice_8: 0.7499  time: 1.3867  data_time: 0.0103  lr: 8.4916e-06  max_mem: 23815M
[01/01 13:07:39] d2.data.datasets.coco INFO: Loaded 653 images with semantic segmentation from /opt/ml/input/data/images/val_0
[01/01 13:07:39] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=1280, sample_style='choice')]
[01/01 13:07:39] d2.data.common INFO: Serializing 653 elements to byte tensors and concatenating them all ...
[01/01 13:07:39] d2.data.common INFO: Serialized dataset takes 0.09 MiB
[01/01 13:07:39] d2.data.datasets.coco INFO: Loaded 653 images with semantic segmentation from /opt/ml/input/data/images/val_0
[01/01 13:07:39] d2.evaluation.evaluator INFO: Start inference on 653 batches
[01/01 13:07:41] d2.evaluation.evaluator INFO: Inference done 11/653. Dataloading: 0.0012 s/iter. Inference: 0.1069 s/iter. Eval: 0.0140 s/iter. Total: 0.1221 s/iter. ETA=0:01:18
[01/01 13:07:46] d2.evaluation.evaluator INFO: Inference done 54/653. Dataloading: 0.0016 s/iter. Inference: 0.1050 s/iter. Eval: 0.0125 s/iter. Total: 0.1190 s/iter. ETA=0:01:11
[01/01 13:07:51] d2.evaluation.evaluator INFO: Inference done 97/653. Dataloading: 0.0016 s/iter. Inference: 0.1047 s/iter. Eval: 0.0123 s/iter. Total: 0.1186 s/iter. ETA=0:01:05
[01/01 13:07:56] d2.evaluation.evaluator INFO: Inference done 140/653. Dataloading: 0.0016 s/iter. Inference: 0.1048 s/iter. Eval: 0.0121 s/iter. Total: 0.1186 s/iter. ETA=0:01:00
[01/01 13:08:01] d2.evaluation.evaluator INFO: Inference done 182/653. Dataloading: 0.0017 s/iter. Inference: 0.1049 s/iter. Eval: 0.0121 s/iter. Total: 0.1188 s/iter. ETA=0:00:55
[01/01 13:08:06] d2.evaluation.evaluator INFO: Inference done 225/653. Dataloading: 0.0016 s/iter. Inference: 0.1048 s/iter. Eval: 0.0121 s/iter. Total: 0.1186 s/iter. ETA=0:00:50
[01/01 13:08:11] d2.evaluation.evaluator INFO: Inference done 268/653. Dataloading: 0.0016 s/iter. Inference: 0.1047 s/iter. Eval: 0.0123 s/iter. Total: 0.1187 s/iter. ETA=0:00:45
[01/01 13:08:16] d2.evaluation.evaluator INFO: Inference done 310/653. Dataloading: 0.0016 s/iter. Inference: 0.1049 s/iter. Eval: 0.0124 s/iter. Total: 0.1190 s/iter. ETA=0:00:40
[01/01 13:08:22] d2.evaluation.evaluator INFO: Inference done 352/653. Dataloading: 0.0017 s/iter. Inference: 0.1050 s/iter. Eval: 0.0124 s/iter. Total: 0.1192 s/iter. ETA=0:00:35
[01/01 13:08:27] d2.evaluation.evaluator INFO: Inference done 394/653. Dataloading: 0.0017 s/iter. Inference: 0.1050 s/iter. Eval: 0.0125 s/iter. Total: 0.1192 s/iter. ETA=0:00:30
[01/01 13:08:32] d2.evaluation.evaluator INFO: Inference done 436/653. Dataloading: 0.0017 s/iter. Inference: 0.1051 s/iter. Eval: 0.0125 s/iter. Total: 0.1193 s/iter. ETA=0:00:25
[01/01 13:08:37] d2.evaluation.evaluator INFO: Inference done 479/653. Dataloading: 0.0017 s/iter. Inference: 0.1050 s/iter. Eval: 0.0125 s/iter. Total: 0.1193 s/iter. ETA=0:00:20
[01/01 13:08:42] d2.evaluation.evaluator INFO: Inference done 521/653. Dataloading: 0.0017 s/iter. Inference: 0.1051 s/iter. Eval: 0.0125 s/iter. Total: 0.1193 s/iter. ETA=0:00:15
[01/01 13:08:47] d2.evaluation.evaluator INFO: Inference done 564/653. Dataloading: 0.0017 s/iter. Inference: 0.1051 s/iter. Eval: 0.0125 s/iter. Total: 0.1193 s/iter. ETA=0:00:10
[01/01 13:08:52] d2.evaluation.evaluator INFO: Inference done 607/653. Dataloading: 0.0017 s/iter. Inference: 0.1051 s/iter. Eval: 0.0124 s/iter. Total: 0.1192 s/iter. ETA=0:00:05
[01/01 13:08:57] d2.evaluation.evaluator INFO: Inference done 649/653. Dataloading: 0.0017 s/iter. Inference: 0.1052 s/iter. Eval: 0.0124 s/iter. Total: 0.1193 s/iter. ETA=0:00:00
[01/01 13:08:58] d2.evaluation.evaluator INFO: Total inference time: 0:01:17.369801 (0.119398 s / iter per device, on 1 devices)
[01/01 13:08:58] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:01:08 (0.105193 s / iter per device, on 1 devices)
[01/01 13:08:58] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 70.24249619216152, 'fwIoU': 90.59658902882394, 'IoU-Backgroud': 97.10155577900169, 'IoU-General trash': 42.48103982138365, 'IoU-Paper': 80.40559686252914, 'IoU-Paper pack': 50.86615545988439, 'IoU-Metal': 54.275278236284294, 'IoU-Glass': 67.8640316564952, 'IoU-Plastic': 53.46111731862416, 'IoU-Styrofoam': 79.27591021143272, 'IoU-Plastic bag': 85.4136773398298, 'IoU-Battery': 80.04119315615458, 'IoU-Clothing': 81.48190227215716, 'mACC': 81.27637098515935, 'pACC': 94.46549992466412, 'ACC-Backgroud': 98.18307850418499, 'ACC-General trash': 58.67207085568177, 'ACC-Paper': 90.80143377635615, 'ACC-Paper pack': 63.20354264312543, 'ACC-Metal': 66.89249653989692, 'ACC-Glass': 92.51696784564616, 'ACC-Plastic': 74.43645146377268, 'ACC-Styrofoam': 82.67907171297949, 'ACC-Plastic bag': 91.92714348600161, 'ACC-Battery': 83.85765663448798, 'ACC-Clothing': 90.87016737461957})])
[01/01 13:08:58] d2.engine.defaults INFO: Evaluation results for trash_recycle_sem_seg_val_0 in csv format:
[01/01 13:08:58] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/01 13:08:58] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/01 13:08:58] d2.evaluation.testing INFO: copypaste: 70.2425,90.5966,81.2764,94.4655
[01/01 13:08:58] fvcore.common.checkpoint INFO: Saving checkpoint to ./trash_dataV1_WarmupPolyLR_1e-5/model_best_6499iter.pth
[01/01 13:09:00] d2.engine.hooks INFO: Saved best model as latest eval score for mIoU is70.24250, better than last best score 66.42078 @ iteration 3899.
[01/01 13:09:00] d2.utils.events INFO:  eta: 12:25:38  iter: 6499  total_loss: 17.08  loss_ce: 0.4882  loss_cate: 0.0802  loss_mask: 0.4198  loss_dice: 0.6875  loss_ce_0: 0.7498  loss_cate_0: 0  loss_mask_0: 0.4513  loss_dice_0: 0.7264  loss_ce_1: 0.5817  loss_cate_1: 0  loss_mask_1: 0.4498  loss_dice_1: 0.6899  loss_ce_2: 0.5679  loss_cate_2: 0  loss_mask_2: 0.4614  loss_dice_2: 0.6401  loss_ce_3: 0.5436  loss_cate_3: 0  loss_mask_3: 0.4492  loss_dice_3: 0.7282  loss_ce_4: 0.5048  loss_cate_4: 0  loss_mask_4: 0.4502  loss_dice_4: 0.6883  loss_ce_5: 0.5038  loss_cate_5: 0  loss_mask_5: 0.438  loss_dice_5: 0.6465  loss_ce_6: 0.4848  loss_cate_6: 0  loss_mask_6: 0.4427  loss_dice_6: 0.6953  loss_ce_7: 0.4315  loss_cate_7: 0  loss_mask_7: 0.4528  loss_dice_7: 0.6843  loss_ce_8: 0.4546  loss_cate_8: 0  loss_mask_8: 0.4453  loss_dice_8: 0.7487  time: 1.3867  data_time: 0.0153  lr: 8.4869e-06  max_mem: 23815M
[01/01 13:09:27] d2.utils.events INFO:  eta: 12:25:08  iter: 6519  total_loss: 17.77  loss_ce: 0.362  loss_cate: 0.1125  loss_mask: 0.5903  loss_dice: 0.6888  loss_ce_0: 0.7595  loss_cate_0: 0  loss_mask_0: 0.4883  loss_dice_0: 0.6643  loss_ce_1: 0.4467  loss_cate_1: 0  loss_mask_1: 0.5405  loss_dice_1: 0.7741  loss_ce_2: 0.4582  loss_cate_2: 0  loss_mask_2: 0.5763  loss_dice_2: 0.6883  loss_ce_3: 0.3792  loss_cate_3: 0  loss_mask_3: 0.5741  loss_dice_3: 0.6969  loss_ce_4: 0.3863  loss_cate_4: 0  loss_mask_4: 0.5628  loss_dice_4: 0.709  loss_ce_5: 0.371  loss_cate_5: 0  loss_mask_5: 0.5886  loss_dice_5: 0.7109  loss_ce_6: 0.3774  loss_cate_6: 0  loss_mask_6: 0.5796  loss_dice_6: 0.7123  loss_ce_7: 0.3635  loss_cate_7: 0  loss_mask_7: 0.564  loss_dice_7: 0.719  loss_ce_8: 0.3824  loss_cate_8: 0  loss_mask_8: 0.595  loss_dice_8: 0.68  time: 1.3867  data_time: 0.0113  lr: 8.4822e-06  max_mem: 23815M
[01/01 13:09:55] d2.utils.events INFO:  eta: 12:24:40  iter: 6539  total_loss: 17.43  loss_ce: 0.4926  loss_cate: 0.101  loss_mask: 0.4402  loss_dice: 0.6859  loss_ce_0: 0.6479  loss_cate_0: 0  loss_mask_0: 0.5325  loss_dice_0: 0.7065  loss_ce_1: 0.426  loss_cate_1: 0  loss_mask_1: 0.4747  loss_dice_1: 0.7399  loss_ce_2: 0.4446  loss_cate_2: 0  loss_mask_2: 0.4733  loss_dice_2: 0.7472  loss_ce_3: 0.471  loss_cate_3: 0  loss_mask_3: 0.4686  loss_dice_3: 0.6956  loss_ce_4: 0.433  loss_cate_4: 0  loss_mask_4: 0.474  loss_dice_4: 0.72  loss_ce_5: 0.3687  loss_cate_5: 0  loss_mask_5: 0.464  loss_dice_5: 0.6823  loss_ce_6: 0.4835  loss_cate_6: 0  loss_mask_6: 0.4761  loss_dice_6: 0.6656  loss_ce_7: 0.5122  loss_cate_7: 0  loss_mask_7: 0.4675  loss_dice_7: 0.6959  loss_ce_8: 0.5108  loss_cate_8: 0  loss_mask_8: 0.4492  loss_dice_8: 0.6973  time: 1.3867  data_time: 0.0131  lr: 8.4775e-06  max_mem: 23815M
[01/01 13:10:23] d2.utils.events INFO:  eta: 12:24:11  iter: 6559  total_loss: 16.46  loss_ce: 0.3196  loss_cate: 0.1331  loss_mask: 0.5381  loss_dice: 0.6797  loss_ce_0: 0.7043  loss_cate_0: 0  loss_mask_0: 0.5812  loss_dice_0: 0.7365  loss_ce_1: 0.4031  loss_cate_1: 0  loss_mask_1: 0.5921  loss_dice_1: 0.7666  loss_ce_2: 0.3523  loss_cate_2: 0  loss_mask_2: 0.5843  loss_dice_2: 0.6877  loss_ce_3: 0.3203  loss_cate_3: 0  loss_mask_3: 0.5714  loss_dice_3: 0.6922  loss_ce_4: 0.3485  loss_cate_4: 0  loss_mask_4: 0.5379  loss_dice_4: 0.7078  loss_ce_5: 0.3745  loss_cate_5: 0  loss_mask_5: 0.546  loss_dice_5: 0.7008  loss_ce_6: 0.3518  loss_cate_6: 0  loss_mask_6: 0.5648  loss_dice_6: 0.7263  loss_ce_7: 0.353  loss_cate_7: 0  loss_mask_7: 0.5421  loss_dice_7: 0.7047  loss_ce_8: 0.3396  loss_cate_8: 0  loss_mask_8: 0.5247  loss_dice_8: 0.6942  time: 1.3868  data_time: 0.0117  lr: 8.4728e-06  max_mem: 23815M
[01/01 13:10:51] d2.utils.events INFO:  eta: 12:23:43  iter: 6579  total_loss: 17.81  loss_ce: 0.4802  loss_cate: 0.113  loss_mask: 0.5239  loss_dice: 0.7546  loss_ce_0: 0.8519  loss_cate_0: 0  loss_mask_0: 0.5356  loss_dice_0: 0.8206  loss_ce_1: 0.589  loss_cate_1: 0  loss_mask_1: 0.5469  loss_dice_1: 0.7986  loss_ce_2: 0.4734  loss_cate_2: 0  loss_mask_2: 0.5445  loss_dice_2: 0.7645  loss_ce_3: 0.5392  loss_cate_3: 0  loss_mask_3: 0.4998  loss_dice_3: 0.7435  loss_ce_4: 0.4898  loss_cate_4: 0  loss_mask_4: 0.4943  loss_dice_4: 0.7805  loss_ce_5: 0.4757  loss_cate_5: 0  loss_mask_5: 0.5389  loss_dice_5: 0.7905  loss_ce_6: 0.3744  loss_cate_6: 0  loss_mask_6: 0.5396  loss_dice_6: 0.8142  loss_ce_7: 0.5073  loss_cate_7: 0  loss_mask_7: 0.5576  loss_dice_7: 0.8183  loss_ce_8: 0.4931  loss_cate_8: 0  loss_mask_8: 0.5044  loss_dice_8: 0.7491  time: 1.3868  data_time: 0.0115  lr: 8.4681e-06  max_mem: 23815M
[01/01 13:11:19] d2.utils.events INFO:  eta: 12:23:08  iter: 6599  total_loss: 13.88  loss_ce: 0.2803  loss_cate: 0.09879  loss_mask: 0.374  loss_dice: 0.5629  loss_ce_0: 0.6211  loss_cate_0: 0  loss_mask_0: 0.4313  loss_dice_0: 0.5077  loss_ce_1: 0.3182  loss_cate_1: 0  loss_mask_1: 0.4462  loss_dice_1: 0.5749  loss_ce_2: 0.3318  loss_cate_2: 0  loss_mask_2: 0.4123  loss_dice_2: 0.5866  loss_ce_3: 0.2992  loss_cate_3: 0  loss_mask_3: 0.4734  loss_dice_3: 0.6096  loss_ce_4: 0.2988  loss_cate_4: 0  loss_mask_4: 0.4146  loss_dice_4: 0.5791  loss_ce_5: 0.3496  loss_cate_5: 0  loss_mask_5: 0.3863  loss_dice_5: 0.5768  loss_ce_6: 0.3008  loss_cate_6: 0  loss_mask_6: 0.4081  loss_dice_6: 0.5958  loss_ce_7: 0.3273  loss_cate_7: 0  loss_mask_7: 0.4008  loss_dice_7: 0.5963  loss_ce_8: 0.2938  loss_cate_8: 0  loss_mask_8: 0.3942  loss_dice_8: 0.5882  time: 1.3868  data_time: 0.0118  lr: 8.4634e-06  max_mem: 23815M
[01/01 13:11:46] d2.utils.events INFO:  eta: 12:22:29  iter: 6619  total_loss: 16.53  loss_ce: 0.4774  loss_cate: 0.0964  loss_mask: 0.3534  loss_dice: 0.6576  loss_ce_0: 0.733  loss_cate_0: 0  loss_mask_0: 0.4033  loss_dice_0: 0.7703  loss_ce_1: 0.5022  loss_cate_1: 0  loss_mask_1: 0.3788  loss_dice_1: 0.6491  loss_ce_2: 0.5494  loss_cate_2: 0  loss_mask_2: 0.3852  loss_dice_2: 0.7073  loss_ce_3: 0.5579  loss_cate_3: 0  loss_mask_3: 0.3603  loss_dice_3: 0.6549  loss_ce_4: 0.5304  loss_cate_4: 0  loss_mask_4: 0.3912  loss_dice_4: 0.6576  loss_ce_5: 0.4905  loss_cate_5: 0  loss_mask_5: 0.3594  loss_dice_5: 0.6394  loss_ce_6: 0.527  loss_cate_6: 0  loss_mask_6: 0.379  loss_dice_6: 0.6789  loss_ce_7: 0.4665  loss_cate_7: 0  loss_mask_7: 0.3934  loss_dice_7: 0.6837  loss_ce_8: 0.4805  loss_cate_8: 0  loss_mask_8: 0.3645  loss_dice_8: 0.6694  time: 1.3867  data_time: 0.0103  lr: 8.4587e-06  max_mem: 23815M
[01/01 13:12:14] d2.utils.events INFO:  eta: 12:22:01  iter: 6639  total_loss: 16.56  loss_ce: 0.3218  loss_cate: 0.1184  loss_mask: 0.5527  loss_dice: 0.8292  loss_ce_0: 0.7701  loss_cate_0: 0  loss_mask_0: 0.6232  loss_dice_0: 0.9235  loss_ce_1: 0.3948  loss_cate_1: 0  loss_mask_1: 0.6041  loss_dice_1: 0.8116  loss_ce_2: 0.3599  loss_cate_2: 0  loss_mask_2: 0.5633  loss_dice_2: 0.8091  loss_ce_3: 0.3999  loss_cate_3: 0  loss_mask_3: 0.5398  loss_dice_3: 0.8015  loss_ce_4: 0.3788  loss_cate_4: 0  loss_mask_4: 0.5433  loss_dice_4: 0.8015  loss_ce_5: 0.352  loss_cate_5: 0  loss_mask_5: 0.5591  loss_dice_5: 0.8091  loss_ce_6: 0.3475  loss_cate_6: 0  loss_mask_6: 0.5694  loss_dice_6: 0.8649  loss_ce_7: 0.3767  loss_cate_7: 0  loss_mask_7: 0.5461  loss_dice_7: 0.785  loss_ce_8: 0.3563  loss_cate_8: 0  loss_mask_8: 0.5508  loss_dice_8: 0.7942  time: 1.3867  data_time: 0.0133  lr: 8.454e-06  max_mem: 23815M
[01/01 13:12:42] d2.utils.events INFO:  eta: 12:21:32  iter: 6659  total_loss: 17.96  loss_ce: 0.406  loss_cate: 0.118  loss_mask: 0.5619  loss_dice: 0.7881  loss_ce_0: 0.7203  loss_cate_0: 0  loss_mask_0: 0.5274  loss_dice_0: 0.7708  loss_ce_1: 0.4872  loss_cate_1: 0  loss_mask_1: 0.5068  loss_dice_1: 0.7418  loss_ce_2: 0.4192  loss_cate_2: 0  loss_mask_2: 0.5057  loss_dice_2: 0.8189  loss_ce_3: 0.3973  loss_cate_3: 0  loss_mask_3: 0.5208  loss_dice_3: 0.8519  loss_ce_4: 0.3574  loss_cate_4: 0  loss_mask_4: 0.6191  loss_dice_4: 0.8856  loss_ce_5: 0.3619  loss_cate_5: 0  loss_mask_5: 0.5833  loss_dice_5: 0.8499  loss_ce_6: 0.3922  loss_cate_6: 0  loss_mask_6: 0.5341  loss_dice_6: 0.8238  loss_ce_7: 0.3926  loss_cate_7: 0  loss_mask_7: 0.5205  loss_dice_7: 0.7843  loss_ce_8: 0.3864  loss_cate_8: 0  loss_mask_8: 0.5315  loss_dice_8: 0.7993  time: 1.3867  data_time: 0.0125  lr: 8.4493e-06  max_mem: 23815M
[01/01 13:13:09] d2.utils.events INFO:  eta: 12:21:04  iter: 6679  total_loss: 16.29  loss_ce: 0.397  loss_cate: 0.09094  loss_mask: 0.5201  loss_dice: 0.5462  loss_ce_0: 0.6347  loss_cate_0: 0  loss_mask_0: 0.5382  loss_dice_0: 0.6907  loss_ce_1: 0.3988  loss_cate_1: 0  loss_mask_1: 0.5702  loss_dice_1: 0.5534  loss_ce_2: 0.3362  loss_cate_2: 0  loss_mask_2: 0.6069  loss_dice_2: 0.5951  loss_ce_3: 0.3699  loss_cate_3: 0  loss_mask_3: 0.6076  loss_dice_3: 0.5838  loss_ce_4: 0.4614  loss_cate_4: 0  loss_mask_4: 0.5328  loss_dice_4: 0.5519  loss_ce_5: 0.3676  loss_cate_5: 0  loss_mask_5: 0.5249  loss_dice_5: 0.5384  loss_ce_6: 0.3479  loss_cate_6: 0  loss_mask_6: 0.5831  loss_dice_6: 0.5389  loss_ce_7: 0.3115  loss_cate_7: 0  loss_mask_7: 0.6143  loss_dice_7: 0.5673  loss_ce_8: 0.3849  loss_cate_8: 0  loss_mask_8: 0.536  loss_dice_8: 0.5251  time: 1.3866  data_time: 0.0126  lr: 8.4446e-06  max_mem: 23815M
[01/01 13:13:37] d2.utils.events INFO:  eta: 12:20:37  iter: 6699  total_loss: 17.93  loss_ce: 0.398  loss_cate: 0.1038  loss_mask: 0.5416  loss_dice: 0.8919  loss_ce_0: 0.7791  loss_cate_0: 0  loss_mask_0: 0.5277  loss_dice_0: 0.8279  loss_ce_1: 0.4596  loss_cate_1: 0  loss_mask_1: 0.5005  loss_dice_1: 0.8239  loss_ce_2: 0.4617  loss_cate_2: 0  loss_mask_2: 0.4457  loss_dice_2: 0.7985  loss_ce_3: 0.4055  loss_cate_3: 0  loss_mask_3: 0.4673  loss_dice_3: 0.8558  loss_ce_4: 0.4108  loss_cate_4: 0  loss_mask_4: 0.4702  loss_dice_4: 0.8846  loss_ce_5: 0.4063  loss_cate_5: 0  loss_mask_5: 0.4687  loss_dice_5: 0.8655  loss_ce_6: 0.4212  loss_cate_6: 0  loss_mask_6: 0.4974  loss_dice_6: 0.8437  loss_ce_7: 0.3564  loss_cate_7: 0  loss_mask_7: 0.4723  loss_dice_7: 0.8324  loss_ce_8: 0.3566  loss_cate_8: 0  loss_mask_8: 0.4873  loss_dice_8: 0.8675  time: 1.3866  data_time: 0.0121  lr: 8.4399e-06  max_mem: 23815M
[01/01 13:14:04] d2.utils.events INFO:  eta: 12:20:06  iter: 6719  total_loss: 15.79  loss_ce: 0.4088  loss_cate: 0.1011  loss_mask: 0.4296  loss_dice: 0.7003  loss_ce_0: 0.8222  loss_cate_0: 0  loss_mask_0: 0.3856  loss_dice_0: 0.6751  loss_ce_1: 0.4668  loss_cate_1: 0  loss_mask_1: 0.4423  loss_dice_1: 0.774  loss_ce_2: 0.4826  loss_cate_2: 0  loss_mask_2: 0.454  loss_dice_2: 0.69  loss_ce_3: 0.4368  loss_cate_3: 0  loss_mask_3: 0.4567  loss_dice_3: 0.6316  loss_ce_4: 0.4235  loss_cate_4: 0  loss_mask_4: 0.4678  loss_dice_4: 0.6823  loss_ce_5: 0.3564  loss_cate_5: 0  loss_mask_5: 0.4011  loss_dice_5: 0.7141  loss_ce_6: 0.4204  loss_cate_6: 0  loss_mask_6: 0.4185  loss_dice_6: 0.7295  loss_ce_7: 0.3595  loss_cate_7: 0  loss_mask_7: 0.394  loss_dice_7: 0.7015  loss_ce_8: 0.4833  loss_cate_8: 0  loss_mask_8: 0.3911  loss_dice_8: 0.6842  time: 1.3866  data_time: 0.0104  lr: 8.4352e-06  max_mem: 23815M
[01/01 13:14:33] d2.utils.events INFO:  eta: 12:19:40  iter: 6739  total_loss: 18.84  loss_ce: 0.416  loss_cate: 0.08552  loss_mask: 0.524  loss_dice: 0.649  loss_ce_0: 0.6284  loss_cate_0: 0  loss_mask_0: 0.5378  loss_dice_0: 0.6562  loss_ce_1: 0.4975  loss_cate_1: 0  loss_mask_1: 0.5106  loss_dice_1: 0.5917  loss_ce_2: 0.4148  loss_cate_2: 0  loss_mask_2: 0.5068  loss_dice_2: 0.6105  loss_ce_3: 0.3952  loss_cate_3: 0  loss_mask_3: 0.5279  loss_dice_3: 0.6096  loss_ce_4: 0.4064  loss_cate_4: 0  loss_mask_4: 0.5024  loss_dice_4: 0.7354  loss_ce_5: 0.4114  loss_cate_5: 0  loss_mask_5: 0.4866  loss_dice_5: 0.6761  loss_ce_6: 0.3881  loss_cate_6: 0  loss_mask_6: 0.5306  loss_dice_6: 0.6634  loss_ce_7: 0.3975  loss_cate_7: 0  loss_mask_7: 0.502  loss_dice_7: 0.6543  loss_ce_8: 0.4417  loss_cate_8: 0  loss_mask_8: 0.4991  loss_dice_8: 0.6699  time: 1.3866  data_time: 0.0130  lr: 8.4305e-06  max_mem: 23815M
[01/01 13:15:00] d2.utils.events INFO:  eta: 12:19:11  iter: 6759  total_loss: 18.11  loss_ce: 0.361  loss_cate: 0.09637  loss_mask: 0.4242  loss_dice: 0.7138  loss_ce_0: 0.8501  loss_cate_0: 0  loss_mask_0: 0.4986  loss_dice_0: 0.7032  loss_ce_1: 0.4968  loss_cate_1: 0  loss_mask_1: 0.4297  loss_dice_1: 0.6615  loss_ce_2: 0.4304  loss_cate_2: 0  loss_mask_2: 0.4302  loss_dice_2: 0.7397  loss_ce_3: 0.4263  loss_cate_3: 0  loss_mask_3: 0.4521  loss_dice_3: 0.7343  loss_ce_4: 0.4501  loss_cate_4: 0  loss_mask_4: 0.4172  loss_dice_4: 0.7157  loss_ce_5: 0.4418  loss_cate_5: 0  loss_mask_5: 0.4835  loss_dice_5: 0.6566  loss_ce_6: 0.4029  loss_cate_6: 0  loss_mask_6: 0.4442  loss_dice_6: 0.681  loss_ce_7: 0.4099  loss_cate_7: 0  loss_mask_7: 0.4404  loss_dice_7: 0.6733  loss_ce_8: 0.3945  loss_cate_8: 0  loss_mask_8: 0.4368  loss_dice_8: 0.7269  time: 1.3866  data_time: 0.0109  lr: 8.4258e-06  max_mem: 23815M
[01/01 13:15:28] d2.utils.events INFO:  eta: 12:18:46  iter: 6779  total_loss: 16.68  loss_ce: 0.4273  loss_cate: 0.1266  loss_mask: 0.5919  loss_dice: 0.5762  loss_ce_0: 0.7595  loss_cate_0: 0  loss_mask_0: 0.4788  loss_dice_0: 0.5073  loss_ce_1: 0.4217  loss_cate_1: 0  loss_mask_1: 0.5436  loss_dice_1: 0.5856  loss_ce_2: 0.4513  loss_cate_2: 0  loss_mask_2: 0.4982  loss_dice_2: 0.541  loss_ce_3: 0.4763  loss_cate_3: 0  loss_mask_3: 0.4784  loss_dice_3: 0.4938  loss_ce_4: 0.4422  loss_cate_4: 0  loss_mask_4: 0.4814  loss_dice_4: 0.4837  loss_ce_5: 0.4247  loss_cate_5: 0  loss_mask_5: 0.5037  loss_dice_5: 0.5564  loss_ce_6: 0.3748  loss_cate_6: 0  loss_mask_6: 0.5436  loss_dice_6: 0.5433  loss_ce_7: 0.3957  loss_cate_7: 0  loss_mask_7: 0.5728  loss_dice_7: 0.5535  loss_ce_8: 0.3987  loss_cate_8: 0  loss_mask_8: 0.5683  loss_dice_8: 0.5531  time: 1.3866  data_time: 0.0126  lr: 8.4211e-06  max_mem: 23815M
[01/01 13:15:56] d2.utils.events INFO:  eta: 12:18:21  iter: 6799  total_loss: 17.26  loss_ce: 0.4006  loss_cate: 0.08809  loss_mask: 0.5846  loss_dice: 0.6923  loss_ce_0: 0.8035  loss_cate_0: 0  loss_mask_0: 0.5706  loss_dice_0: 0.7048  loss_ce_1: 0.4814  loss_cate_1: 0  loss_mask_1: 0.5509  loss_dice_1: 0.7417  loss_ce_2: 0.4648  loss_cate_2: 0  loss_mask_2: 0.5362  loss_dice_2: 0.7675  loss_ce_3: 0.479  loss_cate_3: 0  loss_mask_3: 0.5459  loss_dice_3: 0.7129  loss_ce_4: 0.4657  loss_cate_4: 0  loss_mask_4: 0.5429  loss_dice_4: 0.7226  loss_ce_5: 0.4804  loss_cate_5: 0  loss_mask_5: 0.5395  loss_dice_5: 0.7063  loss_ce_6: 0.472  loss_cate_6: 0  loss_mask_6: 0.5692  loss_dice_6: 0.7274  loss_ce_7: 0.465  loss_cate_7: 0  loss_mask_7: 0.5636  loss_dice_7: 0.7519  loss_ce_8: 0.4836  loss_cate_8: 0  loss_mask_8: 0.5827  loss_dice_8: 0.7389  time: 1.3866  data_time: 0.0157  lr: 8.4164e-06  max_mem: 23815M
[01/01 13:16:23] d2.utils.events INFO:  eta: 12:17:51  iter: 6819  total_loss: 17.78  loss_ce: 0.3869  loss_cate: 0.0813  loss_mask: 0.4941  loss_dice: 0.6495  loss_ce_0: 0.7883  loss_cate_0: 0  loss_mask_0: 0.5176  loss_dice_0: 0.6507  loss_ce_1: 0.417  loss_cate_1: 0  loss_mask_1: 0.4958  loss_dice_1: 0.6451  loss_ce_2: 0.4052  loss_cate_2: 0  loss_mask_2: 0.4886  loss_dice_2: 0.646  loss_ce_3: 0.3806  loss_cate_3: 0  loss_mask_3: 0.4963  loss_dice_3: 0.6278  loss_ce_4: 0.4124  loss_cate_4: 0  loss_mask_4: 0.5036  loss_dice_4: 0.633  loss_ce_5: 0.3738  loss_cate_5: 0  loss_mask_5: 0.5299  loss_dice_5: 0.6742  loss_ce_6: 0.3912  loss_cate_6: 0  loss_mask_6: 0.5029  loss_dice_6: 0.6527  loss_ce_7: 0.4149  loss_cate_7: 0  loss_mask_7: 0.4859  loss_dice_7: 0.6759  loss_ce_8: 0.4219  loss_cate_8: 0  loss_mask_8: 0.4901  loss_dice_8: 0.6478  time: 1.3865  data_time: 0.0104  lr: 8.4117e-06  max_mem: 23815M
[01/01 13:16:51] d2.utils.events INFO:  eta: 12:17:24  iter: 6839  total_loss: 18.86  loss_ce: 0.4618  loss_cate: 0.1177  loss_mask: 0.4854  loss_dice: 0.8527  loss_ce_0: 0.7395  loss_cate_0: 0  loss_mask_0: 0.5254  loss_dice_0: 0.7595  loss_ce_1: 0.3641  loss_cate_1: 0  loss_mask_1: 0.561  loss_dice_1: 0.8111  loss_ce_2: 0.4982  loss_cate_2: 0  loss_mask_2: 0.5191  loss_dice_2: 0.8019  loss_ce_3: 0.4532  loss_cate_3: 0  loss_mask_3: 0.481  loss_dice_3: 0.7544  loss_ce_4: 0.3501  loss_cate_4: 0  loss_mask_4: 0.5001  loss_dice_4: 0.776  loss_ce_5: 0.3504  loss_cate_5: 0  loss_mask_5: 0.5007  loss_dice_5: 0.8779  loss_ce_6: 0.3494  loss_cate_6: 0  loss_mask_6: 0.5134  loss_dice_6: 0.7859  loss_ce_7: 0.5233  loss_cate_7: 0  loss_mask_7: 0.5087  loss_dice_7: 0.8137  loss_ce_8: 0.5123  loss_cate_8: 0  loss_mask_8: 0.4957  loss_dice_8: 0.7812  time: 1.3865  data_time: 0.0107  lr: 8.4069e-06  max_mem: 23815M
[01/01 13:17:19] d2.utils.events INFO:  eta: 12:16:57  iter: 6859  total_loss: 16.39  loss_ce: 0.358  loss_cate: 0.09692  loss_mask: 0.5606  loss_dice: 0.7128  loss_ce_0: 0.7568  loss_cate_0: 0  loss_mask_0: 0.5821  loss_dice_0: 0.6282  loss_ce_1: 0.5083  loss_cate_1: 0  loss_mask_1: 0.551  loss_dice_1: 0.7008  loss_ce_2: 0.4299  loss_cate_2: 0  loss_mask_2: 0.5605  loss_dice_2: 0.677  loss_ce_3: 0.3917  loss_cate_3: 0  loss_mask_3: 0.5624  loss_dice_3: 0.698  loss_ce_4: 0.4129  loss_cate_4: 0  loss_mask_4: 0.555  loss_dice_4: 0.781  loss_ce_5: 0.4056  loss_cate_5: 0  loss_mask_5: 0.5498  loss_dice_5: 0.7846  loss_ce_6: 0.3817  loss_cate_6: 0  loss_mask_6: 0.5596  loss_dice_6: 0.7542  loss_ce_7: 0.3615  loss_cate_7: 0  loss_mask_7: 0.5517  loss_dice_7: 0.688  loss_ce_8: 0.3799  loss_cate_8: 0  loss_mask_8: 0.5552  loss_dice_8: 0.6514  time: 1.3866  data_time: 0.0106  lr: 8.4022e-06  max_mem: 23815M
[01/01 13:17:47] d2.utils.events INFO:  eta: 12:16:35  iter: 6879  total_loss: 17.27  loss_ce: 0.3082  loss_cate: 0.1008  loss_mask: 0.4522  loss_dice: 0.7625  loss_ce_0: 0.7509  loss_cate_0: 0  loss_mask_0: 0.4785  loss_dice_0: 0.7629  loss_ce_1: 0.3989  loss_cate_1: 0  loss_mask_1: 0.4762  loss_dice_1: 0.7738  loss_ce_2: 0.386  loss_cate_2: 0  loss_mask_2: 0.4419  loss_dice_2: 0.7571  loss_ce_3: 0.4059  loss_cate_3: 0  loss_mask_3: 0.4598  loss_dice_3: 0.7649  loss_ce_4: 0.3773  loss_cate_4: 0  loss_mask_4: 0.473  loss_dice_4: 0.7492  loss_ce_5: 0.3082  loss_cate_5: 0  loss_mask_5: 0.4688  loss_dice_5: 0.7398  loss_ce_6: 0.3713  loss_cate_6: 0  loss_mask_6: 0.4421  loss_dice_6: 0.7553  loss_ce_7: 0.3504  loss_cate_7: 0  loss_mask_7: 0.4989  loss_dice_7: 0.7714  loss_ce_8: 0.3254  loss_cate_8: 0  loss_mask_8: 0.4543  loss_dice_8: 0.8018  time: 1.3866  data_time: 0.0107  lr: 8.3975e-06  max_mem: 23815M
[01/01 13:18:14] d2.utils.events INFO:  eta: 12:16:03  iter: 6899  total_loss: 18.68  loss_ce: 0.5289  loss_cate: 0.151  loss_mask: 0.6731  loss_dice: 0.7726  loss_ce_0: 0.9402  loss_cate_0: 0  loss_mask_0: 0.6464  loss_dice_0: 0.8244  loss_ce_1: 0.6174  loss_cate_1: 0  loss_mask_1: 0.6652  loss_dice_1: 0.7449  loss_ce_2: 0.4947  loss_cate_2: 0  loss_mask_2: 0.6341  loss_dice_2: 0.7636  loss_ce_3: 0.5466  loss_cate_3: 0  loss_mask_3: 0.5886  loss_dice_3: 0.8152  loss_ce_4: 0.5162  loss_cate_4: 0  loss_mask_4: 0.6538  loss_dice_4: 0.8051  loss_ce_5: 0.5031  loss_cate_5: 0  loss_mask_5: 0.6115  loss_dice_5: 0.8252  loss_ce_6: 0.4672  loss_cate_6: 0  loss_mask_6: 0.6503  loss_dice_6: 0.8152  loss_ce_7: 0.4752  loss_cate_7: 0  loss_mask_7: 0.6607  loss_dice_7: 0.8584  loss_ce_8: 0.4986  loss_cate_8: 0  loss_mask_8: 0.6998  loss_dice_8: 0.8222  time: 1.3865  data_time: 0.0111  lr: 8.3928e-06  max_mem: 23815M
[01/01 13:18:42] d2.utils.events INFO:  eta: 12:15:53  iter: 6919  total_loss: 20.26  loss_ce: 0.3525  loss_cate: 0.1378  loss_mask: 0.6367  loss_dice: 0.867  loss_ce_0: 0.7399  loss_cate_0: 0  loss_mask_0: 0.6286  loss_dice_0: 0.7556  loss_ce_1: 0.5117  loss_cate_1: 0  loss_mask_1: 0.5808  loss_dice_1: 0.8528  loss_ce_2: 0.3821  loss_cate_2: 0  loss_mask_2: 0.5782  loss_dice_2: 0.8507  loss_ce_3: 0.4451  loss_cate_3: 0  loss_mask_3: 0.5616  loss_dice_3: 0.7816  loss_ce_4: 0.4758  loss_cate_4: 0  loss_mask_4: 0.5496  loss_dice_4: 0.8351  loss_ce_5: 0.4274  loss_cate_5: 0  loss_mask_5: 0.582  loss_dice_5: 0.8097  loss_ce_6: 0.448  loss_cate_6: 0  loss_mask_6: 0.5964  loss_dice_6: 0.8067  loss_ce_7: 0.4705  loss_cate_7: 0  loss_mask_7: 0.5803  loss_dice_7: 0.7594  loss_ce_8: 0.4696  loss_cate_8: 0  loss_mask_8: 0.5841  loss_dice_8: 0.8356  time: 1.3866  data_time: 0.0150  lr: 8.3881e-06  max_mem: 23815M
[01/01 13:19:10] d2.utils.events INFO:  eta: 12:15:24  iter: 6939  total_loss: 16.59  loss_ce: 0.4498  loss_cate: 0.09165  loss_mask: 0.4329  loss_dice: 0.5796  loss_ce_0: 0.7498  loss_cate_0: 0  loss_mask_0: 0.4284  loss_dice_0: 0.6639  loss_ce_1: 0.5036  loss_cate_1: 0  loss_mask_1: 0.4073  loss_dice_1: 0.6036  loss_ce_2: 0.4122  loss_cate_2: 0  loss_mask_2: 0.427  loss_dice_2: 0.5786  loss_ce_3: 0.4813  loss_cate_3: 0  loss_mask_3: 0.4109  loss_dice_3: 0.5906  loss_ce_4: 0.4651  loss_cate_4: 0  loss_mask_4: 0.4052  loss_dice_4: 0.6088  loss_ce_5: 0.4715  loss_cate_5: 0  loss_mask_5: 0.3965  loss_dice_5: 0.564  loss_ce_6: 0.4339  loss_cate_6: 0  loss_mask_6: 0.4008  loss_dice_6: 0.5908  loss_ce_7: 0.4432  loss_cate_7: 0  loss_mask_7: 0.3988  loss_dice_7: 0.5641  loss_ce_8: 0.4694  loss_cate_8: 0  loss_mask_8: 0.4367  loss_dice_8: 0.5904  time: 1.3865  data_time: 0.0117  lr: 8.3834e-06  max_mem: 23815M
[01/01 13:19:37] d2.utils.events INFO:  eta: 12:14:57  iter: 6959  total_loss: 18.09  loss_ce: 0.4165  loss_cate: 0.1237  loss_mask: 0.5418  loss_dice: 0.7869  loss_ce_0: 0.7817  loss_cate_0: 0  loss_mask_0: 0.4916  loss_dice_0: 0.8296  loss_ce_1: 0.4646  loss_cate_1: 0  loss_mask_1: 0.4978  loss_dice_1: 0.8088  loss_ce_2: 0.4016  loss_cate_2: 0  loss_mask_2: 0.492  loss_dice_2: 0.744  loss_ce_3: 0.4409  loss_cate_3: 0  loss_mask_3: 0.4183  loss_dice_3: 0.7585  loss_ce_4: 0.4208  loss_cate_4: 0  loss_mask_4: 0.4231  loss_dice_4: 0.7583  loss_ce_5: 0.3983  loss_cate_5: 0  loss_mask_5: 0.4281  loss_dice_5: 0.7635  loss_ce_6: 0.4217  loss_cate_6: 0  loss_mask_6: 0.4837  loss_dice_6: 0.7669  loss_ce_7: 0.3936  loss_cate_7: 0  loss_mask_7: 0.4747  loss_dice_7: 0.7926  loss_ce_8: 0.4183  loss_cate_8: 0  loss_mask_8: 0.5215  loss_dice_8: 0.77  time: 1.3865  data_time: 0.0138  lr: 8.3787e-06  max_mem: 23815M
[01/01 13:20:05] d2.utils.events INFO:  eta: 12:14:33  iter: 6979  total_loss: 13.44  loss_ce: 0.2521  loss_cate: 0.09296  loss_mask: 0.4528  loss_dice: 0.5321  loss_ce_0: 0.7032  loss_cate_0: 0  loss_mask_0: 0.4482  loss_dice_0: 0.5732  loss_ce_1: 0.4516  loss_cate_1: 0  loss_mask_1: 0.453  loss_dice_1: 0.5846  loss_ce_2: 0.304  loss_cate_2: 0  loss_mask_2: 0.4462  loss_dice_2: 0.6106  loss_ce_3: 0.2531  loss_cate_3: 0  loss_mask_3: 0.464  loss_dice_3: 0.4911  loss_ce_4: 0.2392  loss_cate_4: 0  loss_mask_4: 0.4749  loss_dice_4: 0.5152  loss_ce_5: 0.2363  loss_cate_5: 0  loss_mask_5: 0.4588  loss_dice_5: 0.539  loss_ce_6: 0.2418  loss_cate_6: 0  loss_mask_6: 0.451  loss_dice_6: 0.4722  loss_ce_7: 0.2713  loss_cate_7: 0  loss_mask_7: 0.4597  loss_dice_7: 0.4987  loss_ce_8: 0.2487  loss_cate_8: 0  loss_mask_8: 0.4543  loss_dice_8: 0.5739  time: 1.3865  data_time: 0.0111  lr: 8.374e-06  max_mem: 23815M
[01/01 13:20:33] d2.utils.events INFO:  eta: 12:14:06  iter: 6999  total_loss: 16.29  loss_ce: 0.4003  loss_cate: 0.1175  loss_mask: 0.5504  loss_dice: 0.658  loss_ce_0: 0.7745  loss_cate_0: 0  loss_mask_0: 0.5498  loss_dice_0: 0.7095  loss_ce_1: 0.4296  loss_cate_1: 0  loss_mask_1: 0.5942  loss_dice_1: 0.7178  loss_ce_2: 0.4431  loss_cate_2: 0  loss_mask_2: 0.5206  loss_dice_2: 0.7165  loss_ce_3: 0.3919  loss_cate_3: 0  loss_mask_3: 0.5252  loss_dice_3: 0.6857  loss_ce_4: 0.4024  loss_cate_4: 0  loss_mask_4: 0.5474  loss_dice_4: 0.7816  loss_ce_5: 0.3883  loss_cate_5: 0  loss_mask_5: 0.5571  loss_dice_5: 0.751  loss_ce_6: 0.3737  loss_cate_6: 0  loss_mask_6: 0.5493  loss_dice_6: 0.7588  loss_ce_7: 0.3944  loss_cate_7: 0  loss_mask_7: 0.5463  loss_dice_7: 0.686  loss_ce_8: 0.4074  loss_cate_8: 0  loss_mask_8: 0.5491  loss_dice_8: 0.6689  time: 1.3865  data_time: 0.0119  lr: 8.3693e-06  max_mem: 23815M
[01/01 13:21:00] d2.utils.events INFO:  eta: 12:13:44  iter: 7019  total_loss: 16.75  loss_ce: 0.4054  loss_cate: 0.09568  loss_mask: 0.4257  loss_dice: 0.6275  loss_ce_0: 0.7509  loss_cate_0: 0  loss_mask_0: 0.4084  loss_dice_0: 0.6295  loss_ce_1: 0.3847  loss_cate_1: 0  loss_mask_1: 0.4583  loss_dice_1: 0.6708  loss_ce_2: 0.4549  loss_cate_2: 0  loss_mask_2: 0.4223  loss_dice_2: 0.6195  loss_ce_3: 0.3905  loss_cate_3: 0  loss_mask_3: 0.4413  loss_dice_3: 0.6394  loss_ce_4: 0.3889  loss_cate_4: 0  loss_mask_4: 0.4679  loss_dice_4: 0.611  loss_ce_5: 0.3929  loss_cate_5: 0  loss_mask_5: 0.4668  loss_dice_5: 0.5987  loss_ce_6: 0.418  loss_cate_6: 0  loss_mask_6: 0.4638  loss_dice_6: 0.6084  loss_ce_7: 0.3752  loss_cate_7: 0  loss_mask_7: 0.432  loss_dice_7: 0.6538  loss_ce_8: 0.4114  loss_cate_8: 0  loss_mask_8: 0.4428  loss_dice_8: 0.6005  time: 1.3864  data_time: 0.0149  lr: 8.3646e-06  max_mem: 23815M
[01/01 13:21:28] d2.utils.events INFO:  eta: 12:13:19  iter: 7039  total_loss: 20.78  loss_ce: 0.4294  loss_cate: 0.1177  loss_mask: 0.6609  loss_dice: 0.8521  loss_ce_0: 0.5877  loss_cate_0: 0  loss_mask_0: 0.6269  loss_dice_0: 0.8279  loss_ce_1: 0.3499  loss_cate_1: 0  loss_mask_1: 0.6016  loss_dice_1: 0.8836  loss_ce_2: 0.4982  loss_cate_2: 0  loss_mask_2: 0.5752  loss_dice_2: 0.8351  loss_ce_3: 0.3552  loss_cate_3: 0  loss_mask_3: 0.6317  loss_dice_3: 0.8573  loss_ce_4: 0.3618  loss_cate_4: 0  loss_mask_4: 0.6176  loss_dice_4: 0.7899  loss_ce_5: 0.3991  loss_cate_5: 0  loss_mask_5: 0.5994  loss_dice_5: 0.8309  loss_ce_6: 0.4124  loss_cate_6: 0  loss_mask_6: 0.5924  loss_dice_6: 0.8445  loss_ce_7: 0.4114  loss_cate_7: 0  loss_mask_7: 0.5961  loss_dice_7: 0.8349  loss_ce_8: 0.403  loss_cate_8: 0  loss_mask_8: 0.5974  loss_dice_8: 0.8487  time: 1.3864  data_time: 0.0118  lr: 8.3599e-06  max_mem: 23815M
[01/01 13:21:56] d2.utils.events INFO:  eta: 12:12:53  iter: 7059  total_loss: 17.05  loss_ce: 0.3486  loss_cate: 0.1069  loss_mask: 0.4827  loss_dice: 0.6133  loss_ce_0: 0.6023  loss_cate_0: 0  loss_mask_0: 0.5243  loss_dice_0: 0.6162  loss_ce_1: 0.3523  loss_cate_1: 0  loss_mask_1: 0.5566  loss_dice_1: 0.6735  loss_ce_2: 0.2981  loss_cate_2: 0  loss_mask_2: 0.5018  loss_dice_2: 0.567  loss_ce_3: 0.2999  loss_cate_3: 0  loss_mask_3: 0.495  loss_dice_3: 0.6313  loss_ce_4: 0.4071  loss_cate_4: 0  loss_mask_4: 0.4954  loss_dice_4: 0.5604  loss_ce_5: 0.3842  loss_cate_5: 0  loss_mask_5: 0.4658  loss_dice_5: 0.606  loss_ce_6: 0.3273  loss_cate_6: 0  loss_mask_6: 0.4856  loss_dice_6: 0.6181  loss_ce_7: 0.2969  loss_cate_7: 0  loss_mask_7: 0.4486  loss_dice_7: 0.6038  loss_ce_8: 0.3304  loss_cate_8: 0  loss_mask_8: 0.4856  loss_dice_8: 0.6306  time: 1.3864  data_time: 0.0122  lr: 8.3552e-06  max_mem: 23815M
[01/01 13:22:23] d2.utils.events INFO:  eta: 12:12:22  iter: 7079  total_loss: 15.77  loss_ce: 0.3834  loss_cate: 0.09657  loss_mask: 0.3544  loss_dice: 0.6242  loss_ce_0: 0.7446  loss_cate_0: 0  loss_mask_0: 0.4589  loss_dice_0: 0.6418  loss_ce_1: 0.4783  loss_cate_1: 0  loss_mask_1: 0.4081  loss_dice_1: 0.651  loss_ce_2: 0.4405  loss_cate_2: 0  loss_mask_2: 0.3894  loss_dice_2: 0.6411  loss_ce_3: 0.4169  loss_cate_3: 0  loss_mask_3: 0.3622  loss_dice_3: 0.6188  loss_ce_4: 0.3528  loss_cate_4: 0  loss_mask_4: 0.3579  loss_dice_4: 0.6302  loss_ce_5: 0.3461  loss_cate_5: 0  loss_mask_5: 0.3558  loss_dice_5: 0.6394  loss_ce_6: 0.4133  loss_cate_6: 0  loss_mask_6: 0.3571  loss_dice_6: 0.6242  loss_ce_7: 0.3951  loss_cate_7: 0  loss_mask_7: 0.3857  loss_dice_7: 0.6286  loss_ce_8: 0.3885  loss_cate_8: 0  loss_mask_8: 0.3619  loss_dice_8: 0.6234  time: 1.3864  data_time: 0.0146  lr: 8.3505e-06  max_mem: 23815M
[01/01 13:22:51] d2.utils.events INFO:  eta: 12:11:58  iter: 7099  total_loss: 16.04  loss_ce: 0.4932  loss_cate: 0.1115  loss_mask: 0.3753  loss_dice: 0.607  loss_ce_0: 0.7351  loss_cate_0: 0  loss_mask_0: 0.3872  loss_dice_0: 0.6315  loss_ce_1: 0.4658  loss_cate_1: 0  loss_mask_1: 0.4387  loss_dice_1: 0.6183  loss_ce_2: 0.4564  loss_cate_2: 0  loss_mask_2: 0.39  loss_dice_2: 0.6723  loss_ce_3: 0.4894  loss_cate_3: 0  loss_mask_3: 0.3854  loss_dice_3: 0.5786  loss_ce_4: 0.4654  loss_cate_4: 0  loss_mask_4: 0.3796  loss_dice_4: 0.5731  loss_ce_5: 0.4708  loss_cate_5: 0  loss_mask_5: 0.3755  loss_dice_5: 0.6069  loss_ce_6: 0.4582  loss_cate_6: 0  loss_mask_6: 0.3889  loss_dice_6: 0.623  loss_ce_7: 0.4797  loss_cate_7: 0  loss_mask_7: 0.372  loss_dice_7: 0.5902  loss_ce_8: 0.4923  loss_cate_8: 0  loss_mask_8: 0.3828  loss_dice_8: 0.5916  time: 1.3864  data_time: 0.0177  lr: 8.3458e-06  max_mem: 23815M
